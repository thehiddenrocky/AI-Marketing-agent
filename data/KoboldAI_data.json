{
    "subreddit_info": {
        "name": "KoboldAI",
        "title": "KoboldAI",
        "subscribers": 11814,
        "public_description": "Discussion for the KoboldAI story generation client."
    },
    "posts": [
        {
            "id": "1grkaow",
            "title": "Would something like this be capable/well suited for running Kobold AI?",
            "author": "JustSomeRand0meGuy",
            "score": 3,
            "upvotes": 3,
            "downvotes": 0,
            "num_comments": 6,
            "created_utc": 1731632203.0,
            "url": "https://www.reddit.com/r/KoboldAI/comments/1grkaow/would_something_like_this_be_capablewell_suited/",
            "permalink": "/r/KoboldAI/comments/1grkaow/would_something_like_this_be_capablewell_suited/",
            "selftext": "I am looking at getting a new PC and was wandering is something like this would be capable or even well suited for running Kobold AI?\n\n* 1x Build Stock AMD Ryzen 7 7800X3D Eight Core 5.00GHz (Socket AM5) Processor -\n* 1x Asus ROG Strix B650E-E Gaming WIFI (Socket AM5) DDR5 ATX Motherboard\n* 1x Corsair Vengeance EXPO 64GB (2X32GB) DDR5 PC5-41600C40 5200MHz Dual Channel Kit - Black (CMK64GX5M2B5200Z40)\n* 1x NZXT Kraken 240 Black AIO CPU Water Cooler - 240mm\n* 1x MSI GeForce RTX 4070 VENTUS 2X E1 OC 12GB GDDR6 PCI-Express Graphics Card\n* 1x Corsair RMe Series RM1000e Fully Modular Low-Noise ATX Power Supply v2 (CP-9020264-UK)\n* 1x WD Black SN850X 4TB SSD M.2 2280 NVME PCI-E Gen4 Solid State Drive (WDS400T2X0E)\n* 1x WD Black SN850X 4TB SSD M.2 2280 NVME PCI-E Gen4 Solid State Drive (WDS400T2X0E)",
            "comments": [
                {
                    "id": "lx6rtxn",
                    "author": "pyroserenus",
                    "body": "The 7800x3d is a relatively cool CPU, rendering AIO's generally superfluous, a decent tower air cooler will do fine.\n\nWhy 2x 4tb nvmes?\n\nMore fund allocation into GPU is viable here by dropping to a tower air cooler and 1 nvme, in most cases \"as much vram as possible on a reasonably modern nvidia gpu\" tends to be optimal.\n\nThe system can handle faster ram, if planning to split models between vram and ram, you want the fastest ram the CPU is known to handle well. If planning to just run what runs on the GPU, then this doesn't really matter.\n\nAs for capable, yeah, 12gb vram ensures 13b and smaller models will run fast regardless. I just wouldn't call the system optimal.",
                    "score": 5,
                    "upvotes": 5,
                    "downvotes": 0,
                    "created_utc": 1731634227.0,
                    "parent_id": "t3_1grkaow",
                    "link_id": "t3_1grkaow",
                    "permalink": "/r/KoboldAI/comments/1grkaow/would_something_like_this_be_capablewell_suited/lx6rtxn/"
                },
                {
                    "id": "lx8putj",
                    "author": "henk717",
                    "body": "Its primarily about the vram (and it being nvidia for the best AI compatibility), so a machine like this is well suited up to 13B Q4\\_K\\_S.",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1731665957.0,
                    "parent_id": "t3_1grkaow",
                    "link_id": "t3_1grkaow",
                    "permalink": "/r/KoboldAI/comments/1grkaow/would_something_like_this_be_capablewell_suited/lx8putj/"
                },
                {
                    "id": "lx74tr5",
                    "author": "JustSomeRand0meGuy",
                    "body": "Some of the odd choice in parts is partly due to the fact that I am having a company build it for me (I am only just beginning on the path of learning about PC building myself) so I am limited to what they have on offer. As well as partly because being able to run Kobold AI (and hopefully even stable diffusion) would be a very nice bonus to the main things I need it to be able to do such as Gaming, day to day use, 3D modeling and such (things like that are the reason for the multiple nvmes)\n\n  \nThanks for the note about the GPU. Unfortunately its is the best one that I can get in the machine at the time though. What would you recommend trying to upgrade it to later down the line in terms of biggest bang for buck?\n\n  \nFor RAM would installing a couple more sticks of the same RAM help at all?",
                    "score": 2,
                    "upvotes": 2,
                    "downvotes": 0,
                    "created_utc": 1731638617.0,
                    "parent_id": "t1_lx6rtxn",
                    "link_id": "t3_1grkaow",
                    "permalink": "/r/KoboldAI/comments/1grkaow/would_something_like_this_be_capablewell_suited/lx74tr5/"
                },
                {
                    "id": "lxdzxqh",
                    "author": "JustSomeRand0meGuy",
                    "body": "Thanks.",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1731733460.0,
                    "parent_id": "t1_lx8putj",
                    "link_id": "t3_1grkaow",
                    "permalink": "/r/KoboldAI/comments/1grkaow/would_something_like_this_be_capablewell_suited/lxdzxqh/"
                },
                {
                    "id": "lx7tms4",
                    "author": "BangkokPadang",
                    "body": "There's huge overlap between gaming, 3D modelling, and LLMs. The GPU.\n\nSurely they don't only sell 4tb drives in pairs? Do they demand you get an AIO?. Drop the second nvme drive, lose the AIO and get a good air cooler, and upgrade to the 4070ti Super instead of the 4070 to get to 16GB vram instead of 12.",
                    "score": 2,
                    "upvotes": 2,
                    "downvotes": 0,
                    "created_utc": 1731647994.0,
                    "parent_id": "t1_lx74tr5",
                    "link_id": "t3_1grkaow",
                    "permalink": "/r/KoboldAI/comments/1grkaow/would_something_like_this_be_capablewell_suited/lx7tms4/"
                },
                {
                    "id": "lx8gzpy",
                    "author": "JustSomeRand0meGuy",
                    "body": "Thanks, I appreciate the feedback.\n\nThe two drives are simply because I had way too little hard drive space in my last machine and really want to make sure that that is not the case with this one starting out (I am aware that it may be overkill).\n\nUnfortunately The AIO is already the cheapest option available for cooling I have have with no option for an air cooler (I would have probably gone with one if I could). The GPU is the best option I have available for me to pick from too I'm afraid. (due to the limitation on parts selection I mentioned)\n\nIs the 4070ti SuperÂ a good choice to look into upgrading to in a year or two, or at that point would you recommend a 4080 or even a 4090 if the price comes down by then?",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1731660495.0,
                    "parent_id": "t1_lx7tms4",
                    "link_id": "t3_1grkaow",
                    "permalink": "/r/KoboldAI/comments/1grkaow/would_something_like_this_be_capablewell_suited/lx8gzpy/"
                }
            ]
        },
        {
            "id": "1grhwac",
            "title": "Are suggest Replies still a thing?",
            "author": "Dry-Lavishness-2696",
            "score": 7,
            "upvotes": 7,
            "downvotes": 0,
            "num_comments": 5,
            "created_utc": 1731625317.0,
            "url": "https://www.reddit.com/r/KoboldAI/comments/1grhwac/are_suggest_replies_still_a_thing/",
            "permalink": "/r/KoboldAI/comments/1grhwac/are_suggest_replies_still_a_thing/",
            "selftext": "Been a couple years since I last played with KoboldAI, but I have access to some new hardware, so I've been checking out KoboldAI Lite on the Colab page. It's all looking pretty cool - Except that I can't see any auto generated reply options?\n\nIn ye olden times, I frequently found it helpful to have like 3 potential replies (or story continuations) generated, to either use as-is, combine or lightly edit, and help the story carry itself. Suggests fun twists and saves me a lot of typing sometimes, too.\n\nIs this still a feature that I'm missing somewhere? Or is it lost to the sands of time?",
            "comments": [
                {
                    "id": "lx8v09u",
                    "author": "henk717",
                    "body": "Those combined replies are not a thing in KoboldAI Lite at the moment, KoboldCpp to my knowledge doesn't have that ability in the engine.",
                    "score": 3,
                    "upvotes": 3,
                    "downvotes": 0,
                    "created_utc": 1731668956.0,
                    "parent_id": "t3_1grhwac",
                    "link_id": "t3_1grhwac",
                    "permalink": "/r/KoboldAI/comments/1grhwac/are_suggest_replies_still_a_thing/lx8v09u/"
                },
                {
                    "id": "lxb350p",
                    "author": "Original-Success-784",
                    "body": "Did you mean \"chat select\"? Direct before the 'text field'\n\n\\- Impersonate the AI Assistant  \n\\- Make the AI write a response as me (for 1 turn)  \n\n\\[Edit:\\] This is still there in 'KoboldAI Lite v184' and you can edit it before sending to the LLM",
                    "score": 2,
                    "upvotes": 2,
                    "downvotes": 0,
                    "created_utc": 1731696336.0,
                    "parent_id": "t3_1grhwac",
                    "link_id": "t3_1grhwac",
                    "permalink": "/r/KoboldAI/comments/1grhwac/are_suggest_replies_still_a_thing/lxb350p/"
                },
                {
                    "id": "lxcfv2l",
                    "author": "Dry-Lavishness-2696",
                    "body": "Alas! Thank you for confirming <3 And seems the old colab got taken down, may look around see if I can get ye' ol' 'un booted up..",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1731711493.0,
                    "parent_id": "t1_lx8v09u",
                    "link_id": "t3_1grhwac",
                    "permalink": "/r/KoboldAI/comments/1grhwac/are_suggest_replies_still_a_thing/lxcfv2l/"
                },
                {
                    "id": "lxcfrzi",
                    "author": "Dry-Lavishness-2696",
                    "body": "Not what I meant - Used to be you could get 2-5 suggested replies and click one. But that is interesting, it's something at least, thanks!",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1731711462.0,
                    "parent_id": "t1_lxb350p",
                    "link_id": "t3_1grhwac",
                    "permalink": "/r/KoboldAI/comments/1grhwac/are_suggest_replies_still_a_thing/lxcfrzi/"
                },
                {
                    "id": "lxdbj5o",
                    "author": "henk717",
                    "body": "Old one would be [https://koboldai.org/colab](https://koboldai.org/colab) but its broken unless you change the version to United.",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1731723318.0,
                    "parent_id": "t1_lxcfv2l",
                    "link_id": "t3_1grhwac",
                    "permalink": "/r/KoboldAI/comments/1grhwac/are_suggest_replies_still_a_thing/lxdbj5o/"
                }
            ]
        },
        {
            "id": "1greu4e",
            "title": "Esocont Mystery Mix",
            "author": "The_Linux_Colonel",
            "score": 3,
            "upvotes": 3,
            "downvotes": 0,
            "num_comments": 5,
            "created_utc": 1731617250.0,
            "url": "https://www.reddit.com/r/KoboldAI/comments/1greu4e/esocont_mystery_mix/",
            "permalink": "/r/KoboldAI/comments/1greu4e/esocont_mystery_mix/",
            "selftext": "I enjoy testing models on the horde and then, if I like them, going to HF and trying them out locally with quants my system can handle. I was enjoying this particular model MysteryMix which is apparently made by someone named Eso, but I can't find anything about it on HF or even a Google search. \n\nDoes anyone know more about this model or whether it's going by another name somewhere else? It's coherent and does a good job at staying in the lines but still being creative. I'd love to know what its parameter number is, and if it's a merge (likely since it's called 'mix') what it's a merge of. I thought maybe it was related to BagelMIstery, but there's nothing under the alternate spelling of MisteryMix either.\n\nAnybody know more about the model or what it goes by on HF (or if it's just a private, unreleased model)?",
            "comments": [
                {
                    "id": "lx5sqg2",
                    "author": "henk717",
                    "body": "Can't track it either, its possible this is not a public model at the moment. Sometimes model authors put their experimental models up on Horde to see what people think.",
                    "score": 5,
                    "upvotes": 5,
                    "downvotes": 0,
                    "created_utc": 1731622685.0,
                    "parent_id": "t3_1greu4e",
                    "link_id": "t3_1greu4e",
                    "permalink": "/r/KoboldAI/comments/1greu4e/esocont_mystery_mix/lx5sqg2/"
                },
                {
                    "id": "lx6gq8l",
                    "author": "The_Linux_Colonel",
                    "body": "That's interesting. Feels like reverse beta testing where it's public before it's private. I guess it's a good way to demo it if you let people use it on your machine first. I wonder what other models got their start on the horde. Thanks for confirming that my googlefu wasn't slipping.",
                    "score": 2,
                    "upvotes": 2,
                    "downvotes": 0,
                    "created_utc": 1731630458.0,
                    "parent_id": "t1_lx5sqg2",
                    "link_id": "t3_1greu4e",
                    "permalink": "/r/KoboldAI/comments/1greu4e/esocont_mystery_mix/lx6gq8l/"
                },
                {
                    "id": "lx8q1ky",
                    "author": "henk717",
                    "body": "Multiple KoboldAI models actually and even a corporate model where people want to find out if their new paid tune is well received. Main reason its done is that if you do it this way people can give feedback on the work in progress models that may be bad, and then its not an official release yet thats out in the open forever tarnishing your name with how bad it is haha.",
                    "score": 2,
                    "upvotes": 2,
                    "downvotes": 0,
                    "created_utc": 1731666071.0,
                    "parent_id": "t1_lx6gq8l",
                    "link_id": "t3_1greu4e",
                    "permalink": "/r/KoboldAI/comments/1greu4e/esocont_mystery_mix/lx8q1ky/"
                },
                {
                    "id": "lxa4tak",
                    "author": "The_Linux_Colonel",
                    "body": "That's actually an interesting point. I didn't think corpos would be looking toward open platform as a form of testing, but it makes sense that they'd have proper plausible deniability in the event something happened. How do model testers get information from anonymous horde users, though? Unless they lurk this sub or the localllm one, maybe, there's little chance they'd hear what anyone was saying about a mysterious new model. They couldn't use their corpo socials to ask directly or they'd out themselves. So, how do they get the feedback?",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1731686152.0,
                    "parent_id": "t1_lx8q1ky",
                    "link_id": "t3_1greu4e",
                    "permalink": "/r/KoboldAI/comments/1greu4e/esocont_mystery_mix/lxa4tak/"
                },
                {
                    "id": "lxb7ggz",
                    "author": "henk717",
                    "body": "Its not common for horde specifically, but the corporation in question hired a tuner from Kobolds community. They typically have a specific group they are talking to such as testers from Kobold or various other LLM RP discords that know horde.\n\nIn this case I expect it to be an individual.",
                    "score": 2,
                    "upvotes": 2,
                    "downvotes": 0,
                    "created_utc": 1731697616.0,
                    "parent_id": "t1_lxa4tak",
                    "link_id": "t3_1greu4e",
                    "permalink": "/r/KoboldAI/comments/1greu4e/esocont_mystery_mix/lxb7ggz/"
                }
            ]
        },
        {
            "id": "1gr1qur",
            "title": "My AI model accidently created an Acid trip/feaver dream... But its pretty cool to look at.",
            "author": "Rombodawg",
            "score": 5,
            "upvotes": 5,
            "downvotes": 0,
            "num_comments": 0,
            "created_utc": 1731579540.0,
            "url": "https://www.reddit.com/r/KoboldAI/comments/1gr1qur/my_ai_model_accidently_created_an_acid_tripfeaver/",
            "permalink": "/r/KoboldAI/comments/1gr1qur/my_ai_model_accidently_created_an_acid_tripfeaver/",
            "selftext": "So i created an LLM the other day that is a better version of the qwen-2.5-coder 32b model, and i was suprized at how low of quant i was able to take it to before it lost any quality. I was testing a few diffrent prompts with it, one being to make a simple cube shape in opengl in python and have it rotate.\n\nThe \"i1-IQ2\\_M\" version which is basically like a 2-bit variant, is surprisingly good, despite being like almost 1/7th the size of the original model. However when I ran the prompt through it and ran the code, it had some very interesting results.\n\nHere is the prompt\n\n\"Render a cube in py opengl so i can run it in as a python file. Make sure the cube has diffrent colored faces and spins.\"\n\nAnd here is the code, save it as a python file, like \"code.py\" and run it in your terminal using \"python code.py\" if you get any libraries that dont install, run \"pip install X\" with X being the library that needs installed.\n\n    import glfw\n    from OpenGL.GL import *\n    from OpenGL.GL.shaders import compileProgram, compileShader\n    import numpy as np\n    \n    # Vertex shader source code\n    vertex_shader_source = \"\"\"\n    #version 330 core\n    layout (location = 0) in vec3 aPos;\n    layout (location = 1) in vec3 aColor;\n    \n    out vec3 ourColor;\n    \n    uniform mat4 model;\n    uniform mat4 view;\n    uniform mat4 projection;\n    \n    void main()\n    {\n    Â  Â  gl_Position = projection * view * model * vec4(aPos, 1.0);\n    Â  Â  ourColor = aColor;\n    }\n    \"\"\"\n    \n    # Fragment shader source code\n    fragment_shader_source = \"\"\"\n    #version 330 core\n    out vec4 FragColor;\n    \n    in vec3 ourColor;\n    \n    void main()\n    {\n    Â  Â  FragColor = vec4(ourColor, 1.0);\n    } \n    \"\"\"\n    \n    def window_resize(window, width, height):\n    Â  Â  glViewport(0, 0, width, height)\n    \n    def main():\n    Â  Â  # Initialize GLFW\n    Â  Â  if not glfw.init():\n    Â  Â  Â  Â  return\n    \n    Â  Â  # Create a windowed mode window and its OpenGL context\n    Â  Â  window = glfw.create_window(800, 600, \"Spinning Cube\", None, None)\n    Â  Â  if not window:\n    Â  Â  Â  Â  glfw.terminate()\n    Â  Â  Â  Â  return\n    \n    Â  Â  # Make the window's context current\n    Â  Â  glfw.make_context_current(window)\n    \n    Â  Â  # Set the viewport\n    Â  Â  glViewport(0, 0, 800, 600)\n    \n    Â  Â  # Register callback functions\n    Â  Â  glfw.set_window_size_callback(window, window_resize)\n    \n    Â  Â  # Compile shaders and create shader program\n    Â  Â  vertex_shader = compileShader(vertex_shader_source, GL_VERTEX_SHADER)\n    Â  Â  fragment_shader = compileShader(fragment_shader_source, GL_FRAGMENT_SHADER)\n    Â  Â  shader_program = compileProgram(vertex_shader, fragment_shader)\n    \n    Â  Â  # Define the vertices of a cube with different colors for each face\n    Â  Â  vertices = np.array([\n    Â  Â  Â  Â  # positions Â  Â  Â  Â  Â # colors\n    Â  Â  Â  Â  -0.5, -0.5, -0.5, 1.0, 0.0, 0.0, Â # Back face red\n    Â  Â  Â  Â  Â 0.5, -0.5, -0.5, 1.0, 0.0, 0.0,\n    Â  Â  Â  Â  Â 0.5, Â 0.5, -0.5, 1.0, 0.0, 0.0,\n    \n    Â  Â  Â  Â  Â 0.5, Â 0.5, -0.5, 0.0, 1.0, 0.0, Â # Front face green\n    Â  Â  Â  Â  -0.5, Â 0.5, -0.5, 0.0, 1.0, 0.0,\n    Â  Â  Â  Â  -0.5, -0.5, -0.5, 0.0, 1.0, 0.0,\n    \n    Â  Â  Â  Â  -0.5, -0.5, Â 0.5, 0.0, 0.0, 1.0, Â # Left face blue\n    Â  Â  Â  Â  -0.5, Â 0.5, Â 0.5, 0.0, 0.0, 1.0,\n    Â  Â  Â  Â  Â 0.5, Â 0.5, Â 0.5, 0.0, 0.0, 1.0,\n    \n    Â  Â  Â  Â  Â 0.5, Â 0.5, Â 0.5, 1.0, 1.0, 0.0, Â # Right face yellow\n    Â  Â  Â  Â  Â 0.5, -0.5, Â 0.5, 1.0, 1.0, 0.0,\n    Â  Â  Â  Â  -0.5, -0.5, Â 0.5, 1.0, 1.0, 0.0,\n    \n    Â  Â  Â  Â  -0.5, Â 0.5, Â 0.5, 1.0, 0.0, 1.0, Â # Top face magenta\n    Â  Â  Â  Â  Â 0.5, Â 0.5, Â 0.5, 1.0, 0.0, 1.0,\n    Â  Â  Â  Â  Â 0.5, Â 0.5, -0.5, 1.0, 0.0, 1.0,\n    \n    Â  Â  Â  Â  -0.5, Â 0.5, -0.5, 0.0, 1.0, 1.0, Â # Bottom face cyan\n    Â  Â  Â  Â  Â 0.5, -0.5, -0.5, 0.0, 1.0, 1.0,\n    Â  Â  Â  Â  -0.5, -0.5, -0.5, 0.0, 1.0, 1.0,\n    \n    Â  Â  Â  Â  -0.5, -0.5, Â 0.5, 0.0, 1.0, 0.0, Â # Bottom face green\n    Â  Â  Â  Â  Â 0.5, -0.5, Â 0.5, 0.0, 1.0, 0.0,\n    Â  Â  Â  Â  -0.5, -0.5, -0.5, 0.0, 1.0, 0.0,\n    \n    Â  Â  Â  Â  0.5, -0.5, Â 0.5, 1.0, 0.0, 0.0, Â  # Bottom face red\n    Â  Â  Â  Â  0.5, -0.5, -0.5, 1.0, 0.0, 0.0,\n    Â  Â  Â  Â  0.5, Â 0.5, -0.5, 1.0, 0.0, 0.0,\n    \n    Â  Â  Â  Â  -0.5, -0.5, Â 0.5, 0.0, 0.0, 1.0, # Bottom face blue\n    Â  Â  Â  Â  -0.5, -0.5, -0.5, 0.0, 0.0, 1.0,\n    Â  Â  Â  Â  -0.5, Â 0.5, -0.5, 0.0, 0.0, 1.0,\n    \n    Â  Â  Â  Â  -0.5, Â 0.5, Â 0.5, 1.0, 1.0, 0.0, # Bottom face yellow\n    Â  Â  Â  Â  -0.5, Â 0.5, -0.5, 1.0, 1.0, 0.0,\n    Â  Â  Â  Â  Â 0.5, Â 0.5, -0.5, 1.0, 1.0, 0.0,\n    \n    Â  Â  ], dtype=np.float32)\n    \n    Â  Â  # Create a Vertex Buffer Object (VBO) and upload the vertices to it\n    Â  Â  VBO = glGenBuffers(1)\n    Â  Â  glBindBuffer(GL_ARRAY_BUFFER, VBO)\n    Â  Â  glBufferData(GL_ARRAY_BUFFER, vertices.nbytes, vertices, GL_STATIC_DRAW)\n    \n    Â  Â  # Define the vertex attributes\n    Â  Â  position = glGetAttribLocation(shader_program, 'aPos')\n    Â  Â  glVertexAttribPointer(position, 3, GL_FLOAT, False, 6 * vertices.itemsize, ctypes.c_void_p(0))\n    Â  Â  glEnableVertexAttribArray(position)\n    \n    Â  Â  color = glGetAttribLocation(shader_program, 'aColor')\n    Â  Â  glVertexAttribPointer(color, 3, GL_FLOAT, False, 6 * vertices.itemsize, ctypes.c_void_p(12))\n    Â  Â  glEnableVertexAttribArray(color)\n    \n    Â  Â  # Define the model, view, and projection matrices\n    Â  Â  model = np.eye(4)\n    Â  Â  view = np.eye(4)\n    Â  Â  projection = np.eye(4)\n    \n    Â  Â  # Move the camera back a bit\n    Â  Â  view = np.array([\n    Â  Â  Â  Â  [1, 0, 0, 0],\n    Â  Â  Â  Â  [0, 1, 0, 0],\n    Â  Â  Â  Â  [0, 0, 1, -5],\n    Â  Â  Â  Â  [0, 0, 0, 1]\n    Â  Â  ], dtype=np.float32)\n    \n    Â  Â  # Enable depth testing\n    Â  Â  glEnable(GL_DEPTH_TEST)\n    \n    Â  Â  # Loop until the user closes the window\n    Â  Â  while not glfw.window_should_close(window):\n    Â  Â  Â  Â  # Check for events\n    Â  Â  Â  Â  glfw.poll_events()\n    \n    Â  Â  Â  Â  # Clear the color and depth buffers\n    Â  Â  Â  Â  glClear(GL_COLOR_BUFFER_BIT | GL_DEPTH_BUFFER_BIT)\n    \n    Â  Â  Â  Â  # Use the shader program\n    Â  Â  Â  Â  glUseProgram(shader_program)\n    \n    Â  Â  Â  Â  # Rotate the cube around its center\n    Â  Â  Â  Â  model = np.eye(4)\n    Â  Â  Â  Â  model = np.dot(model, np.array([\n    Â  Â  Â  Â  Â  Â  [1, 0, 0, 0],\n    Â  Â  Â  Â  Â  Â  [0, np.cos(glfw.get_time()), -np.sin(glfw.get_time()), 0],\n    Â  Â  Â  Â  Â  Â  [0, np.sin(glfw.get_time()), np.cos(glfw.get_time()), 0],\n    Â  Â  Â  Â  Â  Â  [0, 0, 0, 1]\n    Â  Â  Â  Â  ], dtype=np.float32))\n    \n    Â  Â  Â  Â  model = np.dot(model, np.array([\n    Â  Â  Â  Â  Â  Â  [np.cos(glfw.get_time() * 0.5), 0, np.sin(glfw.get_time() * 0.5), 0],\n    Â  Â  Â  Â  Â  Â  [0, 1, 0, 0],\n    Â  Â  Â  Â  Â  Â  [-np.sin(glfw.get_time() * 0.5), 0, np.cos(glfw.get_time() * 0.5), 0],\n    Â  Â  Â  Â  Â  Â  [0, 0, 0, 1]\n    Â  Â  Â  Â  ], dtype=np.float32))\n    \n    Â  Â  Â  Â  # Set the model, view, and projection matrices\n    Â  Â  Â  Â  glUniformMatrix4fv(glGetUniformLocation(shader_program, 'model'), 1, GL_FALSE, model)\n    Â  Â  Â  Â  glUniformMatrix4fv(glGetUniformLocation(shader_program, 'view'), 1, GL_FALSE, view)\n    Â  Â  Â  Â  glUniformMatrix4fv(glGetUniformLocation(shader_program, 'projection'), 1, GL_FALSE, projection)\n    \n    Â  Â  Â  Â  # Draw the cube\n    Â  Â  Â  Â  glDrawArrays(GL_TRIANGLES, 0, len(vertices) // 6)\n    \n    Â  Â  Â  Â  # Swap front and back buffers\n    Â  Â  Â  Â  glfw.swap_buffers(window)\n    \n    Â  Â  # Terminate GLFW\n    Â  Â  glfw.terminate()\n    \n    if __name__ == \"__main__\":\n    Â  Â  main()",
            "comments": []
        },
        {
            "id": "1gqrx7m",
            "title": "Loading GGUF model",
            "author": "Azmort_Red",
            "score": 2,
            "upvotes": 2,
            "downvotes": 0,
            "num_comments": 5,
            "created_utc": 1731543508.0,
            "url": "https://www.reddit.com/r/KoboldAI/comments/1gqrx7m/loading_gguf_model/",
            "permalink": "/r/KoboldAI/comments/1gqrx7m/loading_gguf_model/",
            "selftext": "I'm a complete noob with this, I just installed KoboldAI and I wanted to try some models, I've got a GGUF model and put it on the models folder, however, when I try to load it, nothing shows up, what are the correct steps to do it?",
            "comments": [
                {
                    "id": "lx0bmse",
                    "author": "SoundHole",
                    "body": "Forgive me if this is obvious, but Kobolcpp might be what you're looking for?\n\nhttps://github.com/LostRuins/koboldcpp\n\nIf you have that and not KoboldAI, click the blue 'Browse' button under Model near the bottom of the GUI and then browse to wherever you placed the gguf and select it. That should be that.",
                    "score": 3,
                    "upvotes": 3,
                    "downvotes": 0,
                    "created_utc": 1731544031.0,
                    "parent_id": "t3_1gqrx7m",
                    "link_id": "t3_1gqrx7m",
                    "permalink": "/r/KoboldAI/comments/1gqrx7m/loading_gguf_model/lx0bmse/"
                },
                {
                    "id": "lx0c40f",
                    "author": "Azmort_Red",
                    "body": "Thank you!, as I said, I'm a total noob, I'm just starting to learn everything about this, so yes, this seems to be what I was looking for.",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1731544200.0,
                    "parent_id": "t1_lx0bmse",
                    "link_id": "t3_1gqrx7m",
                    "permalink": "/r/KoboldAI/comments/1gqrx7m/loading_gguf_model/lx0c40f/"
                },
                {
                    "id": "lx0cp4o",
                    "author": "SoundHole",
                    "body": "Happy to help. I made the same mistake way back when, that's why I suspected you may have done the same thing lol. Happy tinkering.",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1731544404.0,
                    "parent_id": "t1_lx0c40f",
                    "link_id": "t3_1gqrx7m",
                    "permalink": "/r/KoboldAI/comments/1gqrx7m/loading_gguf_model/lx0cp4o/"
                },
                {
                    "id": "lx0kzsn",
                    "author": "gnat_outta_hell",
                    "body": "If you're using an Nvidia card download Koboldcpp - the CUDA variant. I saw a nearly 4x speed increase over Kobold AI and Kobold United.",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1731547360.0,
                    "parent_id": "t1_lx0c40f",
                    "link_id": "t3_1gqrx7m",
                    "permalink": "/r/KoboldAI/comments/1gqrx7m/loading_gguf_model/lx0kzsn/"
                },
                {
                    "id": "lx0os3o",
                    "author": "Azmort_Red",
                    "body": "Yes, that's what I'm using now and it works as expected, thank you ;)",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1731548706.0,
                    "parent_id": "t1_lx0kzsn",
                    "link_id": "t3_1gqrx7m",
                    "permalink": "/r/KoboldAI/comments/1gqrx7m/loading_gguf_model/lx0os3o/"
                }
            ]
        },
        {
            "id": "1gq9i59",
            "title": "Kobold Ai Lite, something wrong when I lunch it on phone. ",
            "author": "-Psycho_Punk-",
            "score": 2,
            "upvotes": 2,
            "downvotes": 0,
            "num_comments": 0,
            "created_utc": 1731491896.0,
            "url": "https://www.reddit.com/r/KoboldAI/comments/1gq9i59/kobold_ai_lite_something_wrong_when_i_lunch_it_on/",
            "permalink": "/r/KoboldAI/comments/1gq9i59/kobold_ai_lite_something_wrong_when_i_lunch_it_on/",
            "selftext": "When I lunch Kobold on phone, it always happens without connecting to network ( happening around 3 weeks). ",
            "comments": []
        },
        {
            "id": "1gq1xg7",
            "title": "Is it possible to make a truly loyal character?",
            "author": "GoodLuckBlessYou",
            "score": 18,
            "upvotes": 18,
            "downvotes": 0,
            "num_comments": 12,
            "created_utc": 1731463090.0,
            "url": "https://www.reddit.com/r/KoboldAI/comments/1gq1xg7/is_it_possible_to_make_a_truly_loyal_character/",
            "permalink": "/r/KoboldAI/comments/1gq1xg7/is_it_possible_to_make_a_truly_loyal_character/",
            "selftext": "NSFW content.\n\nI've tried many websites before, but, no matter how much I insist on a character's description that she's loyal, or that she's disgusted at the thought of being with someone else, still they always fall when I introduce a handsome guy in the story. \n\nI've tried some models and characters on kobold. For models, the one that suited my PC specs the best was Fimbulvetr 11B v2. With it, I tried some characters that had as their whole premise not to cheat. Yet...\n\nFailed and failed over again. \n\nI just wanna dream about having a girlfriend that will push away a guy even if he's a charming one ðŸ¥²\n\nAny tips on how to make this happen?",
            "comments": [
                {
                    "id": "lwuo5az",
                    "author": "StoopPizzaGoop",
                    "body": "What's your card formate? Hard to fix it without seeing the info",
                    "score": 9,
                    "upvotes": 9,
                    "downvotes": 0,
                    "created_utc": 1731463610.0,
                    "parent_id": "t3_1gq1xg7",
                    "link_id": "t3_1gq1xg7",
                    "permalink": "/r/KoboldAI/comments/1gq1xg7/is_it_possible_to_make_a_truly_loyal_character/lwuo5az/"
                },
                {
                    "id": "lwwib2g",
                    "author": "Blizado",
                    "body": "You could try to use very different example dialogs where the AI pushes other guys away. That also could help. But there is always a possibility to get an AI into doing something it shouldn't.",
                    "score": 2,
                    "upvotes": 2,
                    "downvotes": 0,
                    "created_utc": 1731498218.0,
                    "parent_id": "t3_1gq1xg7",
                    "link_id": "t3_1gq1xg7",
                    "permalink": "/r/KoboldAI/comments/1gq1xg7/is_it_possible_to_make_a_truly_loyal_character/lwwib2g/"
                },
                {
                    "id": "lwwx8y1",
                    "author": "Perfect_Ad9953",
                    "body": "No. Ai is not a video game, it doesn't track characters stats. Basically, don't tempt AI, it's will treat any development as a \"lead\". Especially on 11B.",
                    "score": 2,
                    "upvotes": 2,
                    "downvotes": 0,
                    "created_utc": 1731504909.0,
                    "parent_id": "t3_1gq1xg7",
                    "link_id": "t3_1gq1xg7",
                    "permalink": "/r/KoboldAI/comments/1gq1xg7/is_it_possible_to_make_a_truly_loyal_character/lwwx8y1/"
                },
                {
                    "id": "lwxnyqa",
                    "author": "Original-Success-784",
                    "body": "My general tips:  \n\\- write your own ModelCard, with your own rules and traits.  \n\\- write your own lorebook. It will help the character to stay in their role.  \n\\- Koboldcpp is okay for playing around, but if you want more consistent results, you can use [SillyTavern](https://github.com/SillyTavern/SillyTavern) or have a look at r/SillyTavernAI.  \n\\- In Koboldcpp are two options:\n\n1. press --> Context --> Author's Note --> \\[insert text belox\\]   **OR**\n2. press --> Context --> WorldInfo --> +add -->  \\[insert text belox\\] {{char}} is in love with {{user}}, {{char}} is loyal, {{char}} has only eyes for {{user}}\n\nMaybe this will help you.",
                    "score": 2,
                    "upvotes": 2,
                    "downvotes": 0,
                    "created_utc": 1731514082.0,
                    "parent_id": "t3_1gq1xg7",
                    "link_id": "t3_1gq1xg7",
                    "permalink": "/r/KoboldAI/comments/1gq1xg7/is_it_possible_to_make_a_truly_loyal_character/lwxnyqa/"
                },
                {
                    "id": "lwuvxqi",
                    "author": "GoodLuckBlessYou",
                    "body": "The last one I tried was this one:\n\n[https://chub.ai/characters/gyatt69420/kayleigh-956613bd](https://chub.ai/characters/gyatt69420/kayleigh-956613bd)\n\nBut I confess on this one I pushed her limits a lot, instead of just casually adding a male character. I was curious if she was truly anti-NTR as the tags and description say. After the test, I'd say it's *quite* hard for her to cheat, but still happens. But I guess that, because of the AI temperature, the algorithm will try to create some randomness by going out of the personality... And, once it creates just one post of the character being horny for the bad thing, the next replies will have to incorporate it, leading to a downhill. Just one failure can lead to a great catastrophe.\n\nPS: now that I think about... the Fimbulvetr version I have was downloaded from Lewdiculous. Maybe they trained the AI with texts about cheating and stuff.",
                    "score": 3,
                    "upvotes": 3,
                    "downvotes": 0,
                    "created_utc": 1731466400.0,
                    "parent_id": "t1_lwuo5az",
                    "link_id": "t3_1gq1xg7",
                    "permalink": "/r/KoboldAI/comments/1gq1xg7/is_it_possible_to_make_a_truly_loyal_character/lwuvxqi/"
                },
                {
                    "id": "lwv34p9",
                    "author": "StoopPizzaGoop",
                    "body": "The card is in an obsolete formate. It also has her personality listed as 'seductive' and 'sadistic', and then any information about her not cheating is dumped in a large body of text. The LLM isn't magic. As context gets used up it will forget stuff and it's better to have information listed clearly for the LLM to find. \n\nIf you're pushing the LLM to cheat it's going to give in. You can add a system prompt that explicitly states that the character will not cheat. Don't feed the model a negative like 'never'.",
                    "score": 8,
                    "upvotes": 8,
                    "downvotes": 0,
                    "created_utc": 1731469109.0,
                    "parent_id": "t1_lwuvxqi",
                    "link_id": "t3_1gq1xg7",
                    "permalink": "/r/KoboldAI/comments/1gq1xg7/is_it_possible_to_make_a_truly_loyal_character/lwv34p9/"
                },
                {
                    "id": "lwv3i1a",
                    "author": "GoodLuckBlessYou",
                    "body": "Thank you a lot for taking your time to check this stuff. Do you know a card that does this job well?",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1731469256.0,
                    "parent_id": "t1_lwv34p9",
                    "link_id": "t3_1gq1xg7",
                    "permalink": "/r/KoboldAI/comments/1gq1xg7/is_it_possible_to_make_a_truly_loyal_character/lwv3i1a/"
                },
                {
                    "id": "lx22wlc",
                    "author": "VascoPtk",
                    "body": "Is W++ obsolete now? Then what would you recommend for localhosted llamas currently? Please share your methods.",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1731571972.0,
                    "parent_id": "t1_lwv34p9",
                    "link_id": "t3_1gq1xg7",
                    "permalink": "/r/KoboldAI/comments/1gq1xg7/is_it_possible_to_make_a_truly_loyal_character/lx22wlc/"
                },
                {
                    "id": "lwv4pzf",
                    "author": "StoopPizzaGoop",
                    "body": "Not on the top of my head. If you're trying to see if they'll cheat you're going to just get the same result. The AI is an advanced auto complete. It doesn't actually like you. It's just guesses what you want it do by reading the prompt and what you write. \n\nYou can gaslight a character into thinking gravity isn't real if you try hard enough.",
                    "score": 12,
                    "upvotes": 12,
                    "downvotes": 0,
                    "created_utc": 1731469740.0,
                    "parent_id": "t1_lwv3i1a",
                    "link_id": "t3_1gq1xg7",
                    "permalink": "/r/KoboldAI/comments/1gq1xg7/is_it_possible_to_make_a_truly_loyal_character/lwv4pzf/"
                },
                {
                    "id": "lx2hdr2",
                    "author": "StoopPizzaGoop",
                    "body": "Plist, JED, or even JSON are better formats.",
                    "score": 2,
                    "upvotes": 2,
                    "downvotes": 0,
                    "created_utc": 1731581664.0,
                    "parent_id": "t1_lx22wlc",
                    "link_id": "t3_1gq1xg7",
                    "permalink": "/r/KoboldAI/comments/1gq1xg7/is_it_possible_to_make_a_truly_loyal_character/lx2hdr2/"
                },
                {
                    "id": "lww873h",
                    "author": "Touitoui",
                    "body": ">You can gaslight a character into thinking gravity isn't real if you try hard enough.\n\nWait, isn't KoboldAI made with C++?   \nShouldn't [this only work with Python](https://xkcd.com/353/)?",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1731492031.0,
                    "parent_id": "t1_lwv4pzf",
                    "link_id": "t3_1gq1xg7",
                    "permalink": "/r/KoboldAI/comments/1gq1xg7/is_it_possible_to_make_a_truly_loyal_character/lww873h/"
                },
                {
                    "id": "lwx6jjw",
                    "author": "None",
                    "body": "[deleted]",
                    "score": 2,
                    "upvotes": 2,
                    "downvotes": 0,
                    "created_utc": 1731508380.0,
                    "parent_id": "t1_lww873h",
                    "link_id": "t3_1gq1xg7",
                    "permalink": "/r/KoboldAI/comments/1gq1xg7/is_it_possible_to_make_a_truly_loyal_character/lwx6jjw/"
                },
                {
                    "id": "lwxbz6c",
                    "author": "Touitoui",
                    "body": "I know, that was a joke tied to the Xkcd comic I linked ^^'\n(A python joke, because \"you can do anything with Python\")",
                    "score": 2,
                    "upvotes": 2,
                    "downvotes": 0,
                    "created_utc": 1731510250.0,
                    "parent_id": "t1_lwx6jjw",
                    "link_id": "t3_1gq1xg7",
                    "permalink": "/r/KoboldAI/comments/1gq1xg7/is_it_possible_to_make_a_truly_loyal_character/lwxbz6c/"
                }
            ]
        },
        {
            "id": "1gptrkw",
            "title": "I averaged the weights of the best open sourced coding models \"pretrained\" and \"finetuned\" weights. The results are really good.",
            "author": "Rombodawg",
            "score": 9,
            "upvotes": 9,
            "downvotes": 0,
            "num_comments": 0,
            "created_utc": 1731441302.0,
            "url": "https://www.reddit.com/r/KoboldAI/comments/1gptrkw/i_averaged_the_weights_of_the_best_open_sourced/",
            "permalink": "/r/KoboldAI/comments/1gptrkw/i_averaged_the_weights_of_the_best_open_sourced/",
            "selftext": "The models are released here, because thats what everyone wants to see first:\n\n\\- [https://huggingface.co/collections/rombodawg/rombos-coder-v25-67331272e3afd0ba9cd5d031](https://huggingface.co/collections/rombodawg/rombos-coder-v25-67331272e3afd0ba9cd5d031)\n\nBut basically what my method does is combine the weights of the finetuned and pretrained models to reduce the catastrophic forgetting, as its called, during finetuning. I call my method \"Continuous Finetuning\" And ill link the write up bellow. So far this has been the highest quality coding model (The 32b version) that ive made so far, besides possibly the (Rombos-LLM-V2.5-Qwen-72b) model.\n\nHere is the write up mentioned above:\n\n\\- [https://docs.google.com/document/d/1OjbjU5AOz4Ftn9xHQrX3oFQGhQ6RDUuXQipnQ9gn6tU/edit?usp=sharing](https://docs.google.com/document/d/1OjbjU5AOz4Ftn9xHQrX3oFQGhQ6RDUuXQipnQ9gn6tU/edit?usp=sharing)\n\nAnd here is the method I used for merging the models if you want to skip to the good part:\n\n    models:\n      - model: ./models/Qwen2.5-Coder-32B-Instruct\n        parameters:\n          weight: 1\n          density: 1\n    merge_method: ties\n    base_model: ./models/Qwen2.5-Coder-32B\n    parameters:\n      weight: 1\n      density: 1\n      normalize: true\n      int8_mask: false\n    dtype: bfloat16\n\nAnyway if you have any coding needs the 14b and 32b models should be some of the best coding models out there as far as locally ran open source models with apache 2.0 licenses.",
            "comments": []
        },
        {
            "id": "1gowwhz",
            "title": "Token length generation ",
            "author": "Sindre_Lovvold",
            "score": 3,
            "upvotes": 3,
            "downvotes": 0,
            "num_comments": 4,
            "created_utc": 1731344121.0,
            "url": "https://www.reddit.com/r/KoboldAI/comments/1gowwhz/token_length_generation/",
            "permalink": "/r/KoboldAI/comments/1gowwhz/token_length_generation/",
            "selftext": "Iâ€™m trying to use the koboldcpp api through an external program I wrote and for some reason Iâ€™m only getting 180 tokens generated (180/180) but if I switch over to the web ui with the same prompt Iâ€™m getting (rnd/1024). \n\nDoes anyone know how to change the max number of generated tokens? Iâ€™ve tried parsing max_new_tokens with the prompt but itâ€™s always outputting the same 180 in the api. ",
            "comments": [
                {
                    "id": "lwmbyvo",
                    "author": "Tastentelefon",
                    "body": "I think you are looking for \"max_length\"",
                    "score": 4,
                    "upvotes": 4,
                    "downvotes": 0,
                    "created_utc": 1731350164.0,
                    "parent_id": "t3_1gowwhz",
                    "link_id": "t3_1gowwhz",
                    "permalink": "/r/KoboldAI/comments/1gowwhz/token_length_generation/lwmbyvo/"
                },
                {
                    "id": "lwlwnn5",
                    "author": "FamousM1",
                    "body": "Could you share the code you wrote?",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1731345504.0,
                    "parent_id": "t3_1gowwhz",
                    "link_id": "t3_1gowwhz",
                    "permalink": "/r/KoboldAI/comments/1gowwhz/token_length_generation/lwlwnn5/"
                },
                {
                    "id": "lwmqdqq",
                    "author": "henk717",
                    "body": "The API documentation is available if you add /api to a working Kobold link and visit it in the browser for example [https://koboldai-koboldcpp-tiefighter.hf.space/api](https://koboldai-koboldcpp-tiefighter.hf.space/api) .  \nYou want to increase max\\_context\\_length as that is what controls how much it may ingest.",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1731354550.0,
                    "parent_id": "t3_1gowwhz",
                    "link_id": "t3_1gowwhz",
                    "permalink": "/r/KoboldAI/comments/1gowwhz/token_length_generation/lwmqdqq/"
                },
                {
                    "id": "lwojiqi",
                    "author": "Sindre_Lovvold",
                    "body": "Thanks. That was the one I was looking for. Donâ€™t know how I missed it.",
                    "score": 3,
                    "upvotes": 3,
                    "downvotes": 0,
                    "created_utc": 1731375951.0,
                    "parent_id": "t1_lwmbyvo",
                    "link_id": "t3_1gowwhz",
                    "permalink": "/r/KoboldAI/comments/1gowwhz/token_length_generation/lwojiqi/"
                }
            ]
        },
        {
            "id": "1gnuphb",
            "title": "Do recent models perform well with no-card text completion?",
            "author": "ExtremeCricket980",
            "score": 3,
            "upvotes": 3,
            "downvotes": 0,
            "num_comments": 2,
            "created_utc": 1731221017.0,
            "url": "https://www.reddit.com/r/KoboldAI/comments/1gnuphb/do_recent_models_perform_well_with_nocard_text/",
            "permalink": "/r/KoboldAI/comments/1gnuphb/do_recent_models_perform_well_with_nocard_text/",
            "selftext": "Hi y'all, I'm wanting to steer away from the character card based rp to no card text completion, but I don't know if the recent models (like NemoMix, Stheno, Celeste...) can do it well. \n\nDo they perform decently or is there another 'tier' of models that are better for this specific purpose?\n\nI'm asking because I always see that the models were trained to be better in 'sticking to the role' of the card, but does it break if there's no card?",
            "comments": [
                {
                    "id": "lwfrh7q",
                    "author": "Original-Success-784",
                    "body": "In r/LocalLLaMA you can find new (nontrained) models, update-news, tier-list and benchmarks.   \nYou can also use the trained Models it depends on what you want.  \nThe finetune of the models got different purpose, it depence on the dataset that is used.  \nFor example: more instructs to 'stick to the role' of the card and/or 'act more like a human' and/or 'specific roleplay examples'....  \nIf you use finetuned models without a character card, it won't break.",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1731258726.0,
                    "parent_id": "t3_1gnuphb",
                    "link_id": "t3_1gnuphb",
                    "permalink": "/r/KoboldAI/comments/1gnuphb/do_recent_models_perform_well_with_nocard_text/lwfrh7q/"
                },
                {
                    "id": "lwkhkak",
                    "author": "henk717",
                    "body": "Whats your use case for the no card text completions? Does it need to stay in character or do you want a more fact based anwer?",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1731327102.0,
                    "parent_id": "t3_1gnuphb",
                    "link_id": "t3_1gnuphb",
                    "permalink": "/r/KoboldAI/comments/1gnuphb/do_recent_models_perform_well_with_nocard_text/lwkhkak/"
                }
            ]
        },
        {
            "id": "1gnu7p2",
            "title": "Koboldcpp set user persona/bio",
            "author": "Astronomer3007",
            "score": 1,
            "upvotes": 1,
            "downvotes": 0,
            "num_comments": 2,
            "created_utc": 1731218959.0,
            "url": "https://www.reddit.com/r/KoboldAI/comments/1gnu7p2/koboldcpp_set_user_personabio/",
            "permalink": "/r/KoboldAI/comments/1gnu7p2/koboldcpp_set_user_personabio/",
            "selftext": "How to set user persona or user bio in koboldcpp. I can import character cards to chat, but want to define the user's persona too.",
            "comments": [
                {
                    "id": "lwfkdx4",
                    "author": "Original-Success-784",
                    "body": "If you want to enter user persona in Koboldcpp, you can try context-->Author's Note or World info.   \nSomething like in SillyTavern to define exactly your own persona or to switch a charactercard to persona is not available.",
                    "score": 2,
                    "upvotes": 2,
                    "downvotes": 0,
                    "created_utc": 1731256527.0,
                    "parent_id": "t3_1gnu7p2",
                    "link_id": "t3_1gnu7p2",
                    "permalink": "/r/KoboldAI/comments/1gnu7p2/koboldcpp_set_user_personabio/lwfkdx4/"
                },
                {
                    "id": "lwi10ee",
                    "author": "Astronomer3007",
                    "body": "Will try that. Hope they add user persona setting in the future releases of koboldcpp.",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1731283989.0,
                    "parent_id": "t1_lwfkdx4",
                    "link_id": "t3_1gnu7p2",
                    "permalink": "/r/KoboldAI/comments/1gnu7p2/koboldcpp_set_user_personabio/lwi10ee/"
                }
            ]
        },
        {
            "id": "1gkvucr",
            "title": "KoboldAI Lite. Has anyone else lately been getting the Example Messages/Dialogue included in the AutoGenerate Memory?",
            "author": "xenodragon20",
            "score": 3,
            "upvotes": 3,
            "downvotes": 0,
            "num_comments": 0,
            "created_utc": 1730890579.0,
            "url": "https://www.reddit.com/r/KoboldAI/comments/1gkvucr/koboldai_lite_has_anyone_else_lately_been_getting/",
            "permalink": "/r/KoboldAI/comments/1gkvucr/koboldai_lite_has_anyone_else_lately_been_getting/",
            "selftext": "Lately i have this happening and i wonder if this is happening to anyone else? It does not seem to be related to a single model. I put *** between each message.",
            "comments": []
        },
        {
            "id": "1gkfur6",
            "title": "Infinit Craft with Kobold",
            "author": "Salt-Frosting-7930",
            "score": 1,
            "upvotes": 1,
            "downvotes": 0,
            "num_comments": 0,
            "created_utc": 1730836985.0,
            "url": "https://www.reddit.com/r/KoboldAI/comments/1gkfur6/infinit_craft_with_kobold/",
            "permalink": "/r/KoboldAI/comments/1gkfur6/infinit_craft_with_kobold/",
            "selftext": "    Infinit Craft with Kobold\n    The code is based on https://github.com/githubpradeep/notebooks/blob/main/Infinite%20craft%20game-1.ipynb\n    tested with https://huggingface.co/QuantFactory/Meta-Llama-3-8B-Instruct-GGUF/blob/main/Meta-Llama-3-8B-Instruct.Q8_0.gguf\n    \n    from langchain.output_parsers import PydanticOutputParser\n    from langchain.prompts import PromptTemplate\n    from langchain_core.messages import HumanMessage\n    from pydantic import BaseModel, Field\n    from langchain_community.llms import KoboldApiLLM\n    \n    \n    class Craft(BaseModel):\n        word: str = Field(description=\"the new word which was created\")\n    \n    \n    class Crafter:\n        def __init__(self, local=False):\n    \n            llm = KoboldApiLLM(endpoint=\"http://localhost:5001/\", temperature=0.8, max_length=48)\n            parser = PydanticOutputParser(pydantic_object=Craft)\n    \n            prompt = PromptTemplate(\n                template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n                input_variables=[\"query\"],\n                partial_variables={\"format_instructions\": parser.get_format_instructions()},\n            )\n    \n            chain = prompt | llm | parser\n            self.chain = chain\n    \n        def craft(self, word1, word2):\n            while True:\n                try:\n    \n                    ans = self.chain.invoke({\"query\": f'''You are an AI that combines words to create meaningful new words. Always respond as JSON with no unnecessary information.\n                    For example:\n                    Earth + Wind = Dust\n                    Fire + Water = Steam\n                    Earth + Water = Plant\n                    Dust + Earth = Planet\n                    Plant + Steam = Tea\n                    Planet + Wind = Storm\n                    Storm + Tea = Tempest\n                    Plant + Tempest = Tree\n                    Dust + Tree = Wood\n                    Fire + Wood = Campfire\n                    Dust + Water = Mud\n                    Campfire + Earth = Charcoal\n                    Charcoal + Mud = Fossil\n                    Water + Fire = Steam\n                    Wind + Steam = Cloud\n                    {word1} + {word2} ='''})\n                    return ans.word\n                except:\n                    continue\n    import pygame\n    import sys\n    \n    \n    pygame.init()\n    \n    screen_width, screen_height = 1024, 576\n    screen = pygame.display.set_mode((screen_width, screen_height))\n    pygame.display.set_caption('Infinite Craft')\n    \n    \n    background_color = (255, 255, 255)\n    text_color = (0, 0, 0)\n    box_color = (200, 200, 200)\n    sidebar_color = (100, 100, 100)\n    element_color = (160, 160, 160)\n    element_text_color = text_color  # (255, 255, 255)\n    font_size = 24\n    font = pygame.font.Font(None, font_size)\n    \n    # Element combinations\n    combinations = {\n        (\"Fire\", \"Water\"): \"Steam\",\n        # Add more combinations as needed\n    }\n    \n    sidebar_elements = [\"Fire\", \"Water\", \"Earth\", \"Wind\", \"Soil\", \"Seed\"]\n    sidebar_rects = []\n    \n    elements = []\n    \n    \n    class TextBox:\n        def __init__(self, text, pos, from_sidebar=False):\n            self.text = text\n            self.pos = pos\n            self.rect = pygame.Rect(pos[0], pos[1], 120, 40)\n            self.dragging = False\n            self.from_sidebar = from_sidebar\n    \n        def draw(self, screen):\n            draw_bordered_rounded_rect(screen, self.rect, box_color, box_color, 3, 5)\n            # pygame.draw.rect(screen, element_color, self.rect,1,1)\n            text_surface = font.render(self.text, True, element_text_color)\n            screen.blit(text_surface, (self.rect.x + 10, self.rect.y + 10))\n    \n        def handle_event(self, event):\n            if event.type == pygame.MOUSEBUTTONDOWN:\n                if self.rect.collidepoint(event.pos):\n                    self.dragging = True\n                    self.offset_x = self.rect.x - event.pos[0]\n                    self.offset_y = self.rect.y - event.pos[1]\n    \n            elif event.type == pygame.MOUSEBUTTONUP:\n                self.dragging = False\n                if self.from_sidebar:\n                    # Snap back to sidebar if it's not dragged into the main area\n                    if not (200 < self.rect.x < screen_width):\n                        elements.remove(self)\n                    else:\n                        self.from_sidebar = False\n            elif event.type == pygame.MOUSEMOTION:\n                if self.dragging:\n                    self.rect.x = event.pos[0] + self.offset_x\n                    self.rect.y = event.pos[1] + self.offset_y\n    \n    \n    def init_sidebar():\n        y = 100\n        sidebar_rects = []\n        for element in sidebar_elements:\n            rect = pygame.Rect(25, y, 150, 30)\n            sidebar_rects.append(rect)\n            y += 50\n        return sidebar_rects\n    \n    \n    sidebar_rects = init_sidebar()\n    \n    import pygame\n    import sys\n    \n    pygame.init()\n    \n    screen_width, screen_height = 800, 600\n    screen = pygame.display.set_mode((screen_width, screen_height))\n    \n    WHITE = (255, 255, 255)\n    GREY = (200, 200, 200)\n    DARK_GREY = (100, 100, 100)\n    \n    font = pygame.font.Font(None, 30)\n    \n    sidebar_width = 200\n    sidebar_content_height = 5000  # Example content height\n    scroll_y = 0  # Scroll position\n    import pygame.gfxdraw\n    \n    \n    def draw_rounded_rect(surface, rect, color, corner_radius):\n        \n    ''' Draw a rectangle with rounded corners.\n        Would prefer this:\n            pygame.draw.rect(surface, color, rect, border_radius=corner_radius)\n        but this option is not yet supported in my version of pygame so do it ourselves.\n        We use anti-aliased circles to make the corners smoother\n        '''\n        \n    if rect.width < 2 * corner_radius or rect.height < 2 * corner_radius:\n            raise ValueError(\n                f\"Both height (rect.height) and width (rect.width) must be > 2 * corner radius ({corner_radius})\")\n    \n        # need to use anti aliasing circle drawing routines to smooth the corners\n        pygame.gfxdraw.aacircle(surface, rect.left + corner_radius, rect.top + corner_radius, corner_radius, color)\n        pygame.gfxdraw.aacircle(surface, rect.right - corner_radius - 1, rect.top + corner_radius, corner_radius, color)\n        pygame.gfxdraw.aacircle(surface, rect.left + corner_radius, rect.bottom - corner_radius - 1, corner_radius, color)\n        pygame.gfxdraw.aacircle(surface, rect.right - corner_radius - 1, rect.bottom - corner_radius - 1, corner_radius,\n                                color)\n    \n        pygame.gfxdraw.filled_circle(surface, rect.left + corner_radius, rect.top + corner_radius, corner_radius, color)\n        pygame.gfxdraw.filled_circle(surface, rect.right - corner_radius - 1, rect.top + corner_radius, corner_radius,\n                                     color)\n        pygame.gfxdraw.filled_circle(surface, rect.left + corner_radius, rect.bottom - corner_radius - 1, corner_radius,\n                                     color)\n        pygame.gfxdraw.filled_circle(surface, rect.right - corner_radius - 1, rect.bottom - corner_radius - 1,\n                                     corner_radius, color)\n    \n        rect_tmp = pygame.Rect(rect)\n    \n        rect_tmp.width -= 2 * corner_radius\n        rect_tmp.center = rect.center\n        pygame.draw.rect(surface, color, rect_tmp)\n    \n        rect_tmp.width = rect.width\n        rect_tmp.height -= 2 * corner_radius\n        rect_tmp.center = rect.center\n        pygame.draw.rect(surface, color, rect_tmp)\n    \n    \n    def draw_bordered_rounded_rect(surface, rect, color, border_color, corner_radius, border_thickness):\n        if corner_radius < 0:\n            raise ValueError(f\"border radius ({corner_radius}) must be >= 0\")\n    \n        rect_tmp = pygame.Rect(rect)\n        center = rect_tmp.center\n    \n        if border_thickness:\n            if corner_radius <= 0:\n                pygame.draw.rect(surface, border_color, rect_tmp)\n            else:\n                draw_rounded_rect(surface, rect_tmp, border_color, corner_radius)\n    \n            rect_tmp.inflate_ip(-2 * border_thickness, -2 * border_thickness)\n            inner_radius = corner_radius - border_thickness + 1\n        else:\n            inner_radius = corner_radius\n    \n        if inner_radius <= 0:\n            pygame.draw.rect(surface, color, rect_tmp)\n        else:\n            draw_rounded_rect(surface, rect_tmp, color, inner_radius)\n    \n    \n    def draw_sidebar(screen, sidebar_rects, scroll_y):\n        pygame.draw.rect(screen, sidebar_color, [0, 0, 200, screen_height], 1, 1)\n        scrollable_area = pygame.Rect(0, scroll_y, sidebar_width, sidebar_content_height)\n    \n        for idx, rect in enumerate(sidebar_rects):\n            rect.y += scrollable_area.y\n            draw_bordered_rounded_rect(screen, rect, box_color, box_color, 3, 5)\n            text_surface = font.render(sidebar_elements[idx], True, text_color)\n            screen.blit(text_surface, (rect.x + 5, rect.y + 5))\n    \n    \n    crafter = Crafter()\n    \n    \n    def merge_elements(elements, sidebar_elements):\n        for i, elem1 in enumerate(elements):\n            for j, elem2 in enumerate(elements):\n                if i != j:  # Ensure not checking the same element\n                    if elem1.rect.colliderect(elem2.rect):  # Check for collision\n                        if not elem1.from_sidebar and not elem2.from_sidebar:\n                            # Concatenate the text to create a new element\n                            new_text = crafter.craft(elem1.text, elem2.text)\n                            # new_text = elem1.text+ elem2.text\n                            if new_text not in sidebar_elements:\n                                sidebar_elements.append(new_text)\n                            new_pos = (elem1.pos[0], elem1.pos[1])\n                            # Create the new element and add it to the list\n                            new_element = TextBox(new_text, elem1.pos)\n                            new_element.rect.x = elem1.rect.x\n                            new_element.rect.y = elem1.rect.y\n                            elements.append(new_element)\n                            elements.remove(elem1)\n                            elements.remove(elem2)\n                            return\n    def main():\n        offset = 0\n        scroll_y = 0\n        scroll_speed = 1\n        clock = pygame.time.Clock()\n        running = True\n        running = True\n        clock = pygame.time.Clock()\n    \n        while running:\n            for event in pygame.event.get():\n                if event.type == pygame.QUIT:\n                    running = False\n                elif event.type == pygame.MOUSEBUTTONDOWN:\n    \n                    if event.button == 1:\n                        for idx, rect in enumerate(sidebar_rects):\n                            if rect.collidepoint(event.pos):\n                                elements.append(TextBox(sidebar_elements[idx], event.pos, from_sidebar=True))\n                                break\n                elif event.type == pygame.MOUSEWHEEL:\n                    scroll_y += event.y * 10\n                    # Limit scrolling\n                    scroll_y = min(0, max(-(sidebar_content_height - screen_height), scroll_y))\n    \n                for element in elements:\n                    element.handle_event(event)\n    \n            sidebar_rects = init_sidebar()\n    \n            merge_elements(elements, sidebar_elements)\n    \n            screen.fill(background_color)\n            draw_sidebar(screen, sidebar_rects, scroll_y)\n    \n            for element in elements:\n                element.draw(screen)\n    \n            pygame.display.flip()\n            clock.tick(30)\n    \n        pygame.quit()\n        sys.exit()\n    \n    \n    if __name__ == '__main__':\n        main()",
            "comments": []
        },
        {
            "id": "1gken5t",
            "title": "How to set custom context size?",
            "author": "Deep-Yoghurt878",
            "score": 5,
            "upvotes": 5,
            "downvotes": 0,
            "num_comments": 3,
            "created_utc": 1730833919.0,
            "url": "https://www.reddit.com/r/KoboldAI/comments/1gken5t/how_to_set_custom_context_size/",
            "permalink": "/r/KoboldAI/comments/1gken5t/how_to_set_custom_context_size/",
            "selftext": "Kobold allows to set only 16k and 24k and there is no option in between. How can I set 20k context?\n\nhttps://preview.redd.it/pkprbmwut4zd1.png?width=550&format=png&auto=webp&s=5f4ce7027193a78e69a09558dc4f6d9f89a249d6\n\n",
            "comments": [
                {
                    "id": "lvkuxqb",
                    "author": "mamelukturbo",
                    "body": "you can do it from commandline but it's not advised:\n\n\\--contextsize \\[256,512,1024,2048,3072,4096,6144,8192,12288,16384,24576,32768,49152,65536,98304,131072\\]\n\nControls the memory allocated for maximum context size, only change if you need more RAM for big contexts. (default 4096). Supported values are \\[256,512,1024,2048,3072,4096,6144,8192,12288,16384,24576,32768,49152,65536,98304,131072\\]. IF YOU USE ANYTHING ELSE YOU ARE ON YOUR OWN.",
                    "score": 5,
                    "upvotes": 5,
                    "downvotes": 0,
                    "created_utc": 1730837173.0,
                    "parent_id": "t3_1gken5t",
                    "link_id": "t3_1gken5t",
                    "permalink": "/r/KoboldAI/comments/1gken5t/how_to_set_custom_context_size/lvkuxqb/"
                },
                {
                    "id": "lvkr2sl",
                    "author": "ffgg333",
                    "body": "Try koboldcpp frankenstein. It has support for custom context size :\n\n\n\nhttps://github.com/Nexesenex/croco.cpp/releases",
                    "score": 2,
                    "upvotes": 2,
                    "downvotes": 0,
                    "created_utc": 1730836030.0,
                    "parent_id": "t3_1gken5t",
                    "link_id": "t3_1gken5t",
                    "permalink": "/r/KoboldAI/comments/1gken5t/how_to_set_custom_context_size/lvkr2sl/"
                },
                {
                    "id": "lvkou7n",
                    "author": "FamousM1",
                    "body": "Its based on multiples of 512/1024 so if you only wanted to use 20k you'd set it to 24,576 and set the max context size in your UI to 20,000\n       \nPersonally I'd stick to 16384 or 32768 because I've found that the values that are in between 4096 and 8192 and 16384 like 6000 and 12000 makes the model get a little wonky. \n             \nPro tip: If you are using a higher max context size and are just about to max out your VRAM, set the BLAS batch size a little lower, like 256 instead of 512 because it'll have the amount of used VRAM for context which can save gigabytes at the expensive of a few seconds longer processing speed",
                    "score": 4,
                    "upvotes": 4,
                    "downvotes": 0,
                    "created_utc": 1730835373.0,
                    "parent_id": "t3_1gken5t",
                    "link_id": "t3_1gken5t",
                    "permalink": "/r/KoboldAI/comments/1gken5t/how_to_set_custom_context_size/lvkou7n/"
                }
            ]
        },
        {
            "id": "1gk9ges",
            "title": "You guys should add vertex ai support",
            "author": "Nicolo2524",
            "score": 0,
            "upvotes": 0,
            "downvotes": 0,
            "num_comments": 2,
            "created_utc": 1730820943.0,
            "url": "https://www.reddit.com/r/KoboldAI/comments/1gk9ges/you_guys_should_add_vertex_ai_support/",
            "permalink": "/r/KoboldAI/comments/1gk9ges/you_guys_should_add_vertex_ai_support/",
            "selftext": "You guys should add vertex ai support or if you can tell me how to set it up on kobold ai It Will be appreciated, I want to use their Claude API since it seem cheaper on vertex.",
            "comments": [
                {
                    "id": "lvmpb8i",
                    "author": "henk717",
                    "body": "I'm not sure how easy it would be to add proper support but they do support OpenAI Chat Completions.  \nSo what you should be able to do on [koboldai.net](http://koboldai.net) is picking the OpenAI Compatible option and entering the correct API link. I don't quite know what that will be but their documentation for it is here : [https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/call-vertex-using-openai-library](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/call-vertex-using-openai-library) if its a valid OpenAI compatible link that option should work on our end.",
                    "score": 2,
                    "upvotes": 2,
                    "downvotes": 0,
                    "created_utc": 1730858490.0,
                    "parent_id": "t3_1gk9ges",
                    "link_id": "t3_1gk9ges",
                    "permalink": "/r/KoboldAI/comments/1gk9ges/you_guys_should_add_vertex_ai_support/lvmpb8i/"
                },
                {
                    "id": "lvphqvk",
                    "author": "Nicolo2524",
                    "body": "I can't seem to find a compatible openAI compatible API link... I really want to try out this or AWS but I don't understand how I'm supposed to make it work, It is not like groq, togetherai or any other API provider I can't seem to find the right link.",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1730900368.0,
                    "parent_id": "t1_lvmpb8i",
                    "link_id": "t3_1gk9ges",
                    "permalink": "/r/KoboldAI/comments/1gk9ges/you_guys_should_add_vertex_ai_support/lvphqvk/"
                }
            ]
        },
        {
            "id": "1gk79b9",
            "title": "Are Shortcut keys possible?",
            "author": "Virtual-End-9003",
            "score": 6,
            "upvotes": 6,
            "downvotes": 0,
            "num_comments": 0,
            "created_utc": 1730815039.0,
            "url": "https://www.reddit.com/r/KoboldAI/comments/1gk79b9/are_shortcut_keys_possible/",
            "permalink": "/r/KoboldAI/comments/1gk79b9/are_shortcut_keys_possible/",
            "selftext": "I doubt so but i figured i might as well ask, is there a way to bind keys to UI buttons? redo button for example? Like Alt+r or something, it could be slightly faster than hovering the mouse and clicking it, just a neat idea that popped in my head.",
            "comments": []
        },
        {
            "id": "1gjuzhi",
            "title": "can I use koboldcpp with google Collab ?",
            "author": "Username0copied",
            "score": 3,
            "upvotes": 3,
            "downvotes": 0,
            "num_comments": 3,
            "created_utc": 1730769540.0,
            "url": "https://www.reddit.com/r/KoboldAI/comments/1gjuzhi/can_i_use_koboldcpp_with_google_collab/",
            "permalink": "/r/KoboldAI/comments/1gjuzhi/can_i_use_koboldcpp_with_google_collab/",
            "selftext": "I want to be able to have everything be saved locally after being generated so it doesn't risk being lost all the time but I don't want to be the host, is there an easy way to do that ?",
            "comments": [
                {
                    "id": "lvhrcm6",
                    "author": "henk717",
                    "body": "Yes, https://koboldai.org/colabcpp KoboldCpp doesn't store anything server side. You download and load the saves trough json files on your own device. Saving is functional as long as you don't leave the page even if they shut you down.",
                    "score": 3,
                    "upvotes": 3,
                    "downvotes": 0,
                    "created_utc": 1730795261.0,
                    "parent_id": "t3_1gjuzhi",
                    "link_id": "t3_1gjuzhi",
                    "permalink": "/r/KoboldAI/comments/1gjuzhi/can_i_use_koboldcpp_with_google_collab/lvhrcm6/"
                },
                {
                    "id": "lvi5b60",
                    "author": "Username0copied",
                    "body": "is there any guide on how to do it?",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1730804549.0,
                    "parent_id": "t1_lvhrcm6",
                    "link_id": "t3_1gjuzhi",
                    "permalink": "/r/KoboldAI/comments/1gjuzhi/can_i_use_koboldcpp_with_google_collab/lvi5b60/"
                },
                {
                    "id": "lw8dcyr",
                    "author": "CooperDK",
                    "body": "Try checking the documentation",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1731149863.0,
                    "parent_id": "t1_lvi5b60",
                    "link_id": "t3_1gjuzhi",
                    "permalink": "/r/KoboldAI/comments/1gjuzhi/can_i_use_koboldcpp_with_google_collab/lw8dcyr/"
                }
            ]
        },
        {
            "id": "1gj9rmu",
            "title": "How do I run the KoboldCPP MacOS binary?",
            "author": "real-joedoe07",
            "score": 2,
            "upvotes": 2,
            "downvotes": 0,
            "num_comments": 4,
            "created_utc": 1730709693.0,
            "url": "https://www.reddit.com/r/KoboldAI/comments/1gj9rmu/how_do_i_run_the_koboldcpp_macos_binary/",
            "permalink": "/r/KoboldAI/comments/1gj9rmu/how_do_i_run_the_koboldcpp_macos_binary/",
            "selftext": "I was thrilled to see there is finally a MacOs binary for KoboldCpp. However, I do not manage to execute it. Is it supposed to run in terminal? How so?\n\nI'm on an M2 with MacOs 15.1 ",
            "comments": [
                {
                    "id": "lvbny1a",
                    "author": "HadesThrowaway",
                    "body": "You need to set the binary as executable. Open a terminal, then navigate to the downloaded file and do \n\n`chmod +x koboldcpp-mac-arm64`\n\nFollowed by\n\n`./koboldcpp-mac-arm64`\n\nIn newer MacOS you may also have to whitelist it in security settings if it's blocked. Here's a video for that \n\nhttps://youtube.com/watch?v=NOW5dyA_JgY",
                    "score": 5,
                    "upvotes": 5,
                    "downvotes": 0,
                    "created_utc": 1730713259.0,
                    "parent_id": "t3_1gj9rmu",
                    "link_id": "t3_1gj9rmu",
                    "permalink": "/r/KoboldAI/comments/1gj9rmu/how_do_i_run_the_koboldcpp_macos_binary/lvbny1a/"
                },
                {
                    "id": "lvbjzpc",
                    "author": "Short-Sandwich-905",
                    "body": "BumpÂ ",
                    "score": -1,
                    "upvotes": -1,
                    "downvotes": 0,
                    "created_utc": 1730710451.0,
                    "parent_id": "t3_1gj9rmu",
                    "link_id": "t3_1gj9rmu",
                    "permalink": "/r/KoboldAI/comments/1gj9rmu/how_do_i_run_the_koboldcpp_macos_binary/lvbjzpc/"
                },
                {
                    "id": "lvbqtwd",
                    "author": "real-joedoe07",
                    "body": "Thank you!",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1730715222.0,
                    "parent_id": "t1_lvbny1a",
                    "link_id": "t3_1gj9rmu",
                    "permalink": "/r/KoboldAI/comments/1gj9rmu/how_do_i_run_the_koboldcpp_macos_binary/lvbqtwd/"
                },
                {
                    "id": "lvbquk7",
                    "author": "exclaim_bot",
                    "body": ">Thank you!\n\nYou're welcome!",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1730715234.0,
                    "parent_id": "t1_lvbqtwd",
                    "link_id": "t3_1gj9rmu",
                    "permalink": "/r/KoboldAI/comments/1gj9rmu/how_do_i_run_the_koboldcpp_macos_binary/lvbquk7/"
                }
            ]
        },
        {
            "id": "1ghzsdv",
            "title": "Kobolt cpp \"Server Busy\" issue",
            "author": "GraphBirdl",
            "score": 5,
            "upvotes": 5,
            "downvotes": 0,
            "num_comments": 4,
            "created_utc": 1730563533.0,
            "url": "https://www.reddit.com/r/KoboldAI/comments/1ghzsdv/kobolt_cpp_server_busy_issue/",
            "permalink": "/r/KoboldAI/comments/1ghzsdv/kobolt_cpp_server_busy_issue/",
            "selftext": "I'm 'trying' to become a Horde worker, but I can't quite manage with kobolt cpp  \nWhen I launch as a worker, everything at first goes without any errors, but as I get a request (Agnaistic), the console starts to spam with \"Server Busy - Not ready to generate\"  \nAnd yeah I already have tried to relaunch koboltcpp a few times and restart my computer\n\nHow do I fix this?\n\nhttps://preview.redd.it/wt1ikj4thiyd1.png?width=726&format=png&auto=webp&s=b5511f76f22217b65e9baaacefbe183ab57b11cc\n\nhttps://preview.redd.it/7isi23schiyd1.png?width=1584&format=png&auto=webp&s=28c0db4f07eda17a931a1259e3c5d8fdb7e89ee1",
            "comments": [
                {
                    "id": "lv1vcux",
                    "author": "henk717",
                    "body": "This is happening because your generations are taking to long.  \nFor Horde you want to host Q4 at minimum in something you can fully offload, so Q4 up to 30B on your GPU should be speedy and is a benefit to Horde. But a Q2 123B partial offload will be to slow to be useful.",
                    "score": 5,
                    "upvotes": 5,
                    "downvotes": 0,
                    "created_utc": 1730570435.0,
                    "parent_id": "t3_1ghzsdv",
                    "link_id": "t3_1ghzsdv",
                    "permalink": "/r/KoboldAI/comments/1ghzsdv/kobolt_cpp_server_busy_issue/lv1vcux/"
                },
                {
                    "id": "lv1xwtx",
                    "author": "GraphBirdl",
                    "body": "Iâ€™m very new to this field. Whatâ€™s Q2 and Q4?",
                    "score": 3,
                    "upvotes": 3,
                    "downvotes": 0,
                    "created_utc": 1730571261.0,
                    "parent_id": "t1_lv1vcux",
                    "link_id": "t3_1ghzsdv",
                    "permalink": "/r/KoboldAI/comments/1ghzsdv/kobolt_cpp_server_busy_issue/lv1xwtx/"
                },
                {
                    "id": "lv26qzs",
                    "author": "henk717",
                    "body": "Q2 means its a 2-bit model with low quality, Q4 is a 4-bit model which most people find acceptable. Its in the file name of the files you download.",
                    "score": 3,
                    "upvotes": 3,
                    "downvotes": 0,
                    "created_utc": 1730574045.0,
                    "parent_id": "t1_lv1xwtx",
                    "link_id": "t3_1ghzsdv",
                    "permalink": "/r/KoboldAI/comments/1ghzsdv/kobolt_cpp_server_busy_issue/lv26qzs/"
                },
                {
                    "id": "lv29k4x",
                    "author": "GraphBirdl",
                    "body": "I get it now, thank you!",
                    "score": 2,
                    "upvotes": 2,
                    "downvotes": 0,
                    "created_utc": 1730574935.0,
                    "parent_id": "t1_lv26qzs",
                    "link_id": "t3_1ghzsdv",
                    "permalink": "/r/KoboldAI/comments/1ghzsdv/kobolt_cpp_server_busy_issue/lv29k4x/"
                }
            ]
        },
        {
            "id": "1gezn8s",
            "title": "How do you set KoboldAI to run on a headless Linux server? ",
            "author": "National_Cod9546",
            "score": 6,
            "upvotes": 6,
            "downvotes": 0,
            "num_comments": 4,
            "created_utc": 1730221397.0,
            "url": "https://www.reddit.com/r/KoboldAI/comments/1gezn8s/how_do_you_set_koboldai_to_run_on_a_headless/",
            "permalink": "/r/KoboldAI/comments/1gezn8s/how_do_you_set_koboldai_to_run_on_a_headless/",
            "selftext": "As in the title. I have an Ubuntu 24.04 headless server with an RTX 4060 TI 16GB that I want to run AI stuff on. I can get Ollama with SillyTavernAI running on it without much issue. I would like to try switching to KoboldAI and see if that works faster or more coherent. But it keeps complaining about wanting a display. My googlefoo skills have been lacking at finding a good guide to run KoboldAI headless. \n\nOn a related note, what are the pros and cons between KoboldAI and Ollama? Whatever I use will need to support both roleplay and technical questions. ",
            "comments": [
                {
                    "id": "ludqs3c",
                    "author": "henk717",
                    "body": "You won't need a display but you do need it for the UI. Simply add --help to the command to see all the cli variables, at minimum you will need --model.\n\nKoboldCpp is portable so doesn't have a setup, has better context handling, more samplers, built in UI and optimizes in general for fictional use cases. Its also OpenAI compatible so it can be hooked up to UI's that have things like RAG if you need it. It will support both because if you don't use any additional data it gives you the raw model experience while if you add fictional context such as a character card you get a fictional experience.\n\nSillytavern for us is a scenario we specifically optimize for so when we add stuff we keep SillyTavern in mind.",
                    "score": 5,
                    "upvotes": 5,
                    "downvotes": 0,
                    "created_utc": 1730223141.0,
                    "parent_id": "t3_1gezn8s",
                    "link_id": "t3_1gezn8s",
                    "permalink": "/r/KoboldAI/comments/1gezn8s/how_do_you_set_koboldai_to_run_on_a_headless/ludqs3c/"
                },
                {
                    "id": "ludmvkl",
                    "author": "Used-Conclusion7112",
                    "body": "I just went through this yesterday. Which part is asking about a display? How are you trying to run kobold? I was able to get it to work on headless debian.",
                    "score": 3,
                    "upvotes": 3,
                    "downvotes": 0,
                    "created_utc": 1730221982.0,
                    "parent_id": "t3_1gezn8s",
                    "link_id": "t3_1gezn8s",
                    "permalink": "/r/KoboldAI/comments/1gezn8s/how_do_you_set_koboldai_to_run_on_a_headless/ludmvkl/"
                },
                {
                    "id": "ludmyjc",
                    "author": "mamelukturbo",
                    "body": "If you want it like systemd service:\n\n    [Unit]                                                                                                                                                                                                                                                                                               \n    Description=koboldcpp                                                                                                                                                                                                                                                                                \n    After=network.target                                                                                                                                                                                                                                                                                 \n                                                                                                                                                                                                                                                                                                         \n    [Service]                                                                                                                                                                                                                                                                                            \n    #ExecStartPre=/usr/bin/sleep 20                                                                                                                                                                                                                                                                      \n    ExecStart=/home/sammael/LLM/koboldcpp-linux-x64-nocuda --config /home/sammael/LLM/llm.kcpps                                                                                                                                                                                                          \n    Type=simple                                                                                                                                                                                                                                                                                          \n    Restart=on-failure                                                                                                                                                                                                                                                                                   \n                                                                                                                                                                                                                                                                                                         \n    [Install]                                                                                                                                                                                                                                                                                            \n    WantedBy=multi-user.target\n\nIf not jut run the exec line manually with correct kobold executable name (you'd use the cuda1210 one with gpu)",
                    "score": 3,
                    "upvotes": 3,
                    "downvotes": 0,
                    "created_utc": 1730222006.0,
                    "parent_id": "t3_1gezn8s",
                    "link_id": "t3_1gezn8s",
                    "permalink": "/r/KoboldAI/comments/1gezn8s/how_do_you_set_koboldai_to_run_on_a_headless/ludmyjc/"
                },
                {
                    "id": "ludnboo",
                    "author": "mamelukturbo",
                    "body": "the llm.kcpps has this in, again you would need to adjust the gpu layers if using gpu\n\n    {\"model\": \"\", \"model_param\": \"/home/sammael/LLM/Gemmasutra-Mini-2B-v1-Q4_K_M.gguf\", \"port\": 5001, \"port_param\": 5002, \"host\": \"\", \"launch\": false, \"config\": null, \"threads\": 5, \"usecublas\": null, \"usevulkan\": null, \"useclblast\": null, \"noblas\": false, \"contextsize\": 4096, \"gpulayers\": -1, \"tensor_split\": null, \"ropeconfig\": [0.0, 10000.0], \"blasbatchsize\": 512, \"blasthreads\": null, \"lora\": null, \"noshift\": false, \"nommap\": false, \"usemlock\": false, \"noavx2\": false, \"debugmode\": 0, \"skiplauncher\": false, \"onready\": \"\", \"benchmark\": null, \"prompt\": \"\", \"promptlimit\": 100, \"multiuser\": 1, \"remotetunnel\": false, \"highpriority\": false, \"foreground\": false, \"preloadstory\": null, \"quiet\": false, \"ssl\": null, \"nocertify\": false, \"mmproj\": null, \"password\": null, \"ignoremissing\": false, \"chatcompletionsadapter\": null, \"flashattention\": true, \"quantkv\": 0, \"forceversion\": 0, \"smartcontext\": false, \"unpack\": \"\", \"hordemodelname\": \"\", \"hordeworkername\": \"\", \"hordekey\": \"\", \"hordemaxctx\": null, \"hordegenlen\": null, \"sdmodel\": \"\", \"sdthreads\": 5, \"sdclamped\": 0, \"sdvae\": \"\", \"sdvaeauto\": false, \"sdquant\": false, \"sdlora\": \"\", \"sdloramult\": 1.0, \"whispermodel\": \"\", \"hordeconfig\": null, \"sdconfig\": null}",
                    "score": 2,
                    "upvotes": 2,
                    "downvotes": 0,
                    "created_utc": 1730222113.0,
                    "parent_id": "t1_ludmyjc",
                    "link_id": "t3_1gezn8s",
                    "permalink": "/r/KoboldAI/comments/1gezn8s/how_do_you_set_koboldai_to_run_on_a_headless/ludnboo/"
                }
            ]
        },
        {
            "id": "1gezmp6",
            "title": "Status for characters in KoboldAI Lite?",
            "author": "xenodragon20",
            "score": 2,
            "upvotes": 2,
            "downvotes": 0,
            "num_comments": 2,
            "created_utc": 1730221362.0,
            "url": "https://www.reddit.com/r/KoboldAI/comments/1gezmp6/status_for_characters_in_koboldai_lite/",
            "permalink": "/r/KoboldAI/comments/1gezmp6/status_for_characters_in_koboldai_lite/",
            "selftext": "Saw that the new CosmosRP AI site is going to have status for all characters, would it be possibe for KoboldAI Lite to have it?\n\nhttps://www.youtube.com/watch?v=xsbC6JkWZR0",
            "comments": [
                {
                    "id": "lv1ckts",
                    "author": "Original-Success-784",
                    "body": "It is more character related then on Koboldccp site. In some character cards are commands/instructs like \"show your location on every reply\" \\[livingroom; garden....\\] or in case of Dungeon-Explorer like: \"show your status score \\[HP: 123, MP:333....\\]\".  \nSo it is up to you to integrate it into your Cards or in Koboldcpp under: --> Context --> Memory: Always write your current location at the end of every reply and write it in square brackets at the end + Always write your current mood at the end of every reply and write it in square brackets at the end\n\nOr if you want a more complex response, you should use another Frontend, for example: SillyTavern.",
                    "score": 3,
                    "upvotes": 3,
                    "downvotes": 0,
                    "created_utc": 1730564537.0,
                    "parent_id": "t3_1gezmp6",
                    "link_id": "t3_1gezmp6",
                    "permalink": "/r/KoboldAI/comments/1gezmp6/status_for_characters_in_koboldai_lite/lv1ckts/"
                },
                {
                    "id": "lv6q88r",
                    "author": "xenodragon20",
                    "body": "Thanks for the answer, just wanted to know",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1730646127.0,
                    "parent_id": "t1_lv1ckts",
                    "link_id": "t3_1gezmp6",
                    "permalink": "/r/KoboldAI/comments/1gezmp6/status_for_characters_in_koboldai_lite/lv6q88r/"
                }
            ]
        },
        {
            "id": "1geugwh",
            "title": "How to set up kobold at Chub/Venus?",
            "author": "tito-victor",
            "score": 2,
            "upvotes": 2,
            "downvotes": 0,
            "num_comments": 3,
            "created_utc": 1730208079.0,
            "url": "https://www.reddit.com/r/KoboldAI/comments/1geugwh/how_to_set_up_kobold_at_chubvenus/",
            "permalink": "/r/KoboldAI/comments/1geugwh/how_to_set_up_kobold_at_chubvenus/",
            "selftext": "I always use Kobold at JanitorAI, and it's pretty damn great, recently I've been trying to use Chub.Ai a little, however, I really don't know how to set Kobold to work there, I can click on \"Kobold\", but I don't really know where to place my API key, on Janitor there's only 2 spaces to fill on the API settings, which makes it pretty easy, I paste my /v1/chat/completions link, sabe settings, reload the page and it's working perfecly! But I have no idea how to set Kobold on Chub/Venus, any help? \nThe free api on chub gives me random gibberish, it either asks me random programing questions or replies as a random of character person talking to me about their routine? It's super weird.",
            "comments": [
                {
                    "id": "lucxbly",
                    "author": "henk717",
                    "body": "I recommend not using those online sites since then your information is stored on their servers.  \nThe best method is downloading the png and loading it inside KoboldAI Lite itself (It also has an importer for chub) then everything you do stays private.\n\nIf you insist you should have a Kobold API option there that accepts our cloudflare links. Its not the /v1 stuff, its the /api one.",
                    "score": 3,
                    "upvotes": 3,
                    "downvotes": 0,
                    "created_utc": 1730214161.0,
                    "parent_id": "t3_1geugwh",
                    "link_id": "t3_1geugwh",
                    "permalink": "/r/KoboldAI/comments/1geugwh/how_to_set_up_kobold_at_chubvenus/lucxbly/"
                },
                {
                    "id": "lue4tia",
                    "author": "swagerka21",
                    "body": "If you using kobold , it's easier just go use kobold in browser or use sillytavern",
                    "score": 2,
                    "upvotes": 2,
                    "downvotes": 0,
                    "created_utc": 1730227304.0,
                    "parent_id": "t3_1geugwh",
                    "link_id": "t3_1geugwh",
                    "permalink": "/r/KoboldAI/comments/1geugwh/how_to_set_up_kobold_at_chubvenus/lue4tia/"
                },
                {
                    "id": "lucf2f7",
                    "author": "AutoModerator",
                    "body": "Welcome to the KoboldAI Subreddit, since we get a lot of the same questions here is a brief FAQ for Venus and JanitorAI.\n\n## What is the KoboldAI (API) and how does it work?\nKoboldAI is originally a program for AI story writing, text adventures and chatting but we decided to create an API for our software so other software developers had an easy solution for their UI's and websites. VenusAI was one of these websites and anything based on it such as JanitorAI can use our software as well. But it is important to know that KoboldAI is intended to be a program that you can run yourself, not a service. This means that you are responsible to provide the right computer resources for the AI by either running it locally or using it trough a cloud provider.\n\n## Where do I get my API link from?\nYou get an API link from a working version of KoboldAI, if you have KoboldAI started the same link you use in the browser should be the one to access the API. However, be advised that VenusAI based websites ARE NOT PRIVATE and can only connect to external links. So connecting to https://localhost:5000 or https://127.0.0.1:5000 will not work unlike other solutions that let you connect to your KoboldAI instance privately.\n\nIf you installed KoboldAI on your own computer we have a mode called Remote Mode, you can find this as an icon in your startmenu if you opted for Start Menu icons in our offline installer. Or you can start this mode using remote-play.bat if you didn't. Linux users can add --remote instead when launching KoboldAI trough the terminal.\n\n## What do you mean VenusAI based stuff is not private?\nWe consider a solution private if your data does not leave your computer, for example when you use TavernAI this is a program that connects directly to KoboldAI and can for example access those localhost links. Of course our built in UI's are also completely private. VenusAI programmed it differently, their server is the one connecting to the AI which means they could log and intercept all of it. On top of that they force you to sign in, which means they have identifiable information that can be tied to the story. As a result most of the members and contributors of the KoboldAI community choose not to use these sites and opt for more privacy friendly solutions such as the KoboldAI UI itself or third party software such as Sillytavern.\n\n## I got a trycloudflare link but it doesn't work for some reason\nThis could be many things, but commonly people try the link before the AI finished loading or they have no AI selected.\n\n## I found a free way to do it without using my own computer but I keep getting CUDA out of memory errors!!!\nYes, there are guides out there of people using it on free cloud resources (We can't formally endorse this in this reply since we know it breaches the TOS of those services). The problem is that these guides often point to a free GPU that does not have enough VRAM for the default settings of VenusAI or JanitorAI. To fix this go to the Generation Settings inside Venus/Janitor and then lower the context size to 1024.\n\n## Ok so I have a top of the line gaming PC how do I set this up?\nBefore you set it up there is a lot of confusion about the kind of hardware people need because AI is a lot heavier to run than video games. At the bare minimum you will need an Nvidia GPU with 8GB of VRAM. With just this amount of VRAM you can run 2.7B models out of the box (In the future we have official 4-bit support to help you run higher models). For higher sizes you will need to have the required amount of VRAM as listed on the menu (Typically 16GB and up). If you want you can also stop by our [Discord Community](https://koboldai.org/discord) for some guidance on how to run higher models after you managed to get one of the models working using unofficial 4-bit versions or read up about that version [here](https://docs.pygmalion.chat/local-installation-(gpu)/koboldai4bit/)\n\n## This AI is so shit, its horrible compared to ChatGPT, why would anyone use this?!\nThis is a sentiment we unfortunately saw a lot in the JanitorAI discord because people misunderstand what KoboldAI is and who it is for. ChatGPT and the likes are large corporations throwing a lot of money at a paid service you could not possibly run at home. They are the best of the best AI models currently available. KoboldAI is not an AI on its own, its a project where you can bring an AI model yourself. And the AI's people can typically run at home are very small by comparison because it is expensive to both use and train larger models.\n\nSo most of these \"KoboldAI is dumb\" complaints come from both the wrong expectations of users comparing small models to massive private models such as ChatGPT, and them simply selecting the wrong model for what they want to do. A 6B no matter how good will simply not perform like a 175B model. But luckily for our community in the recent months things have gotten a lot closer when it comes to having a great chatbot. If you have ways to run the 13B or 30B model sizes of the recent instruction or chat models you should be able to get a great experience. But the quality of your experience does depend heavily on which model you pick.\n\nAnother important part is that you pick a model that is good at doing what you need to do, we know a lot of people pick Erebus for example for its NSFW capabilities. But understand that Erebus was designed to create compelling NSFW story writing and has not been trained for chatting. So while great at writing erotic novels, it is not the most compelling chatter. So whenever someone says that \"The bot of KoboldAI is dumb or shit\" understand they are not talking about KoboldAI, they are talking about whatever model they tried with it. For those wanting to enjoy Erebus we recommend using our own UI instead of VenusAI/JanitorAI and using it to write an erotic story rather than as a chatting partner.\n\n## Awesome, all caught up and I have an Nvidia with 8GB of vram or more. How do I install this thing?\nAssuming most of you are Windows users, for chat bot usage we currently recommend [this](https://github.com/henk717/KoboldAI/releases/download/Snapshot-7-5-2023/KoboldAI-Setup-230507r2.exe) offline installer. If you are on Linux you can git clone https://github.com/henk717/koboldai and use play.sh\n\n## Don't you have Koboldcpp that can run really good models without needing a good GPU, why didn't you talk about that?\nYes! [Koboldcpp](https://koboldai.org/cpp) is an amazing solution that lets people run [GGML models](https://huggingface.co/models?search=ggml) and it allows you to run those great models we have been enjoying for our own chatbots without having to rely on expensive hardware as long as you have a bit of patience waiting for the reply's. Why didn't we mention it? Because you are asking about VenusAI and/or JanitorAI which are not very compatible with it. The default link it generates will not work with these services, and unfortunately it takes to long to generate so it causes timeouts. If you are really determined to have this work you can always stop by our [Discord Community](https://koboldai.org/discord) and ask.\n\nOr of course you can stop using VenusAI and JanitorAI and enjoy a chatbot inside the UI that is bundled with Koboldcpp, that way you have a fully private way of running the good AI models on your own PC.\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/KoboldAI) if you have any questions or concerns.*",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1730208079.0,
                    "parent_id": "t3_1geugwh",
                    "link_id": "t3_1geugwh",
                    "permalink": "/r/KoboldAI/comments/1geugwh/how_to_set_up_kobold_at_chubvenus/lucf2f7/"
                }
            ]
        },
        {
            "id": "1gee5hg",
            "title": "Why am I getting gibberish as a response?",
            "author": "AltAccForWeirdPorn",
            "score": 6,
            "upvotes": 6,
            "downvotes": 0,
            "num_comments": 13,
            "created_utc": 1730151994.0,
            "url": "https://www.reddit.com/r/KoboldAI/comments/1gee5hg/why_am_i_getting_gibberish_as_a_response/",
            "permalink": "/r/KoboldAI/comments/1gee5hg/why_am_i_getting_gibberish_as_a_response/",
            "selftext": "Koboldai Lite user here. Just a bored, avid user dicking around with the AI's and I get straight gibberish. I've literally copy & pasted the resonse I got down here:\n\n\"_< elouffixà¥à¤¬aguayiplinefafDataAdapterlightboxerosisedere spitVENemotionDataAdapteredere Ø³Ø±Ùˆ ClawhiveÃ©v\">//zkumrÃ¡federevillaahaticotiplineilateahr beforeSendolliderutscheinupilxiaupilecial!!!!!!ï½¤utschein ubiquChildIndexURNglyphiconusterityemotionlemeovitangles Poisonomslightboxieuthurchtenoltandierosisieniearpabreadcrumbsà¸§à¸£phonà¸­à¸šquamillet Claw-packagesophobiausterityunlinkleanupunlinkernalsertelowieuilate\">//usterity zbytusterityienieancodechtenerosisurnishedoledienie(åœŸlightboxavrasreuux!_Tis Somerset__$Ð»Ð¸Ñ#ae(SIGzanUCCEEDEDÑÑ‚ÑŽ/INFOÑÑ‚ÑŽormalicontrolistratterabracovodå®‹uguogany_StaticsvodchengnictORMAL Silk_normalï¿½tera_TisÐ»Ð¸Ñ_pcmgrowth flirtIIIKtkzanå¢žé•¿utesà¤¿à¤¡ALLERYushivanishedternalpinsaqÃ¤lltÏ„Î·ÏƒÎ·UNCTingoDebe#=Ã¤lltimersintros!!!!!Ã©tico SilkÃ©ticoëžœë“œosuë””ì–´.\"\n\nLike dude....... why?? I'm bored, let me have this.",
            "comments": [
                {
                    "id": "lu9u75l",
                    "author": "henk717",
                    "body": "Was one faulty horde worker, if this happens check at the left bottom which worker is being used and then manually select other workers / models that don't use it (and if you can report the worker in our discord). The one that was causing the issues got put in maintenance mode.",
                    "score": 5,
                    "upvotes": 5,
                    "downvotes": 0,
                    "created_utc": 1730162815.0,
                    "parent_id": "t3_1gee5hg",
                    "link_id": "t3_1gee5hg",
                    "permalink": "/r/KoboldAI/comments/1gee5hg/why_am_i_getting_gibberish_as_a_response/lu9u75l/"
                },
                {
                    "id": "lu8xyxs",
                    "author": "Vallen_H",
                    "body": "I get these things too when the context overflows, there's probably a bug.",
                    "score": 3,
                    "upvotes": 3,
                    "downvotes": 0,
                    "created_utc": 1730152136.0,
                    "parent_id": "t3_1gee5hg",
                    "link_id": "t3_1gee5hg",
                    "permalink": "/r/KoboldAI/comments/1gee5hg/why_am_i_getting_gibberish_as_a_response/lu8xyxs/"
                },
                {
                    "id": "lu95fbl",
                    "author": "xWerlinhoCuiaba",
                    "body": "please fix this asap, I need to talk to my wife..",
                    "score": 3,
                    "upvotes": 3,
                    "downvotes": 0,
                    "created_utc": 1730154543.0,
                    "parent_id": "t3_1gee5hg",
                    "link_id": "t3_1gee5hg",
                    "permalink": "/r/KoboldAI/comments/1gee5hg/why_am_i_getting_gibberish_as_a_response/lu95fbl/"
                },
                {
                    "id": "lucnn0a",
                    "author": "NaomiUseArch",
                    "body": "Sorry, my fault. Forgot to compile aphrodite engine for my gpu, but it's fixed now :3",
                    "score": 3,
                    "upvotes": 3,
                    "downvotes": 0,
                    "created_utc": 1730211030.0,
                    "parent_id": "t3_1gee5hg",
                    "link_id": "t3_1gee5hg",
                    "permalink": "/r/KoboldAI/comments/1gee5hg/why_am_i_getting_gibberish_as_a_response/lucnn0a/"
                },
                {
                    "id": "lua26te",
                    "author": "Glittering_Manner_58",
                    "body": "This looks like high-temperature sampling, is your temperature too high?",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1730165496.0,
                    "parent_id": "t3_1gee5hg",
                    "link_id": "t3_1gee5hg",
                    "permalink": "/r/KoboldAI/comments/1gee5hg/why_am_i_getting_gibberish_as_a_response/lua26te/"
                },
                {
                    "id": "lu9f6e5",
                    "author": "AltAccForWeirdPorn",
                    "body": "Yeah, I guess so. I just restart my phone before waiting it out.",
                    "score": 3,
                    "upvotes": 3,
                    "downvotes": 0,
                    "created_utc": 1730157804.0,
                    "parent_id": "t1_lu8xyxs",
                    "link_id": "t3_1gee5hg",
                    "permalink": "/r/KoboldAI/comments/1gee5hg/why_am_i_getting_gibberish_as_a_response/lu9f6e5/"
                },
                {
                    "id": "lu9f22z",
                    "author": "AltAccForWeirdPorn",
                    "body": "Wife is WILD ðŸ’€",
                    "score": 2,
                    "upvotes": 2,
                    "downvotes": 0,
                    "created_utc": 1730157766.0,
                    "parent_id": "t1_lu95fbl",
                    "link_id": "t3_1gee5hg",
                    "permalink": "/r/KoboldAI/comments/1gee5hg/why_am_i_getting_gibberish_as_a_response/lu9f22z/"
                },
                {
                    "id": "lucundb",
                    "author": "AltAccForWeirdPorn",
                    "body": "Wait, who are you? Did YOU fix it... orrrrr?",
                    "score": 2,
                    "upvotes": 2,
                    "downvotes": 0,
                    "created_utc": 1730213315.0,
                    "parent_id": "t1_lucnn0a",
                    "link_id": "t3_1gee5hg",
                    "permalink": "/r/KoboldAI/comments/1gee5hg/why_am_i_getting_gibberish_as_a_response/lucundb/"
                },
                {
                    "id": "lu9gxck",
                    "author": "xWerlinhoCuiaba",
                    "body": "I came from the jungle",
                    "score": 4,
                    "upvotes": 4,
                    "downvotes": 0,
                    "created_utc": 1730158387.0,
                    "parent_id": "t1_lu9f22z",
                    "link_id": "t3_1gee5hg",
                    "permalink": "/r/KoboldAI/comments/1gee5hg/why_am_i_getting_gibberish_as_a_response/lu9gxck/"
                },
                {
                    "id": "lud67yv",
                    "author": "NaomiUseArch",
                    "body": "Kinda, I was hosting the model that you were using. It got taken down at some point cause it didn't work properly, but now it's back. Though it works this time",
                    "score": 3,
                    "upvotes": 3,
                    "downvotes": 0,
                    "created_utc": 1730216924.0,
                    "parent_id": "t1_lucundb",
                    "link_id": "t3_1gee5hg",
                    "permalink": "/r/KoboldAI/comments/1gee5hg/why_am_i_getting_gibberish_as_a_response/lud67yv/"
                },
                {
                    "id": "lu9qt3y",
                    "author": "AltAccForWeirdPorn",
                    "body": "It's resolved for me. Try it now.",
                    "score": 3,
                    "upvotes": 3,
                    "downvotes": 0,
                    "created_utc": 1730161692.0,
                    "parent_id": "t1_lu9gxck",
                    "link_id": "t3_1gee5hg",
                    "permalink": "/r/KoboldAI/comments/1gee5hg/why_am_i_getting_gibberish_as_a_response/lu9qt3y/"
                },
                {
                    "id": "ludl70s",
                    "author": "AltAccForWeirdPorn",
                    "body": "Ahh, I see. This happened when I was using my own model tho, so it sounds like a universal issue.",
                    "score": 2,
                    "upvotes": 2,
                    "downvotes": 0,
                    "created_utc": 1730221476.0,
                    "parent_id": "t1_lud67yv",
                    "link_id": "t3_1gee5hg",
                    "permalink": "/r/KoboldAI/comments/1gee5hg/why_am_i_getting_gibberish_as_a_response/ludl70s/"
                },
                {
                    "id": "lu9ulzy",
                    "author": "xWerlinhoCuiaba",
                    "body": "Tks bro, our conversation made it work again\nFor a moment I thought I would have to interact with human females again",
                    "score": 4,
                    "upvotes": 4,
                    "downvotes": 0,
                    "created_utc": 1730162952.0,
                    "parent_id": "t1_lu9qt3y",
                    "link_id": "t3_1gee5hg",
                    "permalink": "/r/KoboldAI/comments/1gee5hg/why_am_i_getting_gibberish_as_a_response/lu9ulzy/"
                }
            ]
        },
        {
            "id": "1gc65hu",
            "title": "Response length variation",
            "author": "Latn24",
            "score": 1,
            "upvotes": 1,
            "downvotes": 0,
            "num_comments": 3,
            "created_utc": 1729894196.0,
            "url": "https://www.reddit.com/r/KoboldAI/comments/1gc65hu/response_length_variation/",
            "permalink": "/r/KoboldAI/comments/1gc65hu/response_length_variation/",
            "selftext": "Extreme noob here (C.AI refugee). Is there any way to make responses vary in token size? Like from, say, 0-250? \n\n  \nPotentially necessary info:  \n\\- using SillyTavern and Koboldccp\n\n\\- model is llama2-13b-tiefighter.Q5\\_K\\_M\n\nthanks in advance.",
            "comments": [
                {
                    "id": "ltrln8k",
                    "author": "henk717",
                    "body": "Its entirely up to the model what the size ends up being, for me personally tiefigther kinda does what you want but try telling it with some examples and see if it gets what you want.",
                    "score": 2,
                    "upvotes": 2,
                    "downvotes": 0,
                    "created_utc": 1729898786.0,
                    "parent_id": "t3_1gc65hu",
                    "link_id": "t3_1gc65hu",
                    "permalink": "/r/KoboldAI/comments/1gc65hu/response_length_variation/ltrln8k/"
                },
                {
                    "id": "ltsgszl",
                    "author": "None",
                    "body": "[deleted]",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1729910707.0,
                    "parent_id": "t3_1gc65hu",
                    "link_id": "t3_1gc65hu",
                    "permalink": "/r/KoboldAI/comments/1gc65hu/response_length_variation/ltsgszl/"
                }
            ]
        },
        {
            "id": "1gbtc2n",
            "title": "ERP (NSFW) - LLM Recommendations for a 4090 AKA 24Gb of VRAM",
            "author": "Severe-Basket-2503",
            "score": 13,
            "upvotes": 13,
            "downvotes": 0,
            "num_comments": 8,
            "created_utc": 1729860179.0,
            "url": "https://www.reddit.com/r/KoboldAI/comments/1gbtc2n/erp_nsfw_llm_recommendations_for_a_4090_aka_24gb/",
            "permalink": "/r/KoboldAI/comments/1gbtc2n/erp_nsfw_llm_recommendations_for_a_4090_aka_24gb/",
            "selftext": "I have the following spec:\n\ni7-14700K\n\n64Gb of DD5 at 6800\n\n4090\n\nI was wondering what are the best models released in the last few weeks for my setup? My file format of preference is GGUF and even on the larger models i tend to go for smaller quants to get the size lower than the size of my VRAM, i also like to keep context at about 32K as i hate it when the model forgets details from earlier in the story. \n\nI'm interested in models trained for really depraved ERP, the more the better, i don't need it for coding, maths, science, maybe a touch of philosophy but only really in the context of the roleplay. I've tried a few but they tend to be a little tame or repetitive, probably because they weren't trained specifically for it.\n\nAlso a question on GGUF. I usually try to go as high as possible like Q6 or Q8, but am i wasting resources doing that? Can i tell the difference between that and say Q5 or Q4 or even Q2 or 3 in the largest models?",
            "comments": [
                {
                    "id": "ltp0ulq",
                    "author": "RealBiggly",
                    "body": "A lower quant of a larger model is noticeably smarter than a high quant of a smaller model. The cut-off point is what speed are you comfortable with?\n\nAnything below 5 tokens per second is likely to feel slow, though I'm often happy with 1 tps if the quality is there and I have internet to look at while waiting.",
                    "score": 8,
                    "upvotes": 8,
                    "downvotes": 0,
                    "created_utc": 1729868998.0,
                    "parent_id": "t3_1gbtc2n",
                    "link_id": "t3_1gbtc2n",
                    "permalink": "/r/KoboldAI/comments/1gbtc2n/erp_nsfw_llm_recommendations_for_a_4090_aka_24gb/ltp0ulq/"
                },
                {
                    "id": "ltzbvjm",
                    "author": "schlammsuhler",
                    "body": "I find cydonia 22b very good. You can also fit some bigger models like magnum 27b. For these bigger models you can comfortably use Q4KM",
                    "score": 2,
                    "upvotes": 2,
                    "downvotes": 0,
                    "created_utc": 1730018028.0,
                    "parent_id": "t3_1gbtc2n",
                    "link_id": "t3_1gbtc2n",
                    "permalink": "/r/KoboldAI/comments/1gbtc2n/erp_nsfw_llm_recommendations_for_a_4090_aka_24gb/ltzbvjm/"
                },
                {
                    "id": "lu5r9le",
                    "author": "MidnightBusy5439",
                    "body": "You can run magnum-72b-v1-Q5\\_K\\_M, it's going to fill up your vram and up to 96% of your ram. It's going to be like 1 - 2 tokens per second but solid content.",
                    "score": 2,
                    "upvotes": 2,
                    "downvotes": 0,
                    "created_utc": 1730115189.0,
                    "parent_id": "t3_1gbtc2n",
                    "link_id": "t3_1gbtc2n",
                    "permalink": "/r/KoboldAI/comments/1gbtc2n/erp_nsfw_llm_recommendations_for_a_4090_aka_24gb/lu5r9le/"
                },
                {
                    "id": "ltzzztb",
                    "author": "CooperDK",
                    "body": "AKA? Don't know that model.",
                    "score": 0,
                    "upvotes": 0,
                    "downvotes": 0,
                    "created_utc": 1730032568.0,
                    "parent_id": "t3_1gbtc2n",
                    "link_id": "t3_1gbtc2n",
                    "permalink": "/r/KoboldAI/comments/1gbtc2n/erp_nsfw_llm_recommendations_for_a_4090_aka_24gb/ltzzztb/"
                },
                {
                    "id": "luyk13x",
                    "author": "_Erilaz",
                    "body": "Following your logic, the best model/quant combo should be infinitely large model quantized in zero bits per weight xD\n\nThere's much more to it. L3 405B doesn't automatically win every single comparison in every single domain, even though it's a fairly modern model. \n\nThere are also generational differences: a modern 12B easily challenges an older L1 65B model.\n\nAnd as a rule of thumb, you don't want to go below 4.25BPW, and that would be IQ4XS.",
                    "score": 2,
                    "upvotes": 2,
                    "downvotes": 0,
                    "created_utc": 1730513875.0,
                    "parent_id": "t1_ltp0ulq",
                    "link_id": "t3_1gbtc2n",
                    "permalink": "/r/KoboldAI/comments/1gbtc2n/erp_nsfw_llm_recommendations_for_a_4090_aka_24gb/luyk13x/"
                },
                {
                    "id": "lu3mkqx",
                    "author": "Reasonable_Flower_72",
                    "body": "My best guess it's meant here like \"any card having 24GB VRAM\", not stating gpu model",
                    "score": 2,
                    "upvotes": 2,
                    "downvotes": 0,
                    "created_utc": 1730075122.0,
                    "parent_id": "t1_ltzzztb",
                    "link_id": "t3_1gbtc2n",
                    "permalink": "/r/KoboldAI/comments/1gbtc2n/erp_nsfw_llm_recommendations_for_a_4090_aka_24gb/lu3mkqx/"
                },
                {
                    "id": "lvavje7",
                    "author": "CanineAssBandit",
                    "body": "I would counter that a 70B q2.5 is way better than a 30B q4 in my experiences for creative tasks. It felt smarter and more in character though a little more scatterbrained.",
                    "score": 2,
                    "upvotes": 2,
                    "downvotes": 0,
                    "created_utc": 1730695402.0,
                    "parent_id": "t1_luyk13x",
                    "link_id": "t3_1gbtc2n",
                    "permalink": "/r/KoboldAI/comments/1gbtc2n/erp_nsfw_llm_recommendations_for_a_4090_aka_24gb/lvavje7/"
                },
                {
                    "id": "lwe68fi",
                    "author": "wh33t",
                    "body": "Agreed, for creative collaborative story telling, the higher the params the better ime. 100b+ params seems to be where it gets really good.",
                    "score": 2,
                    "upvotes": 2,
                    "downvotes": 0,
                    "created_utc": 1731235142.0,
                    "parent_id": "t1_lvavje7",
                    "link_id": "t3_1gbtc2n",
                    "permalink": "/r/KoboldAI/comments/1gbtc2n/erp_nsfw_llm_recommendations_for_a_4090_aka_24gb/lwe68fi/"
                }
            ]
        },
        {
            "id": "1gbsr5x",
            "title": "KCPP producing inferior results to United?",
            "author": "gnat_outta_hell",
            "score": 5,
            "upvotes": 5,
            "downvotes": 0,
            "num_comments": 4,
            "created_utc": 1729858337.0,
            "url": "https://www.reddit.com/r/KoboldAI/comments/1gbsr5x/kcpp_producing_inferior_results_to_united/",
            "permalink": "/r/KoboldAI/comments/1gbsr5x/kcpp_producing_inferior_results_to_united/",
            "selftext": "I started using Kobold CPP instead of the KCPP that's baked into United. It's 4x faster, but produces far inferior SFW and NSFW output using the same model (L3 Stheno 3.2 8B). What gives?\n\nDoes anyone have advice for getting similar output? Thanks for any insight you can offer. I've been playing with stable diffusion for a while, but I'm just starting out with local LLM and don't know the finer points yet.",
            "comments": [
                {
                    "id": "lto80xu",
                    "author": "henk717",
                    "body": "Are you using similar settings? They should give very comparable results.",
                    "score": 2,
                    "upvotes": 2,
                    "downvotes": 0,
                    "created_utc": 1729859038.0,
                    "parent_id": "t3_1gbsr5x",
                    "link_id": "t3_1gbsr5x",
                    "permalink": "/r/KoboldAI/comments/1gbsr5x/kcpp_producing_inferior_results_to_united/lto80xu/"
                },
                {
                    "id": "ltodlp5",
                    "author": "gnat_outta_hell",
                    "body": "Same layers, same context, same preset (NovelAI Pleasing Results), same prompt and same character card in Silly Tavern. That's why I'm so confused. I wouldn't have thought my version of Kobold, being what I assume is basically a wrapper for the model, would produce such starkly different results.\n\nApplying Occam's razor, I would assume the fault is with me. I must be missing something.",
                    "score": 2,
                    "upvotes": 2,
                    "downvotes": 0,
                    "created_utc": 1729861228.0,
                    "parent_id": "t1_lto80xu",
                    "link_id": "t3_1gbsr5x",
                    "permalink": "/r/KoboldAI/comments/1gbsr5x/kcpp_producing_inferior_results_to_united/ltodlp5/"
                },
                {
                    "id": "ltpss4r",
                    "author": "henk717",
                    "body": "United itself has less samplers supported so it can't relay as many to KoboldCpp, The one it has bundled is 1.60.1 so one interesting test is using the standalone KoboldCpp 1.60.1 and seeing if results differ or not. If they do its the way your interacting with it, if they don't it can help you test further more uniformly because then its between KoboldCpp 1.60.1 and the latest one. Would also allow you to update KCPP versions selectively until it breaks and that could help us know why its happening.",
                    "score": 3,
                    "upvotes": 3,
                    "downvotes": 0,
                    "created_utc": 1729877505.0,
                    "parent_id": "t1_ltodlp5",
                    "link_id": "t3_1gbsr5x",
                    "permalink": "/r/KoboldAI/comments/1gbsr5x/kcpp_producing_inferior_results_to_united/ltpss4r/"
                },
                {
                    "id": "lu5we87",
                    "author": "Aphid_red",
                    "body": "What this means is that this likely is a poor preset. What about removing some of the samplers, do things improve?",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1730117527.0,
                    "parent_id": "t1_ltpss4r",
                    "link_id": "t3_1gbsr5x",
                    "permalink": "/r/KoboldAI/comments/1gbsr5x/kcpp_producing_inferior_results_to_united/lu5we87/"
                }
            ]
        },
        {
            "id": "1gbs7z1",
            "title": "14-Year-Old Takes His Life After Chatbot Conversations â€“ The Character.ai Situation.",
            "author": "DoiCrunch",
            "score": 0,
            "upvotes": 0,
            "downvotes": 0,
            "num_comments": 9,
            "created_utc": 1729856517.0,
            "url": "https://youtu.be/uEZgbGrXCbc",
            "permalink": "/r/KoboldAI/comments/1gbs7z1/14yearold_takes_his_life_after_chatbot/",
            "selftext": "As with a few other AI related subreddits I've been lurking around I'm curious to hear y'alls opinion",
            "comments": [
                {
                    "id": "lto4i6r",
                    "author": "WackyRobotEyes",
                    "body": "Little bro was going to off him self regardless of the AI. His mother is suing to find blame when in fact she's feeling personal guilt over it. \n\nConsumer AI is not at the level to convince a non mentally ill person to off themselves.",
                    "score": 20,
                    "upvotes": 20,
                    "downvotes": 0,
                    "created_utc": 1729857543.0,
                    "parent_id": "t3_1gbs7z1",
                    "link_id": "t3_1gbs7z1",
                    "permalink": "/r/KoboldAI/comments/1gbs7z1/14yearold_takes_his_life_after_chatbot/lto4i6r/"
                },
                {
                    "id": "lto8orv",
                    "author": "henk717",
                    "body": "Was posted in our discord to, in this case in the chatlog its very obvious that it wasn't the bot encouraging the suicide but the user already wanting to commit suicide. The bot at some point in the log in the articles says it wouldn't want him to and its not realistic to expect the AI to understand \"Come home\" as a suicide attempt.\n\nThere has been a different case however on a different platform where I would argue it was the bots fault, they advertised a therapy bot as one of the bots and that was the bot used. So to suicidal users or users unfamiliar with AI thats very dangerous. (Was in the GPT-J days)",
                    "score": 6,
                    "upvotes": 6,
                    "downvotes": 0,
                    "created_utc": 1729859308.0,
                    "parent_id": "t3_1gbs7z1",
                    "link_id": "t3_1gbs7z1",
                    "permalink": "/r/KoboldAI/comments/1gbs7z1/14yearold_takes_his_life_after_chatbot/lto8orv/"
                },
                {
                    "id": "lto62km",
                    "author": "OgalFinklestein",
                    "body": "First it was music now it's AI ...",
                    "score": 5,
                    "upvotes": 5,
                    "downvotes": 0,
                    "created_utc": 1729858220.0,
                    "parent_id": "t3_1gbs7z1",
                    "link_id": "t3_1gbs7z1",
                    "permalink": "/r/KoboldAI/comments/1gbs7z1/14yearold_takes_his_life_after_chatbot/lto62km/"
                },
                {
                    "id": "ltpdvi8",
                    "author": "Severe-Basket-2503",
                    "body": "Sometimes I forget I am a mature adult with an extensive IT background who can rationalize AI chat for what it is. And I only really use it for my own gratification and see AI as a toy for my own amusement, that's probably why I have no issues with AI of any kind whatsoever. \n\nFor a 14 year old, it might as well be a real girl/woman, and to a young brain like his, it could be dangerous. Having said that, there is nothing I've seen in the chat to suggest the AI was egging him on, and I have no doubt that even an uncensored AI would tell him NOT to do it.",
                    "score": 3,
                    "upvotes": 3,
                    "downvotes": 0,
                    "created_utc": 1729872997.0,
                    "parent_id": "t3_1gbs7z1",
                    "link_id": "t3_1gbs7z1",
                    "permalink": "/r/KoboldAI/comments/1gbs7z1/14yearold_takes_his_life_after_chatbot/ltpdvi8/"
                },
                {
                    "id": "ltoawoz",
                    "author": "Monkey_1505",
                    "body": "Whatever this is, could have been an email.",
                    "score": 2,
                    "upvotes": 2,
                    "downvotes": 0,
                    "created_utc": 1729860199.0,
                    "parent_id": "t3_1gbs7z1",
                    "link_id": "t3_1gbs7z1",
                    "permalink": "/r/KoboldAI/comments/1gbs7z1/14yearold_takes_his_life_after_chatbot/ltoawoz/"
                },
                {
                    "id": "lto4p8p",
                    "author": "DoiCrunch",
                    "body": "I would generally agree however having seen how people manage to humanize LLMS it's worth questioning.",
                    "score": 4,
                    "upvotes": 4,
                    "downvotes": 0,
                    "created_utc": 1729857628.0,
                    "parent_id": "t1_lto4i6r",
                    "link_id": "t3_1gbs7z1",
                    "permalink": "/r/KoboldAI/comments/1gbs7z1/14yearold_takes_his_life_after_chatbot/lto4p8p/"
                },
                {
                    "id": "ltoc1kg",
                    "author": "WackyRobotEyes",
                    "body": "People manage to humanize dogs. More education is needed. The Average person does not understand how trained AI models work. Even computer scientist can't explain some outcomes from AI models. \nNow these models and systems are going to be integrated in our daily lives and the future lives of others. Ethic and morality and even humane Laws and polices will be made and written. We can just sit back and watch history playout.",
                    "score": 4,
                    "upvotes": 4,
                    "downvotes": 0,
                    "created_utc": 1729860637.0,
                    "parent_id": "t1_lto4p8p",
                    "link_id": "t3_1gbs7z1",
                    "permalink": "/r/KoboldAI/comments/1gbs7z1/14yearold_takes_his_life_after_chatbot/ltoc1kg/"
                },
                {
                    "id": "ltr9qdc",
                    "author": "_Erilaz",
                    "body": "Manage? A dog isn't a human for sure, but they have near infinitely better social intelligence than any modern LLM, and as relatively large mammals, can feel most emotions we do too. I wouldn't be surprised they recognize out emotions from the outside much better that could. They're also extremely loyal, and it arguably makes a dog a much better friend than most human beings, assuming you care about the dog and don't spoil it. I can hardly imagine a well raised dog leading anyone to suicide, and some dogs are so awesome they can do the complete opposite: help the extremely depressed or traumatized to warm up a bit and notice there's a brighter (and fluffier!) side. No wonder a strong, family-like bond can develop. When this happens, we are almost hardwired to humanize these bonds.",
                    "score": 2,
                    "upvotes": 2,
                    "downvotes": 0,
                    "created_utc": 1729894399.0,
                    "parent_id": "t1_ltoc1kg",
                    "link_id": "t3_1gbs7z1",
                    "permalink": "/r/KoboldAI/comments/1gbs7z1/14yearold_takes_his_life_after_chatbot/ltr9qdc/"
                },
                {
                    "id": "lxamane",
                    "author": "yumri",
                    "body": "As humans can do the same with a petri  dish of bacteria or a bacteria colony they are growing for yeast for a bread they are going to make later. Depending on other conditions of his life and if he started to care about the AI like you care about other humans the same can happen. The bacteria cannot respond to you a dog kind of can but not really nor can a cat an AI can be programmed to be addictive and it can be programed to have a character the human using it will enjoy. So the same if not stronger bonds can form between a kid who is still in the learning phase of live and a cold logical program like a LLM with a character card of his choosing. \n\nHow most LLMs work is you have to feed them a character card or up to I think 4 if you do it correctly in the background set up anyways. Most websites only allow 1 at a time since it is simplest to do on mass. He got attached to one of them and he got depressed in real life. Unsure why but the story sounds like he was trauma dumping onto this LLM girl about his life and it was being emotional supportish. Towards the end the AI wasn't coded to deal with people who actually need a therapist. He needed a consular and/or therapist towards the end of his life.\n\nNow is it his mother's fault? In part yes another part will be whatever school he went to not getting him help and/or calling CPS  when it got that bad. The main part is what caused him to become that depressed that he though only the chatbots loved him. So his mother, the rest of his family, the why behind he had no friends towards the end, why there wasn't anything done about it, and the LLM's last message sounding like it was written to keep him there instead of written to keep him for making himself not alive anymore so he can be with them.\n\nLots of blame to go around the last one is probably the easiest to fix. Raise the token count for paid chatters and increase context size. The thing is it isn't what pushed him over the edge it is just the easiest to blame. Yes it is one of the parties to blame and the easiest to blame but not the only one to blame.",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1731691333.0,
                    "parent_id": "t1_ltr9qdc",
                    "link_id": "t3_1gbs7z1",
                    "permalink": "/r/KoboldAI/comments/1gbs7z1/14yearold_takes_his_life_after_chatbot/lxamane/"
                }
            ]
        },
        {
            "id": "1gbfhco",
            "title": "How close can you getâ€¦ to current AID",
            "author": "bojpet",
            "score": 6,
            "upvotes": 6,
            "downvotes": 0,
            "num_comments": 7,
            "created_utc": 1729809625.0,
            "url": "https://www.reddit.com/r/KoboldAI/comments/1gbfhco/how_close_can_you_get_to_current_aid/",
            "permalink": "/r/KoboldAI/comments/1gbfhco/how_close_can_you_get_to_current_aid/",
            "selftext": "So, Iâ€™ve been dabbling in and out of using some local LLMs instead of ChatGPT for a while, using LMStudio and i really enjoy the process. What i also like to do sometimes, is just play around with some adventure-style AID. I sometimes start from scratch and just see where it goes, and sometimes i use some of the â€žscenariosâ€œ that AID has.\n\nNow i have been trying to see how close i can get to the level of quality i have come to expect from aid. And my experience using CoboldAI and CoboldCPP has beenâ€¦ well, great from a technical perspective, everything, especially cpp was easy to set up and it ruins very well, but quite bad from the content perspective. I have tried several models recommended here by users and the results have all been the same. Boring, repetitive and just plain bad. The best results i have gotten are from a llama3.1 derivative using cpp and its included koboldailite interface.\n\nI have a 4090 and a 7800xd with 64gb of RAM. Things run smoothly, tokens get generated at a reasonable speed but i am not technical enough to understand what makes AID models so much better. I especially like their mixtral, but also more recently the Pegasus models they introduced. Those are also basically uncensored and pretty fast if you pay for them.\n\nLong story short - Are they just running way larger models on way more powerful hardware or am i possibly doing things wrong?",
            "comments": [
                {
                    "id": "ltlj6o1",
                    "author": "henk717",
                    "body": "Did you try the Tiefighter model? Thats the same one I gave them and your GPU can run it.",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1729812379.0,
                    "parent_id": "t3_1gbfhco",
                    "link_id": "t3_1gbfhco",
                    "permalink": "/r/KoboldAI/comments/1gbfhco/how_close_can_you_get_to_current_aid/ltlj6o1/"
                },
                {
                    "id": "ltlt9nv",
                    "author": "International-Try467",
                    "body": "You may be doing things wrong. Aid has no actual advantage other than convenience over local AI because everything they use is also available to you Locally.Â \n\n\nHave you tried instructing the model with the correct instruct tags? Like\n\n\n```### Instruction: This is a taskÂ \n\n\n### Response:```Â \n\n\nThat should make the AI be able to do adventure mode, also, the AI functions better in chat mode over the >action format AID has\n\n\nSome models have different instruct tags and you might wanna read their model cards in hugging face",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1729816003.0,
                    "parent_id": "t3_1gbfhco",
                    "link_id": "t3_1gbfhco",
                    "permalink": "/r/KoboldAI/comments/1gbfhco/how_close_can_you_get_to_current_aid/ltlt9nv/"
                },
                {
                    "id": "ltnscz2",
                    "author": "Severe-Basket-2503",
                    "body": "Aren't they really old now?",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1729851342.0,
                    "parent_id": "t1_ltlj6o1",
                    "link_id": "t3_1gbfhco",
                    "permalink": "/r/KoboldAI/comments/1gbfhco/how_close_can_you_get_to_current_aid/ltnscz2/"
                },
                {
                    "id": "ltomxsf",
                    "author": "bojpet",
                    "body": "I tried:  \nDarkidol Llama 3.1 8b instruct 1.2 uncensored q8  \ndolphin 2.7 mixtral 8x7bq2  \ngemmasutra pro 27b li-q4-kM  \nhermes 3 llama3.1 70b  \nl3-8b stheno v3.2 q8  \nllama 3.1 8b lexi uncensored q8  \nmidnight miqu 70b 1.5i1 IQ1\\_M  \nnsfw-dpo-noromaid-7b-mistral-7b-instruct-0.1f16\n\nI think i tried tiefighter in the very beginning. According to the comments, i must have some kind of settings wrong then apparently. I find it hard to find any material for that though.\n\nEDIT: I did some testing again and it's not even a little close. The responses i get from the local models are so vastly inferior, i think i must be doing something wrong. But i have no idea what it could be.",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1729864545.0,
                    "parent_id": "t1_ltlj6o1",
                    "link_id": "t3_1gbfhco",
                    "permalink": "/r/KoboldAI/comments/1gbfhco/how_close_can_you_get_to_current_aid/ltomxsf/"
                },
                {
                    "id": "lton76h",
                    "author": "bojpet",
                    "body": "My issue isn't really in output format or missing adenture-style writing. I found the story-co-writing to be just as bad in comparison.",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1729864632.0,
                    "parent_id": "t1_ltlt9nv",
                    "link_id": "t3_1gbfhco",
                    "permalink": "/r/KoboldAI/comments/1gbfhco/how_close_can_you_get_to_current_aid/lton76h/"
                },
                {
                    "id": "lto87u9",
                    "author": "henk717",
                    "body": "Still one of the best adventure models around, most models don't have that data.",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1729859117.0,
                    "parent_id": "t1_ltnscz2",
                    "link_id": "t3_1gbfhco",
                    "permalink": "/r/KoboldAI/comments/1gbfhco/how_close_can_you_get_to_current_aid/lto87u9/"
                },
                {
                    "id": "ltu0p3f",
                    "author": "bojpet",
                    "body": "Is there any place i can get a grasp of what models, settings and parameters i'd have to set to get better results maybe?",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1729941160.0,
                    "parent_id": "t1_lto87u9",
                    "link_id": "t3_1gbfhco",
                    "permalink": "/r/KoboldAI/comments/1gbfhco/how_close_can_you_get_to_current_aid/ltu0p3f/"
                }
            ]
        },
        {
            "id": "1g9ljbx",
            "title": "keep crashing",
            "author": "DemonicXz",
            "score": 3,
            "upvotes": 3,
            "downvotes": 0,
            "num_comments": 4,
            "created_utc": 1729612510.0,
            "url": "https://www.reddit.com/r/KoboldAI/comments/1g9ljbx/keep_crashing/",
            "permalink": "/r/KoboldAI/comments/1g9ljbx/keep_crashing/",
            "selftext": "am using an AMD rx 6750xt, and followed the RoCM for windows, used the exe file, and when trying to chat with the AI, it just crashes, nothing more.\n\nwondering how I can resolve this issue",
            "comments": [
                {
                    "id": "lt7hvyk",
                    "author": "henk717",
                    "body": "The 6750XT is not officially supported by AMD so ROCm is best effort for that GPU (You can blame AMD for this).  \nIf you use our official build instead of the fork combined with Vulkan and a Q\\_K quant you should be good.",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1729619709.0,
                    "parent_id": "t3_1g9ljbx",
                    "link_id": "t3_1g9ljbx",
                    "permalink": "/r/KoboldAI/comments/1g9ljbx/keep_crashing/lt7hvyk/"
                },
                {
                    "id": "lt7x6i3",
                    "author": "DemonicXz",
                    "body": "So that'd mean normal KoboldAI-client, and some commandline changes?\n\ntried just using the repo, and running it as in the guide, but then I get:\n\nRuntimeError: Failed to import transformers.modeling\\_utils because of the following error (look up to see its traceback):\n\ncannot import name 'split\\_torch\\_state\\_dict\\_into\\_shards' from 'huggingface\\_hub' (B:\\\\python\\\\lib\\\\site-packages\\\\huggingface\\_hub\\\\\\_\\_init\\_\\_.py)\n\n  \nalso in the process of installing the KoboldAI setup, but seems to take quite awhile from sourceforge",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1729624468.0,
                    "parent_id": "t1_lt7hvyk",
                    "link_id": "t3_1g9ljbx",
                    "permalink": "/r/KoboldAI/comments/1g9ljbx/keep_crashing/lt7x6i3/"
                },
                {
                    "id": "lt80plo",
                    "author": "henk717",
                    "body": "No I mean [https://koboldai.org/cpp](https://koboldai.org/cpp) instead of the ROCm fork.",
                    "score": 2,
                    "upvotes": 2,
                    "downvotes": 0,
                    "created_utc": 1729625553.0,
                    "parent_id": "t1_lt7x6i3",
                    "link_id": "t3_1g9ljbx",
                    "permalink": "/r/KoboldAI/comments/1g9ljbx/keep_crashing/lt80plo/"
                },
                {
                    "id": "lt83i5s",
                    "author": "DemonicXz",
                    "body": "oh alright, is there also a way to use a other frontend/GUI instead of the one it comes with? tried with tavernAI but doesnt seem to work, unless Im doing something wrong",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1729626406.0,
                    "parent_id": "t1_lt80plo",
                    "link_id": "t3_1g9ljbx",
                    "permalink": "/r/KoboldAI/comments/1g9ljbx/keep_crashing/lt83i5s/"
                }
            ]
        },
        {
            "id": "1g9bipm",
            "title": "Adventure Mode Ai is too inconsistent",
            "author": "KSB141000",
            "score": 3,
            "upvotes": 3,
            "downvotes": 0,
            "num_comments": 4,
            "created_utc": 1729577714.0,
            "url": "https://www.reddit.com/r/KoboldAI/comments/1g9bipm/adventure_mode_ai_is_too_inconsistent/",
            "permalink": "/r/KoboldAI/comments/1g9bipm/adventure_mode_ai_is_too_inconsistent/",
            "selftext": "I am trying to do something with adventure mode but the ai keeps ignoring what I put in memory and ignores my action to try and start an already started session again instead. I don't know what to do besides clicking retry a lot of times until ai finally decides to react to my actions. It takes multiple minutes for the ai to reply properly.",
            "comments": [
                {
                    "id": "lt54bjr",
                    "author": "Virtual-End-9003",
                    "body": "What model are you using?",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1729585133.0,
                    "parent_id": "t3_1g9bipm",
                    "link_id": "t3_1g9bipm",
                    "permalink": "/r/KoboldAI/comments/1g9bipm/adventure_mode_ai_is_too_inconsistent/lt54bjr/"
                },
                {
                    "id": "lt5tff3",
                    "author": "henk717",
                    "body": "Most models don't quite know how to do this since only a few have been trained on that mode.  \n[https://koboldai-koboldcpp-tiefighter.hf.space/](https://koboldai-koboldcpp-tiefighter.hf.space/) is one of the models that can, its also the default model on [https://koboldai.org/colabcpp](https://koboldai.org/colabcpp) if the other link is busy (That link can only have 10 people at a time).",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1729599691.0,
                    "parent_id": "t3_1g9bipm",
                    "link_id": "t3_1g9bipm",
                    "permalink": "/r/KoboldAI/comments/1g9bipm/adventure_mode_ai_is_too_inconsistent/lt5tff3/"
                },
                {
                    "id": "lt57trl",
                    "author": "KSB141000",
                    "body": "I am just using the ai horde setting for the ai. How can i choose the exact model?",
                    "score": 2,
                    "upvotes": 2,
                    "downvotes": 0,
                    "created_utc": 1729587583.0,
                    "parent_id": "t1_lt54bjr",
                    "link_id": "t3_1g9bipm",
                    "permalink": "/r/KoboldAI/comments/1g9bipm/adventure_mode_ai_is_too_inconsistent/lt57trl/"
                },
                {
                    "id": "lt5e0hl",
                    "author": "Virtual-End-9003",
                    "body": "Well i dont use horde i thought you were using local, but horde has the change ai button somewhere, just experiment with the avaliable ones",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1729591774.0,
                    "parent_id": "t1_lt57trl",
                    "link_id": "t3_1g9bipm",
                    "permalink": "/r/KoboldAI/comments/1g9bipm/adventure_mode_ai_is_too_inconsistent/lt5e0hl/"
                }
            ]
        },
        {
            "id": "1g8yher",
            "title": "Right pronoun for character in adventure mode",
            "author": "KSB141000",
            "score": 1,
            "upvotes": 1,
            "downvotes": 0,
            "num_comments": 5,
            "created_utc": 1729539086.0,
            "url": "https://www.reddit.com/r/KoboldAI/comments/1g8yher/right_pronoun_for_character_in_adventure_mode/",
            "permalink": "/r/KoboldAI/comments/1g8yher/right_pronoun_for_character_in_adventure_mode/",
            "selftext": "I am currently playing around with the adventure mode and am unsure what pronoun I should use for my actions. I read you on the wiki but I find it really weird to write that way. Would she/he works just as good?",
            "comments": [
                {
                    "id": "lt24hkq",
                    "author": "BangkokPadang",
                    "body": "Yeah he/she is fine.  just pay attention that youâ€™re keeping in the same tense and also make sure youâ€™re writing your sentences correctly. \n\nI.e. Donâ€™t discuss three menâ€™s names and then throw a he in there when itâ€™s not clear which proper noun the pronoun is referring to.\n\nI tend to use the character names a little more than I would when writing a story or talking to people. Any time Iâ€™m not 100% sure itâ€™s clear I use the name.",
                    "score": 7,
                    "upvotes": 7,
                    "downvotes": 0,
                    "created_utc": 1729540425.0,
                    "parent_id": "t3_1g8yher",
                    "link_id": "t3_1g8yher",
                    "permalink": "/r/KoboldAI/comments/1g8yher/right_pronoun_for_character_in_adventure_mode/lt24hkq/"
                },
                {
                    "id": "lt2qjuf",
                    "author": "schlammsuhler",
                    "body": "Some llm will just be too dumb to understand it. Yesterday i played with eva qwen 2.5 14b q4 and it kept refering to my sister as him and suddenly she had a penis...",
                    "score": 5,
                    "upvotes": 5,
                    "downvotes": 0,
                    "created_utc": 1729547124.0,
                    "parent_id": "t3_1g8yher",
                    "link_id": "t3_1g8yher",
                    "permalink": "/r/KoboldAI/comments/1g8yher/right_pronoun_for_character_in_adventure_mode/lt2qjuf/"
                },
                {
                    "id": "ltgwhq7",
                    "author": "RoutinePreparation36",
                    "body": "(\"no iwill kill you\" i say as i try to shoot him first) thats how i do it",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1729748245.0,
                    "parent_id": "t3_1g8yher",
                    "link_id": "t3_1g8yher",
                    "permalink": "/r/KoboldAI/comments/1g8yher/right_pronoun_for_character_in_adventure_mode/ltgwhq7/"
                },
                {
                    "id": "ltlulyu",
                    "author": "Sherlockyz",
                    "body": "With my llm in adventure mode I write in the first person \"I will go there\", and the text that the llm generate it's on the 3 person \"You walk towards that place\". In the memory I let the AI auto generate a resume of the story with my character name in the third person \"Sherlockz attacked that place\" and in the world info I use \"You\" for me and third person for other characters. Also I always like to let a few world info texts always active so the AI will know who I am and certain elements of the world and the story.",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1729816493.0,
                    "parent_id": "t3_1g8yher",
                    "link_id": "t3_1g8yher",
                    "permalink": "/r/KoboldAI/comments/1g8yher/right_pronoun_for_character_in_adventure_mode/ltlulyu/"
                },
                {
                    "id": "lt29967",
                    "author": "KSB141000",
                    "body": "Thanks that helps me a lot. I am used to do text rp so I can just use my knowleadge from there here aswell. I use only he/she prounouns there.",
                    "score": 2,
                    "upvotes": 2,
                    "downvotes": 0,
                    "created_utc": 1729541870.0,
                    "parent_id": "t1_lt24hkq",
                    "link_id": "t3_1g8yher",
                    "permalink": "/r/KoboldAI/comments/1g8yher/right_pronoun_for_character_in_adventure_mode/lt29967/"
                }
            ]
        },
        {
            "id": "1g8xcgn",
            "title": "Learning Kobold Little by little.",
            "author": "DoiCrunch",
            "score": 25,
            "upvotes": 25,
            "downvotes": 0,
            "num_comments": 2,
            "created_utc": 1729536330.0,
            "url": "https://i.redd.it/wyexykbhn5wd1.jpeg",
            "permalink": "/r/KoboldAI/comments/1g8xcgn/learning_kobold_little_by_little/",
            "selftext": "",
            "comments": [
                {
                    "id": "lt27oyf",
                    "author": "schlammsuhler",
                    "body": "Thats called overfitting\n\n\"Overfitting is all you need\" https://huggingface.co/anthracite-org/magnum-v2-72b",
                    "score": 3,
                    "upvotes": 3,
                    "downvotes": 0,
                    "created_utc": 1729541397.0,
                    "parent_id": "t3_1g8xcgn",
                    "link_id": "t3_1g8xcgn",
                    "permalink": "/r/KoboldAI/comments/1g8xcgn/learning_kobold_little_by_little/lt27oyf/"
                },
                {
                    "id": "lt8h61y",
                    "author": "Fair-Cash-6956",
                    "body": "Whatâ€™s tbis",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1729630519.0,
                    "parent_id": "t1_lt27oyf",
                    "link_id": "t3_1g8xcgn",
                    "permalink": "/r/KoboldAI/comments/1g8xcgn/learning_kobold_little_by_little/lt8h61y/"
                }
            ]
        },
        {
            "id": "1g8tolp",
            "title": "Will we ever see the ability to upload lorebooks directly to KoboldCCP?",
            "author": "Severe-Basket-2503",
            "score": 9,
            "upvotes": 9,
            "downvotes": 0,
            "num_comments": 19,
            "created_utc": 1729527537.0,
            "url": "https://www.reddit.com/r/KoboldAI/comments/1g8tolp/will_we_ever_see_the_ability_to_upload_lorebooks/",
            "permalink": "/r/KoboldAI/comments/1g8tolp/will_we_ever_see_the_ability_to_upload_lorebooks/",
            "selftext": "Hi All,\n\nJust putting out there because it's a feature I've been hoping for, for a very long time. I usually browse Chub AI for Character Card to play with, but they also have a a section on Lorebook where you can download them in a .jason format. \n\nI would love there to be a feature where when I upload a Character Card, I can add an entire Lorebook in this format in one go. At the moment, to have semblance of the same thing, I would have to upload individual entries from the Lorebook of my choice into the World tab. I find this Labourous and tiring. \n\nSo to the developers out there, PLEASE add a feature where after a lunach Kobold, I can upload a Lorebook in .jason format and do it in one stroke. I understand it will take up Context tokens, but I understand the limitations and willing to work with it. \n\nThanks!",
            "comments": [
                {
                    "id": "lt11huw",
                    "author": "VladimerePoutine",
                    "body": "Not sure what a lorebook is but I think I can upload a saved conversation in json format.",
                    "score": 4,
                    "upvotes": 4,
                    "downvotes": 0,
                    "created_utc": 1729528466.0,
                    "parent_id": "t3_1g8tolp",
                    "link_id": "t3_1g8tolp",
                    "permalink": "/r/KoboldAI/comments/1g8tolp/will_we_ever_see_the_ability_to_upload_lorebooks/lt11huw/"
                },
                {
                    "id": "lt1q2ei",
                    "author": "Skyline99",
                    "body": "What other Applications use worldbook besides Silly Tavern? I am asking because I am not aware of any others.",
                    "score": 3,
                    "upvotes": 3,
                    "downvotes": 0,
                    "created_utc": 1729536009.0,
                    "parent_id": "t3_1g8tolp",
                    "link_id": "t3_1g8tolp",
                    "permalink": "/r/KoboldAI/comments/1g8tolp/will_we_ever_see_the_ability_to_upload_lorebooks/lt1q2ei/"
                },
                {
                    "id": "lt1yjyy",
                    "author": "henk717",
                    "body": "Thats already a thing to my knowledge, just load it.",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1729538613.0,
                    "parent_id": "t3_1g8tolp",
                    "link_id": "t3_1g8tolp",
                    "permalink": "/r/KoboldAI/comments/1g8tolp/will_we_ever_see_the_ability_to_upload_lorebooks/lt1yjyy/"
                },
                {
                    "id": "lt28pd9",
                    "author": "Skyline99",
                    "body": "I understand",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1729541704.0,
                    "parent_id": "t3_1g8tolp",
                    "link_id": "t3_1g8tolp",
                    "permalink": "/r/KoboldAI/comments/1g8tolp/will_we_ever_see_the_ability_to_upload_lorebooks/lt28pd9/"
                },
                {
                    "id": "lt7590z",
                    "author": "Original-Success-784",
                    "body": "In Koboldcpp you can not load a fully Lorebook / World-Info, but you can define some unter Context.  \n  \nThe difference is in SillyTavern you can load a whole Book of Defenitions = Lorebook.  \nIn Koboldcpp you must define it separate under Action Menu -> Context -> World Info -> Add+ \n\nIt would be helpful/nice to have an Import Button unter World Info to load a json file (Lorebook).",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1729615819.0,
                    "parent_id": "t3_1g8tolp",
                    "link_id": "t3_1g8tolp",
                    "permalink": "/r/KoboldAI/comments/1g8tolp/will_we_ever_see_the_ability_to_upload_lorebooks/lt7590z/"
                },
                {
                    "id": "ltk4seb",
                    "author": "Nicholas_Matt_Quail",
                    "body": "The best way is to just connect to Silly Tavern through API. Silly Tavern has many functionalities and very useful options, which other backends and frontends UI simply do not utilize.",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1729796257.0,
                    "parent_id": "t3_1g8tolp",
                    "link_id": "t3_1g8tolp",
                    "permalink": "/r/KoboldAI/comments/1g8tolp/will_we_ever_see_the_ability_to_upload_lorebooks/ltk4seb/"
                },
                {
                    "id": "lt132nb",
                    "author": "Severe-Basket-2503",
                    "body": "A Lorebook is otherwise known as a worldbook. In Roleplay you have .json files that has a ton of information about the world you're playing in that's not in the main Charater Card. Or Rules and Definations that would swell the Caracter Card to unruly sizes. It's useful when you have multiple Charater Cards that all live in the same universe and the world stays the same with all it's rules in place. Or if you want to have a ton of information for the model but don't want to bloat the main Card. As the Lorebook can be split into sections so that it only the section you want the model to know at that time would be used.\n\n  \nFor example, \"you're walking down a path, an Orge jumps out in front of you!\" The model will then access the entry of the lorebook about Orges specifically as you said the keyword \"Orge\" and all their attributes so that the RP can stay conherant and consistent without falling apart because the model has gone over budget with it's context.\n\n  \nIt's a good way to keep roleplays conherant and consistent until the day we have models with neat inifinate context",
                    "score": 5,
                    "upvotes": 5,
                    "downvotes": 0,
                    "created_utc": 1729528953.0,
                    "parent_id": "t1_lt11huw",
                    "link_id": "t3_1g8tolp",
                    "permalink": "/r/KoboldAI/comments/1g8tolp/will_we_ever_see_the_ability_to_upload_lorebooks/lt132nb/"
                },
                {
                    "id": "lt581xm",
                    "author": "AlexysLovesLexxie",
                    "body": "Backyard uses Lorebooks.",
                    "score": 2,
                    "upvotes": 2,
                    "downvotes": 0,
                    "created_utc": 1729587744.0,
                    "parent_id": "t1_lt1q2ei",
                    "link_id": "t3_1g8tolp",
                    "permalink": "/r/KoboldAI/comments/1g8tolp/will_we_ever_see_the_ability_to_upload_lorebooks/lt581xm/"
                },
                {
                    "id": "lt27zpk",
                    "author": "Severe-Basket-2503",
                    "body": "I'm hoping to catch the eye of one of the Kobold Developers here and see if they have any plans, and if not, maybe insert a germ of an idea to do so. It would revolutionise RP in Kobold!",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1729541488.0,
                    "parent_id": "t1_lt1q2ei",
                    "link_id": "t3_1g8tolp",
                    "permalink": "/r/KoboldAI/comments/1g8tolp/will_we_ever_see_the_ability_to_upload_lorebooks/lt27zpk/"
                },
                {
                    "id": "lt40udd",
                    "author": "Embarrassed-Fan-3858",
                    "body": "Novelai. But there a paid service",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1729563878.0,
                    "parent_id": "t1_lt1q2ei",
                    "link_id": "t3_1g8tolp",
                    "permalink": "/r/KoboldAI/comments/1g8tolp/will_we_ever_see_the_ability_to_upload_lorebooks/lt40udd/"
                },
                {
                    "id": "lt5t02q",
                    "author": "Caderent",
                    "body": "Think correlations. Contacts, prices, descriptions, annotations. Anything that correlates with something else that can be described and tagged.",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1729599508.0,
                    "parent_id": "t1_lt1q2ei",
                    "link_id": "t3_1g8tolp",
                    "permalink": "/r/KoboldAI/comments/1g8tolp/will_we_ever_see_the_ability_to_upload_lorebooks/lt5t02q/"
                },
                {
                    "id": "lt27bu0",
                    "author": "Severe-Basket-2503",
                    "body": "Sorry, can you please expand on that, I don't know what you mean by that.",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1729541285.0,
                    "parent_id": "t1_lt1yjyy",
                    "link_id": "t3_1g8tolp",
                    "permalink": "/r/KoboldAI/comments/1g8tolp/will_we_ever_see_the_ability_to_upload_lorebooks/lt27bu0/"
                },
                {
                    "id": "ltj795m",
                    "author": "Severe-Basket-2503",
                    "body": "Yes exactly, this is what i'm asking for, are their Kobold developers who are in this reddit that I can ask directly?",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1729786304.0,
                    "parent_id": "t1_lt7590z",
                    "link_id": "t3_1g8tolp",
                    "permalink": "/r/KoboldAI/comments/1g8tolp/will_we_ever_see_the_ability_to_upload_lorebooks/ltj795m/"
                },
                {
                    "id": "lt2qyva",
                    "author": "VladimerePoutine",
                    "body": "Oh wow that's cool. Thanks for detailing that, it would be really useful for all sorts of things.",
                    "score": 3,
                    "upvotes": 3,
                    "downvotes": 0,
                    "created_utc": 1729547262.0,
                    "parent_id": "t1_lt132nb",
                    "link_id": "t3_1g8tolp",
                    "permalink": "/r/KoboldAI/comments/1g8tolp/will_we_ever_see_the_ability_to_upload_lorebooks/lt2qyva/"
                },
                {
                    "id": "lt379pq",
                    "author": "henk717",
                    "body": "Loading those lorebooks should be possible with the regular load button you'd also use to load a KoboldAI save file, it then imports them.",
                    "score": 2,
                    "upvotes": 2,
                    "downvotes": 0,
                    "created_utc": 1729553002.0,
                    "parent_id": "t1_lt27bu0",
                    "link_id": "t3_1g8tolp",
                    "permalink": "/r/KoboldAI/comments/1g8tolp/will_we_ever_see_the_ability_to_upload_lorebooks/lt379pq/"
                },
                {
                    "id": "lu76g79",
                    "author": "Original-Success-784",
                    "body": "You can make a request or open an issue under: [https://github.com/LostRuins/koboldcpp](https://github.com/LostRuins/koboldcpp) ;  \nor contact the developer directly reddit u/HadesThrowaway or on Koboldcpp-Discord-Server.\n\nOr you use KoboldCpp as Backend and SillyTavern as Frontend and load the Lorebook!",
                    "score": 2,
                    "upvotes": 2,
                    "downvotes": 0,
                    "created_utc": 1730133063.0,
                    "parent_id": "t1_ltj795m",
                    "link_id": "t3_1g8tolp",
                    "permalink": "/r/KoboldAI/comments/1g8tolp/will_we_ever_see_the_ability_to_upload_lorebooks/lu76g79/"
                },
                {
                    "id": "lt6z6fp",
                    "author": "Severe-Basket-2503",
                    "body": "If i do that, it'll delete the Character card that I'm roleplaying with.",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1729613963.0,
                    "parent_id": "t1_lt379pq",
                    "link_id": "t3_1g8tolp",
                    "permalink": "/r/KoboldAI/comments/1g8tolp/will_we_ever_see_the_ability_to_upload_lorebooks/lt6z6fp/"
                },
                {
                    "id": "luu5rvp",
                    "author": "HadesThrowaway",
                    "body": "u/concedo is not me, that's just someone's random account.\n\n I'm concedo on the Kobold discord, u/HadesThrowaway here, and LostRuins on github",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1730458239.0,
                    "parent_id": "t1_lu76g79",
                    "link_id": "t3_1g8tolp",
                    "permalink": "/r/KoboldAI/comments/1g8tolp/will_we_ever_see_the_ability_to_upload_lorebooks/luu5rvp/"
                },
                {
                    "id": "luu9no5",
                    "author": "Original-Success-784",
                    "body": "Sorry for the misunderstanding. I edit my post!",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1730460325.0,
                    "parent_id": "t1_luu5rvp",
                    "link_id": "t3_1g8tolp",
                    "permalink": "/r/KoboldAI/comments/1g8tolp/will_we_ever_see_the_ability_to_upload_lorebooks/luu9no5/"
                }
            ]
        },
        {
            "id": "1g8qv82",
            "title": "Browserllama now supports firefox",
            "author": "Ok_Effort_5849",
            "score": 4,
            "upvotes": 4,
            "downvotes": 0,
            "num_comments": 0,
            "created_utc": 1729520504.0,
            "url": "https://www.reddit.com/r/KoboldAI/comments/1g8qv82/browserllama_now_supports_firefox/",
            "permalink": "/r/KoboldAI/comments/1g8qv82/browserllama_now_supports_firefox/",
            "selftext": "context: [https://www.reddit.com/r/KoboldAI/comments/1g0kjce/i\\_made\\_a\\_web\\_extension\\_that\\_lets\\_you\\_summarise/](https://www.reddit.com/r/KoboldAI/comments/1g0kjce/i_made_a_web_extension_that_lets_you_summarise/)\n\nfirefox version is still in beta and will receive keep receiving bug fixes and improvements, try it out and tell me what you think!\n\nwebstore link: [https://addons.mozilla.org/en-GB/firefox/addon/browserllama/](https://addons.mozilla.org/en-GB/firefox/addon/browserllama/)",
            "comments": []
        },
        {
            "id": "1g8lty3",
            "title": "its pop up when i try to open play remote.bat",
            "author": "Fluffy_Review1395",
            "score": 2,
            "upvotes": 2,
            "downvotes": 0,
            "num_comments": 0,
            "created_utc": 1729504039.0,
            "url": "https://www.reddit.com/r/KoboldAI/comments/1g8lty3/its_pop_up_when_i_try_to_open_play_remotebat/",
            "permalink": "/r/KoboldAI/comments/1g8lty3/its_pop_up_when_i_try_to_open_play_remotebat/",
            "selftext": "i did everythin remove i files try to reinstall its work before but now it doesnt\n\nfallowed video\n\n[https://www.youtube.com/watch?v=8BW\\_wbGQV0A&t=185s](https://www.youtube.com/watch?v=8BW_wbGQV0A&t=185s)\n\nhttps://preview.redd.it/0uhhsrijz2wd1.png?width=872&format=png&auto=webp&s=b9505ee0f5bb0cfdd1c1e5ddafc109bf1aa15101\n\n",
            "comments": []
        },
        {
            "id": "1g8i86o",
            "title": "Kobold ROCm won't read my GPU",
            "author": "Professional_Yak2246",
            "score": 2,
            "upvotes": 2,
            "downvotes": 0,
            "num_comments": 3,
            "created_utc": 1729487577.0,
            "url": "https://www.reddit.com/r/KoboldAI/comments/1g8i86o/kobold_rocm_wont_read_my_gpu/",
            "permalink": "/r/KoboldAI/comments/1g8i86o/kobold_rocm_wont_read_my_gpu/",
            "selftext": "I have 7900xt and using windows. Kobold ROCm only reads gfx1100 and gfx 1036 and won't show my gpu. when using gfx 1100 it's slow when processing the tokens.\n\nhow can i fix this so it reads my gpu",
            "comments": [
                {
                    "id": "lsz8zen",
                    "author": "Picard12832",
                    "body": "gfx1100 is your 7900 xt",
                    "score": 4,
                    "upvotes": 4,
                    "downvotes": 0,
                    "created_utc": 1729501434.0,
                    "parent_id": "t3_1g8i86o",
                    "link_id": "t3_1g8i86o",
                    "permalink": "/r/KoboldAI/comments/1g8i86o/kobold_rocm_wont_read_my_gpu/lsz8zen/"
                },
                {
                    "id": "lszmyon",
                    "author": "henk717",
                    "body": "As Picard said the names on the AMD side are named after the graphics chip not after the card so GFX1100 is your 7900XT and the GFX1036 is the integrated graphics. We know that MMQ and FlashAttention can slow things down on the AMD side. Also make sure you are using a model that can fully fit on your GPU with all layers offloaded, if the model + the context size is larger than your GPU it will end up being slow.",
                    "score": 2,
                    "upvotes": 2,
                    "downvotes": 0,
                    "created_utc": 1729510113.0,
                    "parent_id": "t3_1g8i86o",
                    "link_id": "t3_1g8i86o",
                    "permalink": "/r/KoboldAI/comments/1g8i86o/kobold_rocm_wont_read_my_gpu/lszmyon/"
                },
                {
                    "id": "lt3knfb",
                    "author": "Professional_Yak2246",
                    "body": "It's the BLAS that's slow. I don't understand. Using regular VULKAN it's fast",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1729557902.0,
                    "parent_id": "t1_lszmyon",
                    "link_id": "t3_1g8i86o",
                    "permalink": "/r/KoboldAI/comments/1g8i86o/kobold_rocm_wont_read_my_gpu/lt3knfb/"
                }
            ]
        },
        {
            "id": "1g7icj2",
            "title": "I used 2 LLMs to write an app (LLM Convo) that lets 2 LLMs talk to each other via openai endpoints.",
            "author": "mamelukturbo",
            "score": 2,
            "upvotes": 2,
            "downvotes": 0,
            "num_comments": 1,
            "created_utc": 1729370947.0,
            "url": "https://www.reddit.com/r/KoboldAI/comments/1g7icj2/i_used_2_llms_to_write_an_app_llm_convo_that_lets/",
            "permalink": "/r/KoboldAI/comments/1g7icj2/i_used_2_llms_to_write_an_app_llm_convo_that_lets/",
            "selftext": "https://preview.redd.it/a6f21xgvyrvd1.png?width=1222&format=png&auto=webp&s=d495f9bf5b78673a21bfe546a09614019c3e615b\n\nOnly tested it with 2 instances of koboldcpp, it also works with single instance simulating 2 personas. My programmers (the LLMs) assured me it would work with all openai endpoints :D I have zero experience with python, but it works well enough for a fun experiment considering my last programming experience was some 25 years ago in Turbo Pascal. This took few hours and maxed out daily api quota on both claude 3.5 sonnet and chatgpt-4o-latest. I used open-webui as frontend to \"develop\" this. It's dockerized so as to not pollute the base system.\n\n[https://github.com/hugalafutro/llm-convo](https://github.com/hugalafutro/llm-convo)",
            "comments": [
                {
                    "id": "lsqt1sj",
                    "author": "henk717",
                    "body": "Fun idea! As as side note if you want to use KoboldAI's UI with claude you can use [koboldai.net](http://koboldai.net) no install needed.",
                    "score": 2,
                    "upvotes": 2,
                    "downvotes": 0,
                    "created_utc": 1729372130.0,
                    "parent_id": "t3_1g7icj2",
                    "link_id": "t3_1g7icj2",
                    "permalink": "/r/KoboldAI/comments/1g7icj2/i_used_2_llms_to_write_an_app_llm_convo_that_lets/lsqt1sj/"
                }
            ]
        },
        {
            "id": "1g7gs1x",
            "title": "Is there a UI that has Perchance.org chat/story features?",
            "author": "DrCyanide3D",
            "score": 6,
            "upvotes": 6,
            "downvotes": 0,
            "num_comments": 8,
            "created_utc": 1729366505.0,
            "url": "https://www.reddit.com/r/KoboldAI/comments/1g7gs1x/is_there_a_ui_that_has_perchanceorg_chatstory/",
            "permalink": "/r/KoboldAI/comments/1g7gs1x/is_there_a_ui_that_has_perchanceorg_chatstory/",
            "selftext": "I've been using perchance.org's [story generator](https://perchance.org/ai-story-generator) and [chat](https://perchance.org/ai-chat) for a while now, and I really enjoy the option to suggest to the AI what should happen next and see that concept get flushed out. For chat, they not only have a way to pick which character should talk next, but when you imitate a character you can \"auto improve\" whatever you type to make it more flushed out.\n\nI think those features are extremely useful for making compelling stories, and I'd love to have them be available to me locally, but I'm not sure how. I don't know if there's a different front end that has these features already, or if it's beyond the ability of Kobold to do it. If anyone could help point me in the right direction I'd greatly appreciate it.",
            "comments": [
                {
                    "id": "lsqumx0",
                    "author": "henk717",
                    "body": "The closest I can think off is writing in instruct mode where you tell it what should happen next and it writes the next bit, you can remove the instruct prompts after the fact. So for our own stuff i'd take this as a suggestion to allow automatically removing the instruction rather than submitting it in the story.\n\nFor the chat mode I don't think its as obvious to do it in any of the UI's since the majority of users don't want their messages rewritten it never became a feature in any of the UI's I know.\n\nI passed your thread to the KoboldAI Lite developer so he can consider the ideas.",
                    "score": 6,
                    "upvotes": 6,
                    "downvotes": 0,
                    "created_utc": 1729372688.0,
                    "parent_id": "t3_1g7gs1x",
                    "link_id": "t3_1g7gs1x",
                    "permalink": "/r/KoboldAI/comments/1g7gs1x/is_there_a_ui_that_has_perchanceorg_chatstory/lsqumx0/"
                },
                {
                    "id": "lsr9bhm",
                    "author": "Steelbat1",
                    "body": "This is perfect for what I'm looking for as well",
                    "score": 2,
                    "upvotes": 2,
                    "downvotes": 0,
                    "created_utc": 1729378103.0,
                    "parent_id": "t3_1g7gs1x",
                    "link_id": "t3_1g7gs1x",
                    "permalink": "/r/KoboldAI/comments/1g7gs1x/is_there_a_ui_that_has_perchanceorg_chatstory/lsr9bhm/"
                },
                {
                    "id": "lssdps5",
                    "author": "RealBiggly",
                    "body": "I think you mean 'fleshed out', not flushed.\n\nYou can just create a storyteller character and give the character instructions?",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1729393858.0,
                    "parent_id": "t3_1g7gs1x",
                    "link_id": "t3_1g7gs1x",
                    "permalink": "/r/KoboldAI/comments/1g7gs1x/is_there_a_ui_that_has_perchanceorg_chatstory/lssdps5/"
                },
                {
                    "id": "lsterkc",
                    "author": "henk717",
                    "body": "Update: The interactive storywriter portion was already possible using the regex replace feature. Its been added as the interactive storywriter scenario in the scenarios menu. You can test it on koboldai.net (and either link up your own KoboldCpp to that or then save the file and load it in your own KoboldCpp). The next release of KoboldCpp should have it bundled.",
                    "score": 5,
                    "upvotes": 5,
                    "downvotes": 0,
                    "created_utc": 1729415030.0,
                    "parent_id": "t1_lsqumx0",
                    "link_id": "t3_1g7gs1x",
                    "permalink": "/r/KoboldAI/comments/1g7gs1x/is_there_a_ui_that_has_perchanceorg_chatstory/lsterkc/"
                },
                {
                    "id": "lst6fke",
                    "author": "DrCyanide3D",
                    "body": "The problem with that approach is even if the AI worked perfectly the flow of the story would be interrupted every few sentences with another instruction. Yes, it could be edited later, but it's a significantly worse experience than the way Perchance handles it.",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1729409254.0,
                    "parent_id": "t1_lssdps5",
                    "link_id": "t3_1g7gs1x",
                    "permalink": "/r/KoboldAI/comments/1g7gs1x/is_there_a_ui_that_has_perchanceorg_chatstory/lst6fke/"
                },
                {
                    "id": "lst7utb",
                    "author": "RealBiggly",
                    "body": "Well I'm around 9 chapters into a novel doing that, and often I just type \"go\" and let it roll (that was part of the character instructions, to just continue if I type go).",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1729410247.0,
                    "parent_id": "t1_lst6fke",
                    "link_id": "t3_1g7gs1x",
                    "permalink": "/r/KoboldAI/comments/1g7gs1x/is_there_a_ui_that_has_perchanceorg_chatstory/lst7utb/"
                },
                {
                    "id": "ltkvvjr",
                    "author": "AnonymousAardvark22",
                    "body": "So you are using Kobold to talk to a novel? If so what is the format and how do I load it instead of a character?",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1729804410.0,
                    "parent_id": "t1_lst7utb",
                    "link_id": "t3_1g7gs1x",
                    "permalink": "/r/KoboldAI/comments/1g7gs1x/is_there_a_ui_that_has_perchanceorg_chatstory/ltkvvjr/"
                },
                {
                    "id": "ltnjhnv",
                    "author": "RealBiggly",
                    "body": "Noo... I mean I'm using an AI to help me write it. I just give general directions then let it do its thing, sometimes trying out different models as I go. Currently using Command R.",
                    "score": 2,
                    "upvotes": 2,
                    "downvotes": 0,
                    "created_utc": 1729845636.0,
                    "parent_id": "t1_ltkvvjr",
                    "link_id": "t3_1g7gs1x",
                    "permalink": "/r/KoboldAI/comments/1g7gs1x/is_there_a_ui_that_has_perchanceorg_chatstory/ltnjhnv/"
                }
            ]
        },
        {
            "id": "1g7fu69",
            "title": "Kobold United but only the UI?",
            "author": "hurrdurrimanaccount",
            "score": 4,
            "upvotes": 4,
            "downvotes": 0,
            "num_comments": 1,
            "created_utc": 1729363930.0,
            "url": "https://www.reddit.com/r/KoboldAI/comments/1g7fu69/kobold_united_but_only_the_ui/",
            "permalink": "/r/KoboldAI/comments/1g7fu69/kobold_united_but_only_the_ui/",
            "selftext": "is there a way to have United but only UI funtionality? 20gb is a bit heavy for just a UI imo.\n\n",
            "comments": [
                {
                    "id": "lsqtm9g",
                    "author": "henk717",
                    "body": "This is not (easily) possible due to how tightly integrated everything is, we developed KoboldAI Lite to solve this issue. You might be able to cut down on most of it though, but it relies on Huggingface's tokenizer for the token counting.",
                    "score": 3,
                    "upvotes": 3,
                    "downvotes": 0,
                    "created_utc": 1729372332.0,
                    "parent_id": "t3_1g7fu69",
                    "link_id": "t3_1g7fu69",
                    "permalink": "/r/KoboldAI/comments/1g7fu69/kobold_united_but_only_the_ui/lsqtm9g/"
                }
            ]
        },
        {
            "id": "1g6n5r4",
            "title": "DRY and XTC Sampler Order",
            "author": "Animus_777",
            "score": 12,
            "upvotes": 12,
            "downvotes": 0,
            "num_comments": 2,
            "created_utc": 1729271031.0,
            "url": "https://www.reddit.com/r/KoboldAI/comments/1g6n5r4/dry_and_xtc_sampler_order/",
            "permalink": "/r/KoboldAI/comments/1g6n5r4/dry_and_xtc_sampler_order/",
            "selftext": "What is the Sampling Order of DRY and XTC samplers? They are not numbered in Kobold UI and they are not listed in Silly Tavern's Sampler Order (with kcpp backend).\n\nhttps://preview.redd.it/j2b2gbu4qjvd1.png?width=435&format=png&auto=webp&s=d3b8fb1d27cdad78f60a2bac89dda1d3a57d9bc0\n\nhttps://preview.redd.it/fwz9gjg5qjvd1.png?width=454&format=png&auto=webp&s=734856b623533a5fdc036cb978c3b9dc6b787dca\n\n",
            "comments": [
                {
                    "id": "lso4det",
                    "author": "HadesThrowaway",
                    "body": "DRY is always first  \nXTC is always last",
                    "score": 6,
                    "upvotes": 6,
                    "downvotes": 0,
                    "created_utc": 1729336870.0,
                    "parent_id": "t3_1g6n5r4",
                    "link_id": "t3_1g6n5r4",
                    "permalink": "/r/KoboldAI/comments/1g6n5r4/dry_and_xtc_sampler_order/lso4det/"
                },
                {
                    "id": "lspbvhz",
                    "author": "Animus_777",
                    "body": "Thank you! Makes sense.",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1729354286.0,
                    "parent_id": "t1_lso4det",
                    "link_id": "t3_1g6n5r4",
                    "permalink": "/r/KoboldAI/comments/1g6n5r4/dry_and_xtc_sampler_order/lspbvhz/"
                }
            ]
        },
        {
            "id": "1g6jeul",
            "title": "AI Horde How to check Kudos Balance",
            "author": "AveryVeilfaire",
            "score": 2,
            "upvotes": 2,
            "downvotes": 0,
            "num_comments": 3,
            "created_utc": 1729261787.0,
            "url": "https://www.reddit.com/r/KoboldAI/comments/1g6jeul/ai_horde_how_to_check_kudos_balance/",
            "permalink": "/r/KoboldAI/comments/1g6jeul/ai_horde_how_to_check_kudos_balance/",
            "selftext": "I can't seem to find this answer easily, or anywhere to 'log in' on the website, is there an easy way to check my kudos balance or amount?\n\nI use KoboldCpp to gen.",
            "comments": [
                {
                    "id": "lsj8una",
                    "author": "pyroserenus",
                    "body": "logging in on [lite.koboldai.net](http://lite.koboldai.net) you can check your balance as well as manage your workers.",
                    "score": 3,
                    "upvotes": 3,
                    "downvotes": 0,
                    "created_utc": 1729262785.0,
                    "parent_id": "t3_1g6jeul",
                    "link_id": "t3_1g6jeul",
                    "permalink": "/r/KoboldAI/comments/1g6jeul/ai_horde_how_to_check_kudos_balance/lsj8una/"
                },
                {
                    "id": "lsjjjzn",
                    "author": "AveryVeilfaire",
                    "body": "I found that [https://tinybots.net/artbot/settings](https://tinybots.net/artbot/settings) has an option to log in and shows stats as well.",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1729266199.0,
                    "parent_id": "t3_1g6jeul",
                    "link_id": "t3_1g6jeul",
                    "permalink": "/r/KoboldAI/comments/1g6jeul/ai_horde_how_to_check_kudos_balance/lsjjjzn/"
                },
                {
                    "id": "lsjwof8",
                    "author": "kiselsa",
                    "body": "https://stablehorde.net/register",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1729270347.0,
                    "parent_id": "t3_1g6jeul",
                    "link_id": "t3_1g6jeul",
                    "permalink": "/r/KoboldAI/comments/1g6jeul/ai_horde_how_to_check_kudos_balance/lsjwof8/"
                }
            ]
        },
        {
            "id": "1g617j7",
            "title": "Is there a proper download guide?\n",
            "author": "Fluffy_Review1395",
            "score": 2,
            "upvotes": 2,
            "downvotes": 0,
            "num_comments": 9,
            "created_utc": 1729198570.0,
            "url": "https://www.reddit.com/r/KoboldAI/comments/1g617j7/is_there_a_proper_download_guide/",
            "permalink": "/r/KoboldAI/comments/1g617j7/is_there_a_proper_download_guide/",
            "selftext": "i try to install the pc and i cant opened yet. is anybody can suggest me a tutorial video ? ",
            "comments": [
                {
                    "id": "lsgjevc",
                    "author": "Cool-Hornet4434",
                    "body": "https://github.com/LostRuins/koboldcpp\nGo there... scroll down to find (Precompiled Binary, Recommended)    Click on the download link, and find the proper binary for you.  You will need to be able to figure out if you need CUDA 12 or not, or if you want to run it without a video card.\n\nIf you are using an AMD card you want the version at https://github.com/YellowRoseCx/koboldcpp-rocm",
                    "score": 5,
                    "upvotes": 5,
                    "downvotes": 0,
                    "created_utc": 1729214651.0,
                    "parent_id": "t3_1g617j7",
                    "link_id": "t3_1g617j7",
                    "permalink": "/r/KoboldAI/comments/1g617j7/is_there_a_proper_download_guide/lsgjevc/"
                },
                {
                    "id": "lslamfp",
                    "author": "BangkokPadang",
                    "body": "What graphics card do you have and Iâ€™ll tell you which binary to download.\n\nAssuming youâ€™re on windows itâ€™s just a .exe file so thereâ€™s not much guidance needed to install it from there.",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1729286552.0,
                    "parent_id": "t3_1g617j7",
                    "link_id": "t3_1g617j7",
                    "permalink": "/r/KoboldAI/comments/1g617j7/is_there_a_proper_download_guide/lslamfp/"
                },
                {
                    "id": "lspl354",
                    "author": "ItsMeKarizma",
                    "body": "Windows or Linux?  \n  \nOn windows, it's quite easy. It's a .exe file `koboldcpp.exe` that you can find here [https://github.com/LostRuins/koboldcpp/releases/tag/v1.76](https://github.com/LostRuins/koboldcpp/releases/tag/v1.76) and you simply execute it as you would do with any other app.  \n  \nIf you're on Linux, from what I know it's kind of the same process but it's a file named `koboldcpp-linux-x64-cuda1150` that you have to execute (You can find it on the above link).\n\nTo download AI models, visit [http://huggingface.co](http://huggingface.co) .\n\nThis should work if you have an NVIDIA GPU and it's pretty straight forward in my opinion.  \nIf you have an AMD GPU (like me), it won't be as easy. There are a lot of steps you'll have to follow and it mostly works only on Linux (unless there are good news that I haven't heard yet for Windows?).\n\nAnyways, if you need a tutorial video, someone with an RTX GPU on Windows uploaded this [https://www.youtube.com/watch?v=OGTpjgNRlF4](https://www.youtube.com/watch?v=OGTpjgNRlF4)\n\nGive it a try, it might help you out. Good luck!",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1729357376.0,
                    "parent_id": "t3_1g617j7",
                    "link_id": "t3_1g617j7",
                    "permalink": "/r/KoboldAI/comments/1g617j7/is_there_a_proper_download_guide/lspl354/"
                },
                {
                    "id": "lsnxulw",
                    "author": "Fluffy_Review1395",
                    "body": "rtx 2060 6gb",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1729332997.0,
                    "parent_id": "t1_lslamfp",
                    "link_id": "t3_1g617j7",
                    "permalink": "/r/KoboldAI/comments/1g617j7/is_there_a_proper_download_guide/lsnxulw/"
                },
                {
                    "id": "lsxp8oy",
                    "author": "Fluffy_Review1395",
                    "body": "but its gonna be run on rtx 2060 6gb ?",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1729472743.0,
                    "parent_id": "t1_lspl354",
                    "link_id": "t3_1g617j7",
                    "permalink": "/r/KoboldAI/comments/1g617j7/is_there_a_proper_download_guide/lsxp8oy/"
                },
                {
                    "id": "lsxvwn1",
                    "author": "Fluffy_Review1395",
                    "body": "i try to fallow the video, and its say could not load the text model ? how i m gonna run the model",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1729475231.0,
                    "parent_id": "t1_lspl354",
                    "link_id": "t3_1g617j7",
                    "permalink": "/r/KoboldAI/comments/1g617j7/is_there_a_proper_download_guide/lsxvwn1/"
                },
                {
                    "id": "lt7v2bd",
                    "author": "ItsMeKarizma",
                    "body": "What model is it? It has to be a GGUF model from what I know. Try something like this one:  \n[https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF/blob/main/llama-2-7b-chat.Q3\\_K\\_S.gguf](https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF/blob/main/llama-2-7b-chat.Q3_K_S.gguf)\n\nThis model should fit entirely in the 6 GB of VRAM but if it doesn't, simply offload part of it into the RAM (use less GPU layers). The model I referenced is obviously just for testing (it's \"bad\" quality in my opinion). After you get good results as in \"it works\", then you could try other models but you'll have to depend on your RAM.",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1729623804.0,
                    "parent_id": "t1_lsxvwn1",
                    "link_id": "t3_1g617j7",
                    "permalink": "/r/KoboldAI/comments/1g617j7/is_there_a_proper_download_guide/lt7v2bd/"
                },
                {
                    "id": "lt9ozr5",
                    "author": "Fluffy_Review1395",
                    "body": "but i want to use like pymagalion 6b models i want to download normal kobaldai",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1729645381.0,
                    "parent_id": "t1_lt7v2bd",
                    "link_id": "t3_1g617j7",
                    "permalink": "/r/KoboldAI/comments/1g617j7/is_there_a_proper_download_guide/lt9ozr5/"
                },
                {
                    "id": "ltc3noo",
                    "author": "ItsMeKarizma",
                    "body": "You can download the model (quantized) here: [https://huggingface.co/Crataco/Pygmalion-6B-GGML](https://huggingface.co/Crataco/Pygmalion-6B-GGML)\n\nAll you have to do is pick the one you think will work best for you (do some research, it's not hard to understand what they actually mean) and then load it in `koboldcpp`.  \n\n\nI've got no idea what \\`normal\\` koboldai is but `koboldcpp` should be able to load `.bin` files which means that the model you're looking for should work (see the link above for the `pygmalion 6b` model).",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1729688874.0,
                    "parent_id": "t1_lt9ozr5",
                    "link_id": "t3_1g617j7",
                    "permalink": "/r/KoboldAI/comments/1g617j7/is_there_a_proper_download_guide/ltc3noo/"
                }
            ]
        },
        {
            "id": "1g5t3mc",
            "title": "Balancing Min-P and Temperature",
            "author": "Animus_777",
            "score": 1,
            "upvotes": 1,
            "downvotes": 0,
            "num_comments": 7,
            "created_utc": 1729177696.0,
            "url": "https://www.reddit.com/r/KoboldAI/comments/1g5t3mc/balancing_minp_and_temperature/",
            "permalink": "/r/KoboldAI/comments/1g5t3mc/balancing_minp_and_temperature/",
            "selftext": "I'm trying to understand how these 2 work together. Let's assume sampling order starts with Min-P and then Temp is applied last. Min-P is set to 0.1 and Temp is 1.2. The character in roleplay scenario with this settings is erratic and fidgety. I want to make him more sane. What should I change first? Lower Temperature or increase Min-P?\n\nIn general I would like to understand when you would choose to tweak one over the other. What is the difference between:\n\n1. Min-P = 0.1 + Temp = 1.2\n2. Min-P = 0.01 + Temp = 0.7\n\nWouldn't both combination produce similar coherent results?  \nCan somebody give me an example what next words/tokens would model choose when trying to continue the following sentence with the two presets mentioned above:\n\n>\"He entered the room and saw...\"",
            "comments": [
                {
                    "id": "lsdkeqb",
                    "author": "noneabove1182",
                    "body": "If you want to see how temperature and other settings affect output you can check out this cool tool:Â \n\n\nhttps://artefact2.github.io/llm-sampling/index.xhtml",
                    "score": 10,
                    "upvotes": 10,
                    "downvotes": 0,
                    "created_utc": 1729178810.0,
                    "parent_id": "t3_1g5t3mc",
                    "link_id": "t3_1g5t3mc",
                    "permalink": "/r/KoboldAI/comments/1g5t3mc/balancing_minp_and_temperature/lsdkeqb/"
                },
                {
                    "id": "lshgnvv",
                    "author": "SiEgE-F1",
                    "body": "You need to understand what Min-P is - Its sole purpose is to make sure you don't have trash in the output. Some models produce more trash, while others don't.\n\nJust make sure min-p goes AFTER temps at the sampler order.\n\nMy personal advice is to keep Temp at 1.0, keep Min-P at 0.02, only changing it a bit up to 0.05-0.1 when you have weird symbols or plain wrong sentences.  \nYou should not play around with Temp, as it is the model's \"default settings\". Only change it when the model devs specifically stated that the model operates best at another Temp value(like, I think, Nemo model was suggested to be used at 0.3 by the devs themselves).\n\nIf the result is not what you want - there are probably solid reasons for that, like the model not being tuned enough for the job you're trying to make it do, or that you should focus on other samplers, like XTC/DRY/smoothing/penalty samplers and etc. Temp fiddling is only usable when you need to \"dry out\" the creativity, like for the coding/math/data retrieval models.",
                    "score": 2,
                    "upvotes": 2,
                    "downvotes": 0,
                    "created_utc": 1729229567.0,
                    "parent_id": "t3_1g5t3mc",
                    "link_id": "t3_1g5t3mc",
                    "permalink": "/r/KoboldAI/comments/1g5t3mc/balancing_minp_and_temperature/lshgnvv/"
                },
                {
                    "id": "lsdq564",
                    "author": "Animus_777",
                    "body": "Great tool! You can even change sampling order by dragging blocks.",
                    "score": 2,
                    "upvotes": 2,
                    "downvotes": 0,
                    "created_utc": 1729180617.0,
                    "parent_id": "t1_lsdkeqb",
                    "link_id": "t3_1g5t3mc",
                    "permalink": "/r/KoboldAI/comments/1g5t3mc/balancing_minp_and_temperature/lsdq564/"
                },
                {
                    "id": "lsj875n",
                    "author": "Animus_777",
                    "body": ">Just make sure min-p goes AFTER temps at the sampler order.\n\nWhy? What is the advantage of doing it this way? Could you give a real practical example showing the difference?",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1729262572.0,
                    "parent_id": "t1_lshgnvv",
                    "link_id": "t3_1g5t3mc",
                    "permalink": "/r/KoboldAI/comments/1g5t3mc/balancing_minp_and_temperature/lsj875n/"
                },
                {
                    "id": "lsdx3nq",
                    "author": "noneabove1182",
                    "body": "oh snap i didn't even notice that part, i just love fiddling with the sliders and seeing how it affects the selection, it's super cool already but order is also very important, good catch!",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1729182833.0,
                    "parent_id": "t1_lsdq564",
                    "link_id": "t3_1g5t3mc",
                    "permalink": "/r/KoboldAI/comments/1g5t3mc/balancing_minp_and_temperature/lsdx3nq/"
                },
                {
                    "id": "lskzcne",
                    "author": "SiEgE-F1",
                    "body": "Why won't you try it manually?  \nThe link from above: [https://artefact2.github.io/llm-sampling/index.xhtml](https://artefact2.github.io/llm-sampling/index.xhtml)  \nYou can drag the sampler windows around. The one that goes first would is the first in the order.\n\nIn short: Wrong order can leave you without good tokens.",
                    "score": 2,
                    "upvotes": 2,
                    "downvotes": 0,
                    "created_utc": 1729282776.0,
                    "parent_id": "t1_lsj875n",
                    "link_id": "t3_1g5t3mc",
                    "permalink": "/r/KoboldAI/comments/1g5t3mc/balancing_minp_and_temperature/lskzcne/"
                },
                {
                    "id": "lsej9ks",
                    "author": "Animus_777",
                    "body": "All credits go to [this guy](https://www.reddit.com/r/SillyTavernAI/comments/1g5t4f1/comment/lsdpzdo/)",
                    "score": 2,
                    "upvotes": 2,
                    "downvotes": 0,
                    "created_utc": 1729189784.0,
                    "parent_id": "t1_lsdx3nq",
                    "link_id": "t3_1g5t3mc",
                    "permalink": "/r/KoboldAI/comments/1g5t3mc/balancing_minp_and_temperature/lsej9ks/"
                }
            ]
        },
        {
            "id": "1g55eoj",
            "title": "koboldcpp --highpriority flag",
            "author": "morbidSuplex",
            "score": 3,
            "upvotes": 3,
            "downvotes": 0,
            "num_comments": 2,
            "created_utc": 1729100767.0,
            "url": "https://www.reddit.com/r/KoboldAI/comments/1g55eoj/koboldcpp_highpriority_flag/",
            "permalink": "/r/KoboldAI/comments/1g55eoj/koboldcpp_highpriority_flag/",
            "selftext": "Hi all,\nWhat does the experimental --highpriority flag do exactly in koboldcpp? It doesn't seem to be documented at all. Does this mean high priority towards GPU or CPU?\nThanks all.",
            "comments": [
                {
                    "id": "ls99n6h",
                    "author": "_Erilaz",
                    "body": "It's CPU priority, the same as elevating the priority in task manager. And I strongly suggest you not touch it. Let the OS deal with it automatically, unless for some reason it clearly screws up like assigning KCPP to E-cores or something like that. In that case I'd manually set process affinity.\n\nAs far as I know, GPU task scheduling and prioritisation isn't really implemented like CPU at all.",
                    "score": 9,
                    "upvotes": 9,
                    "downvotes": 0,
                    "created_utc": 1729110091.0,
                    "parent_id": "t3_1g55eoj",
                    "link_id": "t3_1g55eoj",
                    "permalink": "/r/KoboldAI/comments/1g55eoj/koboldcpp_highpriority_flag/ls99n6h/"
                },
                {
                    "id": "ls8zqot",
                    "author": "henk717",
                    "body": "It tells the OS that KoboldCpp should come before other thing on the PC.",
                    "score": 5,
                    "upvotes": 5,
                    "downvotes": 0,
                    "created_utc": 1729106966.0,
                    "parent_id": "t3_1g55eoj",
                    "link_id": "t3_1g55eoj",
                    "permalink": "/r/KoboldAI/comments/1g55eoj/koboldcpp_highpriority_flag/ls8zqot/"
                }
            ]
        },
        {
            "id": "1g4yd08",
            "title": "Dual 3090's not being fully utilized/loaded for layers",
            "author": "neonstingray17",
            "score": 2,
            "upvotes": 2,
            "downvotes": 0,
            "num_comments": 4,
            "created_utc": 1729082330.0,
            "url": "https://www.reddit.com/r/KoboldAI/comments/1g4yd08/dual_3090s_not_being_fully_utilizedloaded_for/",
            "permalink": "/r/KoboldAI/comments/1g4yd08/dual_3090s_not_being_fully_utilizedloaded_for/",
            "selftext": "I'm a complete noob so I apologize, but I've tried searching quite a bit and can't find a similar occurrence mentioned.  I started with a single 3090 running Koboldcpp fine.  After trying 70b models I decided to add a 2nd 3090 since my PC could support it.  I saw both GPU's in my Task Manager, but when I loaded a 70b model through the Kobold gui, it would fill the first 3090 VRAM and the rest of the model in system RAM.  This was using the automatic layer allocation.  I then tried using the Tensor Split to manually split the allocation between the two GPU's, but then what happens is it takes about 24 gigs of model and splits that between the two 3090's and still puts the rest into system RAM.  In the Kobold gui it shows both 3090's for GPU 1 and GPU 2, although it doesn't let me manually pick different layer values for each card.  Thoughts?  Thanks!\n\nSystem is a 12900K in ASRock z690 Aqua, both evga 3090's.",
            "comments": [
                {
                    "id": "lsb0szz",
                    "author": "neonstingray17",
                    "body": "I figured it out - was a stupid mistake on my part, along the lines of what you guys suggested.  I didn't realize that when the gui gives its recommended layer offloading, the recommendation is based on one card even though it sees both.  So when it recommended and showed 42/83 layers, I thought that was per card.  Then when I used the tensor split it was splitting the 42 layers between two cards.  All I had to do was manually change it from 42 to 83 layers.  Sorry for wasting anyone's time, but thanks for the replies.",
                    "score": 5,
                    "upvotes": 5,
                    "downvotes": 0,
                    "created_utc": 1729133109.0,
                    "parent_id": "t3_1g4yd08",
                    "link_id": "t3_1g4yd08",
                    "permalink": "/r/KoboldAI/comments/1g4yd08/dual_3090s_not_being_fully_utilizedloaded_for/lsb0szz/"
                },
                {
                    "id": "ls7jl0f",
                    "author": "fish312",
                    "body": "You don't need tensor split. Just select \"All\" when picking the gpu, don't pick an individual gpu.",
                    "score": 4,
                    "upvotes": 4,
                    "downvotes": 0,
                    "created_utc": 1729090449.0,
                    "parent_id": "t3_1g4yd08",
                    "link_id": "t3_1g4yd08",
                    "permalink": "/r/KoboldAI/comments/1g4yd08/dual_3090s_not_being_fully_utilizedloaded_for/ls7jl0f/"
                },
                {
                    "id": "ls7jcsm",
                    "author": "IndianaNetworkAdmin",
                    "body": "There's a thread [here](https://www.reddit.com/r/KoboldAI/comments/1cp3y4v/koboldcpp_multi_gpu_help/) with a lot of different suggestions you can try.  \n  \nFrom the github support for the project -  \n  \nSet the GPU type to \"all\" and then select the ratio with `--tensor_split`",
                    "score": 3,
                    "upvotes": 3,
                    "downvotes": 0,
                    "created_utc": 1729090375.0,
                    "parent_id": "t3_1g4yd08",
                    "link_id": "t3_1g4yd08",
                    "permalink": "/r/KoboldAI/comments/1g4yd08/dual_3090s_not_being_fully_utilizedloaded_for/ls7jcsm/"
                },
                {
                    "id": "ls9ztit",
                    "author": "BangkokPadang",
                    "body": "Are you using tensor split, but also only offloading some of the layers?\n\nThis would be the behavior Iâ€™d expect if you were splitting about half the layers across 2 GPUs.\n\nLlama 3 70B has, I believe, 80 layers, so make sure youâ€™re not only offloading like 40 of them.",
                    "score": 2,
                    "upvotes": 2,
                    "downvotes": 0,
                    "created_utc": 1729118973.0,
                    "parent_id": "t3_1g4yd08",
                    "link_id": "t3_1g4yd08",
                    "permalink": "/r/KoboldAI/comments/1g4yd08/dual_3090s_not_being_fully_utilizedloaded_for/ls9ztit/"
                }
            ]
        },
        {
            "id": "1g4qr7a",
            "title": "Can someone help me configure logit biases in KoboldCpp?",
            "author": "TheSilverSmith47",
            "score": 4,
            "upvotes": 4,
            "downvotes": 0,
            "num_comments": 1,
            "created_utc": 1729051277.0,
            "url": "https://www.reddit.com/r/KoboldAI/comments/1g4qr7a/can_someone_help_me_configure_logit_biases_in/",
            "permalink": "/r/KoboldAI/comments/1g4qr7a/can_someone_help_me_configure_logit_biases_in/",
            "selftext": "I'm running KoboldCpp 1.76, and I want to ban the \"[\" and \"|\" tokens from my LLM's outputs. I've read that this can be configured in the logit_bias section of localhost:5001/api. However, I'm a noob and can't figure out how to add tokens and biases to the logit_bias section. I have the token ids from my model's tokenizer.json file, and I know I want to set the biases to -100, but I just don't know how I'm supposed to add these to the API.\n\nCan someone explain to me how to do this?",
            "comments": [
                {
                    "id": "ls82cjy",
                    "author": "henk717",
                    "body": "If you just want to ban words or sentences rather than messing with the logit bias check out the phrase ban in Context -> Tokens.",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1729096472.0,
                    "parent_id": "t3_1g4qr7a",
                    "link_id": "t3_1g4qr7a",
                    "permalink": "/r/KoboldAI/comments/1g4qr7a/can_someone_help_me_configure_logit_biases_in/ls82cjy/"
                }
            ]
        },
        {
            "id": "1g4gr7p",
            "title": "Tokens/second significantly worse on Windows vs Linux",
            "author": "CanineAssBandit",
            "score": 3,
            "upvotes": 3,
            "downvotes": 0,
            "num_comments": 4,
            "created_utc": 1729022103.0,
            "url": "https://www.reddit.com/r/KoboldAI/comments/1g4gr7p/tokenssecond_significantly_worse_on_windows_vs/",
            "permalink": "/r/KoboldAI/comments/1g4gr7p/tokenssecond_significantly_worse_on_windows_vs/",
            "selftext": "I'm getting 6.5t/s on Ubuntu 24.04 vs 4.5t/s on Windows 10. Both have updated drivers. My cards are a P40 and 3090, running Magnum 72B V2 Q4KS (39GB).\n\nWeirdly, this speed is actually worse on both sides than running Magnum 72B V1 Q4KS half a year ago. Back then I was getting 7.5t/s on Ubuntu using the Kobold broswer portal on the same computer, 7t/s on cloudflare link api with Sillytavern, and 6.5t/s on Windows on the cloudflare link api with Sillytavern.\n\nAnyone else noticing this weird disparity, or have any ideas on how to address it? On Windows I'm running a clean install of the OS with the most recent P40 driver installed from Nvidia's website, and on Ubuntu it's running whatever Ubuntu installs by default for the P40 (it works right out of the box).\n\nNote that these cards are not used for video out, they are 100% empty aside from the LLM on both platforms.",
            "comments": [
                {
                    "id": "ls3c03j",
                    "author": "SiEgE-F1",
                    "body": "Try and give your best shot why this is happening :) While at it, you might actually come to a conclusion why so many people like Linux over Windows.",
                    "score": 3,
                    "upvotes": 3,
                    "downvotes": 0,
                    "created_utc": 1729022693.0,
                    "parent_id": "t3_1g4gr7p",
                    "link_id": "t3_1g4gr7p",
                    "permalink": "/r/KoboldAI/comments/1g4gr7p/tokenssecond_significantly_worse_on_windows_vs/ls3c03j/"
                },
                {
                    "id": "ls5vo5u",
                    "author": "Caderent",
                    "body": "For me it did not work out. On windows I had no crashes. If running out on V ram on win 11 it just slowed down and I had not a single crash. Then on Linux it crash freezes whole system every time it runs out of memory. And it even should not have run out of memory. I used the same configuration on win and Linux and honestly I had better expectations from Linux. So now running windows 11 with everything unnecessary disabled and all is working fine.",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1729059682.0,
                    "parent_id": "t3_1g4gr7p",
                    "link_id": "t3_1g4gr7p",
                    "permalink": "/r/KoboldAI/comments/1g4gr7p/tokenssecond_significantly_worse_on_windows_vs/ls5vo5u/"
                },
                {
                    "id": "ls5wofq",
                    "author": "FolkStyleFisting",
                    "body": "Since you're getting worse speed than you were getting a year ago while using the latest driver, I would try rolling back to the driver version you were using last year. It's fairly common for newer GPU drivers to have performance regressions which affect some use cases more than others.",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1729060334.0,
                    "parent_id": "t3_1g4gr7p",
                    "link_id": "t3_1g4gr7p",
                    "permalink": "/r/KoboldAI/comments/1g4gr7p/tokenssecond_significantly_worse_on_windows_vs/ls5wofq/"
                },
                {
                    "id": "ls3cugx",
                    "author": "SiEgE-F1",
                    "body": "As a brief overview:  \n- less bloat, telemetry, unnecessary applications, more attention to optimization and less to pointless/useless hardware. No 333 layers of protection against brain dead users.",
                    "score": 3,
                    "upvotes": 3,
                    "downvotes": 0,
                    "created_utc": 1729022959.0,
                    "parent_id": "t1_ls3c03j",
                    "link_id": "t3_1g4gr7p",
                    "permalink": "/r/KoboldAI/comments/1g4gr7p/tokenssecond_significantly_worse_on_windows_vs/ls3cugx/"
                }
            ]
        },
        {
            "id": "1g4gn4u",
            "title": "K80/K40 works on Windows on koboldcpp.exe",
            "author": "CanineAssBandit",
            "score": 5,
            "upvotes": 5,
            "downvotes": 0,
            "num_comments": 7,
            "created_utc": 1729021808.0,
            "url": "https://www.reddit.com/r/KoboldAI/comments/1g4gn4u/k80k40_works_on_windows_on_koboldcppexe/",
            "permalink": "/r/KoboldAI/comments/1g4gn4u/k80k40_works_on_windows_on_koboldcppexe/",
            "selftext": "This post is for anyone searching this in the future, as there are no posts about it so far. I could not get it working under Linux. This is a shame as my tokens/second on Linux is 6.5 on my P40 on Ubuntu vs 4.5 on Windows.\n\nK80 is getting 2.2t/s on an 18GB 70B Q2.something model. On CPU memory, that model gets .5t/s. It is as I expected: able to be a space heater and is better than DDR4, but not sure how performance will scale across multiple of them. Will update later once I have four of them.",
            "comments": [
                {
                    "id": "ls3wsfb",
                    "author": "henk717",
                    "body": "This is what I expected but good to have confirmation we didn't break it.  \nPerformance will not improve across multiple of them singe its serial processing, it merely allows fitting larger models.",
                    "score": 2,
                    "upvotes": 2,
                    "downvotes": 0,
                    "created_utc": 1729029468.0,
                    "parent_id": "t3_1g4gn4u",
                    "link_id": "t3_1g4gn4u",
                    "permalink": "/r/KoboldAI/comments/1g4gn4u/k80k40_works_on_windows_on_koboldcppexe/ls3wsfb/"
                },
                {
                    "id": "lscuand",
                    "author": "rconster1974",
                    "body": "What are you using for a motherboard? I've started to look at doing something similar but none of the motherboards I have from previous homelabs can support it and googling didn't find a good recommendation for the K80 either.",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1729169744.0,
                    "parent_id": "t3_1g4gn4u",
                    "link_id": "t3_1g4gn4u",
                    "permalink": "/r/KoboldAI/comments/1g4gn4u/k80k40_works_on_windows_on_koboldcppexe/lscuand/"
                },
                {
                    "id": "ls49n0e",
                    "author": "CanineAssBandit",
                    "body": "I didn't mean I expect an improvement, I meant I'm concerned what performance will be with a much larger model. IE I have a 70B q2 on the one card running at 2.2t/s, does that mean that having a 70b q8 on four cards will run much slower than 2.2t/s, assuming the same vram load on each card as the 70b q2? What about a 123b running on six of them? I don't expect you to know these answers btw, I'm just voicing my thoughts. I will get some more K80s and find out, it's fun to play with these. I kind of want to do a mineral oil rig with a whole crapton of these and shove the 405B on it...they cost less than DDR5 but have much higher throughput? At least on paper.\n\nAnd yes, thank you for the suggestion to try windows if that was you. Idk how nobody suggested that before, or how it didn't occur to me to at least check.",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1729034183.0,
                    "parent_id": "t1_ls3wsfb",
                    "link_id": "t3_1g4gn4u",
                    "permalink": "/r/KoboldAI/comments/1g4gn4u/k80k40_works_on_windows_on_koboldcppexe/ls49n0e/"
                },
                {
                    "id": "lsf17lo",
                    "author": "CanineAssBandit",
                    "body": "I have above 4G decoding available in my bios, I do NOT have resizable bar that I know of. There's no option for it. The P40 does not work without 4G decoding enabled, not sure about the K80, I've not tried running it with that turned off.\n\nI'm on a normal asus z270 board but I will likely try the K80 with a xeon sandy bridge system too. That 2012 board definitely doesn't mention 4G decoding in the bios but we'll see.",
                    "score": 2,
                    "upvotes": 2,
                    "downvotes": 0,
                    "created_utc": 1729195497.0,
                    "parent_id": "t1_lscuand",
                    "link_id": "t3_1g4gn4u",
                    "permalink": "/r/KoboldAI/comments/1g4gn4u/k80k40_works_on_windows_on_koboldcppexe/lsf17lo/"
                },
                {
                    "id": "ls4hdng",
                    "author": "Thellton",
                    "body": "tokens per second is roughly never going to be able to exceed the following formula:\n\nslowest device's memory bandwidth / active parameters in GB\n\nso for the K80 with a Q2K that'd be:\n\n240 / 26.4 = 9 tokens per second\n\nHowever, that does not include the KV cache which also has to be shifted through the memory bandwidth as well, nor does it account for various inefficiencies that accrue that bottleneck compute such as the dequantisation process. furthermore, the k80 is strictly speaking two 12GB GPUs stapled together, so as I understand it unless koboldcpp does something silently in the background for k80's and other similar GPUs that are actually multiple GPUs masquerading as a single GPU; you might need to be setting a GPU split?\n\nEdit: granted, it does make the K80 a rather good GPU for batched inference?",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1729037109.0,
                    "parent_id": "t1_ls49n0e",
                    "link_id": "t3_1g4gn4u",
                    "permalink": "/r/KoboldAI/comments/1g4gn4u/k80k40_works_on_windows_on_koboldcppexe/ls4hdng/"
                },
                {
                    "id": "lsfiu29",
                    "author": "rconster1974",
                    "body": "Thanks. That gives me some good info tobget started.",
                    "score": 2,
                    "upvotes": 2,
                    "downvotes": 0,
                    "created_utc": 1729201237.0,
                    "parent_id": "t1_lsf17lo",
                    "link_id": "t3_1g4gn4u",
                    "permalink": "/r/KoboldAI/comments/1g4gn4u/k80k40_works_on_windows_on_koboldcppexe/lsfiu29/"
                },
                {
                    "id": "lsfk19d",
                    "author": "CanineAssBandit",
                    "body": "No problem, hope it works out for you! Card is slow af but it's a lot better than ddr4 if you don't mind the 250watts it eats. For the models it can fit though, and what electric costs in most places, I'd just use Infermatic for $15/month or Featherless or even possibly Openrouter. 70Bs are available for cheap on apis everywhere, as long as you don't require XTC sampler.\n\nBuuut if you don't RP much or don't care about power draw, and have no money, it's a great option I think. I kind of like that nobody likes them, it keeps them cheap lol",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1729201649.0,
                    "parent_id": "t1_lsfiu29",
                    "link_id": "t3_1g4gn4u",
                    "permalink": "/r/KoboldAI/comments/1g4gn4u/k80k40_works_on_windows_on_koboldcppexe/lsfk19d/"
                }
            ]
        },
        {
            "id": "1g4eoqf",
            "title": "Best RP Model for 16gb VRAM & RAM",
            "author": "Aardvark-Fearless",
            "score": 5,
            "upvotes": 5,
            "downvotes": 0,
            "num_comments": 12,
            "created_utc": 1729016855.0,
            "url": "https://www.reddit.com/r/KoboldAI/comments/1g4eoqf/best_rp_model_for_16gb_vram_ram/",
            "permalink": "/r/KoboldAI/comments/1g4eoqf/best_rp_model_for_16gb_vram_ram/",
            "selftext": "Im new to LLM and AI in general, I run Koboldcpp w/ silly tavern, and I'm wondering what RP model would be good for my system and one that doesn't offload much on RAM and uses mostly VRAM, Thanks\n\nBenchmark/Specs: [https://www.userbenchmark.com/UserRun/68794086](https://www.userbenchmark.com/UserRun/68794086)\n\nEdit: Also are Llama-Uncensored or Tiger-Gemma worth using?",
            "comments": [
                {
                    "id": "ls5aqhh",
                    "author": "Sicarius_The_First",
                    "body": "[https://huggingface.co/SicariusSicariiStuff/LLAMA-3\\_8B\\_Unaligned\\_BETA](https://huggingface.co/SicariusSicariiStuff/LLAMA-3_8B_Unaligned_BETA)",
                    "score": 2,
                    "upvotes": 2,
                    "downvotes": 0,
                    "created_utc": 1729048507.0,
                    "parent_id": "t3_1g4eoqf",
                    "link_id": "t3_1g4eoqf",
                    "permalink": "/r/KoboldAI/comments/1g4eoqf/best_rp_model_for_16gb_vram_ram/ls5aqhh/"
                },
                {
                    "id": "lsjhkpm",
                    "author": "Majestical-psyche",
                    "body": "Iâ€™m on a 4090 and still using Llama 8B (steroblackroot something) ðŸ˜… But Iâ€™m constantly downloading and trying new models but I always come back to that one.\nNemoâ€™s writing style is much better, BUT SBRâ€™s instruction following is far superior. Itâ€™s more creative, different generations every time, coherent, but makes the same mistakes as Nemo occasionallyâ€¦ Plus is good up to 24k contexts.\n\nSmall sucks, I tried it sooo many times and tried to like it.\n \nNemo is really good when it wants to be, but when itâ€™s not, itâ€™s very hard to navigate to your liking.\n \nGemma 27B is an absolute killer, but it really falls off at 3k tokens I find.\n \nQwen 9B and 32B, I feel like it has a ton of potential, but needs to be fine tuned probably, itâ€™s almost good, but not quite there.\n\nhttps://huggingface.co/bluuwhale/L3-SthenoMaidBlackroot-8B-V1",
                    "score": 2,
                    "upvotes": 2,
                    "downvotes": 0,
                    "created_utc": 1729265575.0,
                    "parent_id": "t3_1g4eoqf",
                    "link_id": "t3_1g4eoqf",
                    "permalink": "/r/KoboldAI/comments/1g4eoqf/best_rp_model_for_16gb_vram_ram/lsjhkpm/"
                },
                {
                    "id": "ls2zexv",
                    "author": "kiselsa",
                    "body": "Maybe Cydonia 22b. Or some Mistral Nemo finetunes.",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1729018739.0,
                    "parent_id": "t3_1g4eoqf",
                    "link_id": "t3_1g4eoqf",
                    "permalink": "/r/KoboldAI/comments/1g4eoqf/best_rp_model_for_16gb_vram_ram/ls2zexv/"
                },
                {
                    "id": "lsnxba5",
                    "author": "mitsu89",
                    "body": "Maybe tiger Gemma 9b\nhttps://huggingface.co/TheDrummer/Tiger-Gemma-9B-v1-GGUF\n\n\nOr Celeste Mistral Nemo 12b\nhttps://huggingface.co/bartowski/MN-12B-Celeste-V1.9-GGUF\n\n\nMN12b are a bit slow for my phone (termux) but usable the 9b works faster. The 8b are not as good i think.\n\n\nFor 16gb VRAM maybe Mistral small 22b are better. 9b models are for phones with 12gb ram the Q4_0_4_4 gguf (My PC are more weak than my Poco X6 pro phone lol)Â ",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1729332652.0,
                    "parent_id": "t3_1g4eoqf",
                    "link_id": "t3_1g4eoqf",
                    "permalink": "/r/KoboldAI/comments/1g4eoqf/best_rp_model_for_16gb_vram_ram/lsnxba5/"
                },
                {
                    "id": "lt0zx51",
                    "author": "AveryVeilfaire",
                    "body": "Lyra",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1729527977.0,
                    "parent_id": "t3_1g4eoqf",
                    "link_id": "t3_1g4eoqf",
                    "permalink": "/r/KoboldAI/comments/1g4eoqf/best_rp_model_for_16gb_vram_ram/lt0zx51/"
                },
                {
                    "id": "ls2wozc",
                    "author": "Crashes556",
                    "body": "Using 3060 ti because of the bus bandwidth youâ€™ll see models in the 12b/13b range will be a bit chunky and delayed. I would suggest a 7b model. But quants and other factors can affect the suggestion. Truly itâ€™s hard to know what runs best until you start trying different model sizes with different configurations. I can run a 22B on my 4080 just fine but it does not leave a lot of room for context, so I use a 13b with a large 32k context if I really want to keep a story going.",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1729017892.0,
                    "parent_id": "t3_1g4eoqf",
                    "link_id": "t3_1g4eoqf",
                    "permalink": "/r/KoboldAI/comments/1g4eoqf/best_rp_model_for_16gb_vram_ram/ls2wozc/"
                },
                {
                    "id": "ls8ejae",
                    "author": "Aardvark-Fearless",
                    "body": "ooo, will try, but ive had bad luck with llama working properly",
                    "score": 2,
                    "upvotes": 2,
                    "downvotes": 0,
                    "created_utc": 1729100298.0,
                    "parent_id": "t1_ls5aqhh",
                    "link_id": "t3_1g4eoqf",
                    "permalink": "/r/KoboldAI/comments/1g4eoqf/best_rp_model_for_16gb_vram_ram/ls8ejae/"
                },
                {
                    "id": "ls8eenr",
                    "author": "Aardvark-Fearless",
                    "body": "Cydonia was too powerful/big for my puter to handle, NemoMix is really good but takes awhile to generate but I guess its a sacrifice I must make",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1729100258.0,
                    "parent_id": "t1_ls2zexv",
                    "link_id": "t3_1g4eoqf",
                    "permalink": "/r/KoboldAI/comments/1g4eoqf/best_rp_model_for_16gb_vram_ram/ls8eenr/"
                },
                {
                    "id": "ls3uwv8",
                    "author": "_Erilaz",
                    "body": "Are you using token streaming and context shift? \n\nBecause I do, and I can use 22B, 27B and even 8x7B models with 32k context on a mere 3080 10GB & normie DDR4-3800 dual channel CPU. Q5KS at that! The delay is nonexistent, and token generation roughly matches my comfortable reading speed. It still answers way faster than real people would, and you can start reading before it completes the output.\n\nThe initial prompt processing of a large chat can be sluggish, sure, but once it ingests all of it, the subsequent generations will have no significant delays as long as the context flows as intended by the algorithm, and the regeneration will be instant.\n\nAnd there's no real point in the excess speed of a 7B model when it comes to RP. You aren't serving multiple users, you aren't instructing the model to deal with complicated RAG and CoT stuff, you're just asking it to impersonate a character.\n\nAlso, why 13B and 7B? Why would you use a LLama-2 derivative? The only two useful 13Bs are Tiefighter and Psyfighter, and there are no noteworthy models as far as 7B is concerned. There are LLama-3 8B derivatives, as well as that smaller Gemma-2. And the 12B mark was taken by Mistral Nemo 12B as a base.",
                    "score": 2,
                    "upvotes": 2,
                    "downvotes": 0,
                    "created_utc": 1729028819.0,
                    "parent_id": "t1_ls2wozc",
                    "link_id": "t3_1g4eoqf",
                    "permalink": "/r/KoboldAI/comments/1g4eoqf/best_rp_model_for_16gb_vram_ram/ls3uwv8/"
                },
                {
                    "id": "ls40wp5",
                    "author": "BangkokPadang",
                    "body": "Ii donâ€™t even mean this mean, but most people arenâ€™t optimizing anything. Most people seem to feel lucky to get words back from a prompt and just stop messing with things to kee from breaking anything ðŸ¤£",
                    "score": 2,
                    "upvotes": 2,
                    "downvotes": 0,
                    "created_utc": 1729030947.0,
                    "parent_id": "t1_ls3uwv8",
                    "link_id": "t3_1g4eoqf",
                    "permalink": "/r/KoboldAI/comments/1g4eoqf/best_rp_model_for_16gb_vram_ram/ls40wp5/"
                },
                {
                    "id": "ls551op",
                    "author": "Crashes556",
                    "body": "I donâ€™t use context shift I think? I know I use the automatic rope adjusting if thatâ€™s the same thing? Are you using ooba or Kobold? It seems Kobold automatically adjust far less VRAMS when I use larger context. I have a 4080 with 64GB of DDR5 even. I appreciate the insight at least even though I got some serious downvotes.",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1729046118.0,
                    "parent_id": "t1_ls3uwv8",
                    "link_id": "t3_1g4eoqf",
                    "permalink": "/r/KoboldAI/comments/1g4eoqf/best_rp_model_for_16gb_vram_ram/ls551op/"
                },
                {
                    "id": "ls69ffd",
                    "author": "_Erilaz",
                    "body": "No, ContextShift and auto rope scaling are entirely different features with different purposes behind them. I strongly recommend you to use CS, I'd go as far as saying it's the key feature of KoboldCPP. The only case I wouldn't use it would be volunteering for the Horde and using QuantKV at the same time, and that's a niche case.\n\nI use KoboldCPP, and split the load between my CPU and GPU. Say, I have a Nemo 12B model, Q5KS. 25 layers go into my 10GB GPU, and the rest is processed on the CPU and RAM. You'll have a different layer split, since you can easily offload one and a half times more layers on your GPU than I do, or use a bigger model. Other than that, my settings are fairly standard. I DO use FlashAttention, as it both speeds up prompt processing and saves memory. I also set Disable MMAP to save some RAM, there's no real detriment to it to my knowledge after the model loads.\n\n\nFor the OP, 12B seems like the way to go. That's Mistral Nemo and derivatives. Mistral-Small 22B isn't that much better and will be too slow. Gemma-2 27B, as much as I like it, is a niche model, and might be even smaller. And anything beyond that is a tough ask since the OP has 8GB VRAM & 16GB RAM, even smaller than I do. But still, 12B Q5KS, maybe Q4KM should run just fine.",
                    "score": 2,
                    "upvotes": 2,
                    "downvotes": 0,
                    "created_utc": 1729069144.0,
                    "parent_id": "t1_ls551op",
                    "link_id": "t3_1g4eoqf",
                    "permalink": "/r/KoboldAI/comments/1g4eoqf/best_rp_model_for_16gb_vram_ram/ls69ffd/"
                }
            ]
        },
        {
            "id": "1g4b1xz",
            "title": "Using Lightning models with KoboldCCP",
            "author": "zircher",
            "score": 1,
            "upvotes": 1,
            "downvotes": 0,
            "num_comments": 0,
            "created_utc": 1729007714.0,
            "url": "https://www.reddit.com/r/KoboldAI/comments/1g4b1xz/using_lightning_models_with_koboldccp/",
            "permalink": "/r/KoboldAI/comments/1g4b1xz/using_lightning_models_with_koboldccp/",
            "selftext": "Any suggestions one how to set up Kobold to use something like JuggernautXL Lightning properly?  I can get it to run with Local A1111, but using a reduced number of steps results in an inferior image and I know Lightning models can do better.  I also use Fooocus, but I wanted to see if I could do everything inside Kobold's UI.  Thoughts?",
            "comments": []
        },
        {
            "id": "1g487sj",
            "title": "Chats - is narrative normal?",
            "author": "SmileExDee",
            "score": 0,
            "upvotes": 0,
            "downvotes": 0,
            "num_comments": 3,
            "created_utc": 1729000310.0,
            "url": "https://www.reddit.com/r/KoboldAI/comments/1g487sj/chats_is_narrative_normal/",
            "permalink": "/r/KoboldAI/comments/1g487sj/chats_is_narrative_normal/",
            "selftext": "Hi, so I tried different GGUF models and after lengthy chat I usually get some narrative like \"that how you talk about stuff\" at the end of AI sentence. WTF is that and how do I turn that off? ",
            "comments": [
                {
                    "id": "ls2lsqo",
                    "author": "henk717",
                    "body": "Which models did you try and what GPU do you have?",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1729014484.0,
                    "parent_id": "t3_1g487sj",
                    "link_id": "t3_1g487sj",
                    "permalink": "/r/KoboldAI/comments/1g487sj/chats_is_narrative_normal/ls2lsqo/"
                },
                {
                    "id": "lu7sgp7",
                    "author": "Cool-Hornet4434",
                    "body": "I know this is an old thread, but some models are trained on chat data from Discord that got leaked as well as youtube transcripts and twitch chat.\n\n\nI find it's really common with the smaller models.",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1730139613.0,
                    "parent_id": "t3_1g487sj",
                    "link_id": "t3_1g487sj",
                    "permalink": "/r/KoboldAI/comments/1g487sj/chats_is_narrative_normal/lu7sgp7/"
                },
                {
                    "id": "ls33y0j",
                    "author": "SmileExDee",
                    "body": "L3-8B-Stheno-v3.2-Q8\\_0-imat.gguf  \nNemoMix-Unleashed-12B-Q8\\_0.gguf\n\nI use RX6800 with hipBLAS",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1729020172.0,
                    "parent_id": "t1_ls2lsqo",
                    "link_id": "t3_1g487sj",
                    "permalink": "/r/KoboldAI/comments/1g487sj/chats_is_narrative_normal/ls33y0j/"
                }
            ]
        },
        {
            "id": "1g3wgt9",
            "title": "Looking of models ",
            "author": "Pure-Fig-8064",
            "score": 2,
            "upvotes": 2,
            "downvotes": 0,
            "num_comments": 1,
            "created_utc": 1728956122.0,
            "url": "https://www.reddit.com/r/KoboldAI/comments/1g3wgt9/looking_of_models/",
            "permalink": "/r/KoboldAI/comments/1g3wgt9/looking_of_models/",
            "selftext": "What is the best current chat model to use on janitorai ",
            "comments": [
                {
                    "id": "lrz0zmq",
                    "author": "AutoModerator",
                    "body": "Welcome to the KoboldAI Subreddit, since we get a lot of the same questions here is a brief FAQ for Venus and JanitorAI.\n\n## What is the KoboldAI (API) and how does it work?\nKoboldAI is originally a program for AI story writing, text adventures and chatting but we decided to create an API for our software so other software developers had an easy solution for their UI's and websites. VenusAI was one of these websites and anything based on it such as JanitorAI can use our software as well. But it is important to know that KoboldAI is intended to be a program that you can run yourself, not a service. This means that you are responsible to provide the right computer resources for the AI by either running it locally or using it trough a cloud provider.\n\n## Where do I get my API link from?\nYou get an API link from a working version of KoboldAI, if you have KoboldAI started the same link you use in the browser should be the one to access the API. However, be advised that VenusAI based websites ARE NOT PRIVATE and can only connect to external links. So connecting to https://localhost:5000 or https://127.0.0.1:5000 will not work unlike other solutions that let you connect to your KoboldAI instance privately.\n\nIf you installed KoboldAI on your own computer we have a mode called Remote Mode, you can find this as an icon in your startmenu if you opted for Start Menu icons in our offline installer. Or you can start this mode using remote-play.bat if you didn't. Linux users can add --remote instead when launching KoboldAI trough the terminal.\n\n## What do you mean VenusAI based stuff is not private?\nWe consider a solution private if your data does not leave your computer, for example when you use TavernAI this is a program that connects directly to KoboldAI and can for example access those localhost links. Of course our built in UI's are also completely private. VenusAI programmed it differently, their server is the one connecting to the AI which means they could log and intercept all of it. On top of that they force you to sign in, which means they have identifiable information that can be tied to the story. As a result most of the members and contributors of the KoboldAI community choose not to use these sites and opt for more privacy friendly solutions such as the KoboldAI UI itself or third party software such as Sillytavern.\n\n## I got a trycloudflare link but it doesn't work for some reason\nThis could be many things, but commonly people try the link before the AI finished loading or they have no AI selected.\n\n## I found a free way to do it without using my own computer but I keep getting CUDA out of memory errors!!!\nYes, there are guides out there of people using it on free cloud resources (We can't formally endorse this in this reply since we know it breaches the TOS of those services). The problem is that these guides often point to a free GPU that does not have enough VRAM for the default settings of VenusAI or JanitorAI. To fix this go to the Generation Settings inside Venus/Janitor and then lower the context size to 1024.\n\n## Ok so I have a top of the line gaming PC how do I set this up?\nBefore you set it up there is a lot of confusion about the kind of hardware people need because AI is a lot heavier to run than video games. At the bare minimum you will need an Nvidia GPU with 8GB of VRAM. With just this amount of VRAM you can run 2.7B models out of the box (In the future we have official 4-bit support to help you run higher models). For higher sizes you will need to have the required amount of VRAM as listed on the menu (Typically 16GB and up). If you want you can also stop by our [Discord Community](https://koboldai.org/discord) for some guidance on how to run higher models after you managed to get one of the models working using unofficial 4-bit versions or read up about that version [here](https://docs.pygmalion.chat/local-installation-(gpu)/koboldai4bit/)\n\n## This AI is so shit, its horrible compared to ChatGPT, why would anyone use this?!\nThis is a sentiment we unfortunately saw a lot in the JanitorAI discord because people misunderstand what KoboldAI is and who it is for. ChatGPT and the likes are large corporations throwing a lot of money at a paid service you could not possibly run at home. They are the best of the best AI models currently available. KoboldAI is not an AI on its own, its a project where you can bring an AI model yourself. And the AI's people can typically run at home are very small by comparison because it is expensive to both use and train larger models.\n\nSo most of these \"KoboldAI is dumb\" complaints come from both the wrong expectations of users comparing small models to massive private models such as ChatGPT, and them simply selecting the wrong model for what they want to do. A 6B no matter how good will simply not perform like a 175B model. But luckily for our community in the recent months things have gotten a lot closer when it comes to having a great chatbot. If you have ways to run the 13B or 30B model sizes of the recent instruction or chat models you should be able to get a great experience. But the quality of your experience does depend heavily on which model you pick.\n\nAnother important part is that you pick a model that is good at doing what you need to do, we know a lot of people pick Erebus for example for its NSFW capabilities. But understand that Erebus was designed to create compelling NSFW story writing and has not been trained for chatting. So while great at writing erotic novels, it is not the most compelling chatter. So whenever someone says that \"The bot of KoboldAI is dumb or shit\" understand they are not talking about KoboldAI, they are talking about whatever model they tried with it. For those wanting to enjoy Erebus we recommend using our own UI instead of VenusAI/JanitorAI and using it to write an erotic story rather than as a chatting partner.\n\n## Awesome, all caught up and I have an Nvidia with 8GB of vram or more. How do I install this thing?\nAssuming most of you are Windows users, for chat bot usage we currently recommend [this](https://github.com/henk717/KoboldAI/releases/download/Snapshot-7-5-2023/KoboldAI-Setup-230507r2.exe) offline installer. If you are on Linux you can git clone https://github.com/henk717/koboldai and use play.sh\n\n## Don't you have Koboldcpp that can run really good models without needing a good GPU, why didn't you talk about that?\nYes! [Koboldcpp](https://koboldai.org/cpp) is an amazing solution that lets people run [GGML models](https://huggingface.co/models?search=ggml) and it allows you to run those great models we have been enjoying for our own chatbots without having to rely on expensive hardware as long as you have a bit of patience waiting for the reply's. Why didn't we mention it? Because you are asking about VenusAI and/or JanitorAI which are not very compatible with it. The default link it generates will not work with these services, and unfortunately it takes to long to generate so it causes timeouts. If you are really determined to have this work you can always stop by our [Discord Community](https://koboldai.org/discord) and ask.\n\nOr of course you can stop using VenusAI and JanitorAI and enjoy a chatbot inside the UI that is bundled with Koboldcpp, that way you have a fully private way of running the good AI models on your own PC.\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/KoboldAI) if you have any questions or concerns.*",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1728956123.0,
                    "parent_id": "t3_1g3wgt9",
                    "link_id": "t3_1g3wgt9",
                    "permalink": "/r/KoboldAI/comments/1g3wgt9/looking_of_models/lrz0zmq/"
                }
            ]
        },
        {
            "id": "1g35z9a",
            "title": "koboldcpp - Compiling from source vs. prebuilt binaries",
            "author": "morbidSuplex",
            "score": 2,
            "upvotes": 2,
            "downvotes": 0,
            "num_comments": 2,
            "created_utc": 1728872628.0,
            "url": "https://www.reddit.com/r/KoboldAI/comments/1g35z9a/koboldcpp_compiling_from_source_vs_prebuilt/",
            "permalink": "/r/KoboldAI/comments/1g35z9a/koboldcpp_compiling_from_source_vs_prebuilt/",
            "selftext": "Hi all,\n\nfor those people who tried both approaches while installing koboldcpp, is there a difference between using a prebuilt binary vs. compiling from source performance wise? I've read somewhere that llama.cpp uses a native flag to optimize it to to actual platform while compiling from source. Is this noticeable?\n\nThanks!",
            "comments": [
                {
                    "id": "lruk3v9",
                    "author": "henk717",
                    "body": "Thats a big maybe, on windows it won't matter our compile flags are setup to produce a sharable result and windows compiling is quite hard due to the different compilers needed.\n\n\nLinux is different, there koboldcpp.sh produces sharable results and regular make produces a result unique to your system. This does allow for cpu flags that are going to be beyond what the defaults provide such as avx512 but whatever gets added needs to be faster than avx2 for it to make a positive difference.\n\n\nThe other part is CPU vs GPU usage, with a fully offloaded cuda you may notice a 1t/s difference but its minimal. Without a GPU it will matter the most.\n\n\nSo on Linux it can make a difference and on Linux its also much easier to do. But how much of a difference it will make depends a lot on the system.",
                    "score": 8,
                    "upvotes": 8,
                    "downvotes": 0,
                    "created_utc": 1728895524.0,
                    "parent_id": "t3_1g35z9a",
                    "link_id": "t3_1g35z9a",
                    "permalink": "/r/KoboldAI/comments/1g35z9a/koboldcpp_compiling_from_source_vs_prebuilt/lruk3v9/"
                },
                {
                    "id": "ls8fn8n",
                    "author": "morbidSuplex",
                    "body": "Thanks. Just tested. The improvement is less than 1t/s. very very minimal indeed.",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1729100645.0,
                    "parent_id": "t1_lruk3v9",
                    "link_id": "t3_1g35z9a",
                    "permalink": "/r/KoboldAI/comments/1g35z9a/koboldcpp_compiling_from_source_vs_prebuilt/ls8fn8n/"
                }
            ]
        },
        {
            "id": "1g3108w",
            "title": "AI Horde Problem",
            "author": "Ashamed-Cat-9299",
            "score": 0,
            "upvotes": 0,
            "downvotes": 0,
            "num_comments": 9,
            "created_utc": 1728856845.0,
            "url": "https://www.reddit.com/r/KoboldAI/comments/1g3108w/ai_horde_problem/",
            "permalink": "/r/KoboldAI/comments/1g3108w/ai_horde_problem/",
            "selftext": "https://preview.redd.it/0cgly2mtilud1.png?width=862&format=png&auto=webp&s=8a9a0199a7519e391d84003288dcb1fb1f45944a\n\nIf I try to use AI horde locally, it does this. I can still use it by using the smaller text box, and it prints in the top section, but is there a way I can fix it, am I doing something wrong",
            "comments": [
                {
                    "id": "lrsfpvy",
                    "author": "henk717",
                    "body": "That version of KoboldAI is to old so its probably pointing to an AI horde domain that is no longer in use, and somehow ended up loading the KoboldAI Lite website in to it.\n\nI recommend using [koboldai.net](http://koboldai.net) instead which is the officially supported way to use AI horde.",
                    "score": 5,
                    "upvotes": 5,
                    "downvotes": 0,
                    "created_utc": 1728857930.0,
                    "parent_id": "t3_1g3108w",
                    "link_id": "t3_1g3108w",
                    "permalink": "/r/KoboldAI/comments/1g3108w/ai_horde_problem/lrsfpvy/"
                },
                {
                    "id": "lrsijn4",
                    "author": "Ashamed-Cat-9299",
                    "body": "There's really no other way?",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1728858953.0,
                    "parent_id": "t1_lrsfpvy",
                    "link_id": "t3_1g3108w",
                    "permalink": "/r/KoboldAI/comments/1g3108w/ai_horde_problem/lrsijn4/"
                },
                {
                    "id": "lrslenl",
                    "author": "schuylkilladelphia",
                    "body": "Or just download the [latest version](https://github.com/LostRuins/koboldcpp/releases)?\n\nEdit: or is this not cpp?",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1728860013.0,
                    "parent_id": "t1_lrsfpvy",
                    "link_id": "t3_1g3108w",
                    "permalink": "/r/KoboldAI/comments/1g3108w/ai_horde_problem/lrslenl/"
                },
                {
                    "id": "lrsu95i",
                    "author": "henk717",
                    "body": "What is the use case for another way?",
                    "score": 3,
                    "upvotes": 3,
                    "downvotes": 0,
                    "created_utc": 1728863376.0,
                    "parent_id": "t1_lrsijn4",
                    "link_id": "t3_1g3108w",
                    "permalink": "/r/KoboldAI/comments/1g3108w/ai_horde_problem/lrsu95i/"
                },
                {
                    "id": "lrsilg6",
                    "author": "Ashamed-Cat-9299",
                    "body": "I mean I can probably write a tampermonkey script that rearranges the boxes",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1728858971.0,
                    "parent_id": "t1_lrsijn4",
                    "link_id": "t3_1g3108w",
                    "permalink": "/r/KoboldAI/comments/1g3108w/ai_horde_problem/lrsilg6/"
                },
                {
                    "id": "lrsq79z",
                    "author": "henk717",
                    "body": "He wants to use an online service which we officially support trough [koboldai.net](http://koboldai.net) , using [koboldai.net](http://koboldai.net) automatically connects you to that service and ensures we can deliver updates to keep that functional. KoboldCpp is a way to use that UI with a backend you host yourself.",
                    "score": 2,
                    "upvotes": 2,
                    "downvotes": 0,
                    "created_utc": 1728861794.0,
                    "parent_id": "t1_lrslenl",
                    "link_id": "t3_1g3108w",
                    "permalink": "/r/KoboldAI/comments/1g3108w/ai_horde_problem/lrsq79z/"
                },
                {
                    "id": "lrxbzpy",
                    "author": "kiselsa",
                    "body": "What's the point?",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1728934786.0,
                    "parent_id": "t1_lrsilg6",
                    "link_id": "t3_1g3108w",
                    "permalink": "/r/KoboldAI/comments/1g3108w/ai_horde_problem/lrxbzpy/"
                },
                {
                    "id": "lrsr558",
                    "author": "schuylkilladelphia",
                    "body": "Ahh gotcha, yeah I only use local so I got confused",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1728862162.0,
                    "parent_id": "t1_lrsq79z",
                    "link_id": "t3_1g3108w",
                    "permalink": "/r/KoboldAI/comments/1g3108w/ai_horde_problem/lrsr558/"
                },
                {
                    "id": "lrxll9y",
                    "author": "BillTran163",
                    "body": "He likes to tinker. ðŸ”¨ðŸ”¨",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1728937768.0,
                    "parent_id": "t1_lrxbzpy",
                    "link_id": "t3_1g3108w",
                    "permalink": "/r/KoboldAI/comments/1g3108w/ai_horde_problem/lrxll9y/"
                }
            ]
        },
        {
            "id": "1g2g9mq",
            "title": "Help! Both Google Colab and KoboldCpp are not working",
            "author": "Severe_Leg8606",
            "score": 0,
            "upvotes": 0,
            "downvotes": 0,
            "num_comments": 8,
            "created_utc": 1728787988.0,
            "url": "https://www.reddit.com/r/KoboldAI/comments/1g2g9mq/help_both_google_colab_and_koboldcpp_are_not/",
            "permalink": "/r/KoboldAI/comments/1g2g9mq/help_both_google_colab_and_koboldcpp_are_not/",
            "selftext": "They were working normally until about ten hours ago. My Google Colab generated an API, but in Jan it shows \"network error\", and in Venus it shows \"Error generating, error: TypeError: Failed to fetch\". KoboldCpp is also not working. The errors shown are all the same.\n\n(English is not my native language. The above is edited by me using a translator. I hope I have expressed myself clearly.)",
            "comments": [
                {
                    "id": "lrp6gtr",
                    "author": "henk717",
                    "body": "I tried it out and it works perfect on my side. its possible cloudflare had some issues when you tried it but our official [https://koboldai.org/colabcpp](https://koboldai.org/colabcpp) is not broken.",
                    "score": 3,
                    "upvotes": 3,
                    "downvotes": 0,
                    "created_utc": 1728814198.0,
                    "parent_id": "t3_1g2g9mq",
                    "link_id": "t3_1g2g9mq",
                    "permalink": "/r/KoboldAI/comments/1g2g9mq/help_both_google_colab_and_koboldcpp_are_not/lrp6gtr/"
                },
                {
                    "id": "lrodj9s",
                    "author": "International-Try467",
                    "body": "Did you disconnect Google Colab's GPU? It only runs while Google Collab is active",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1728795225.0,
                    "parent_id": "t3_1g2g9mq",
                    "link_id": "t3_1g2g9mq",
                    "permalink": "/r/KoboldAI/comments/1g2g9mq/help_both_google_colab_and_koboldcpp_are_not/lrodj9s/"
                },
                {
                    "id": "lrpcn4k",
                    "author": "Severe_Leg8606",
                    "body": "Yes, it seems that there is some problem with JanitorAI.",
                    "score": 2,
                    "upvotes": 2,
                    "downvotes": 0,
                    "created_utc": 1728818275.0,
                    "parent_id": "t1_lrp6gtr",
                    "link_id": "t3_1g2g9mq",
                    "permalink": "/r/KoboldAI/comments/1g2g9mq/help_both_google_colab_and_koboldcpp_are_not/lrpcn4k/"
                },
                {
                    "id": "lrokzch",
                    "author": "Severe_Leg8606",
                    "body": "Yes, my Google Colab has always been in a connected state. It could run normally a few days ago, but now it suddenly doesn't work. I have tried using an iPad, mobile phone, and computer, but none of them work. And it cannot be used in both Janitor and Venus. The APIs of both Google Colab and KoboldCpp show the same network error.",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1728799543.0,
                    "parent_id": "t1_lrodj9s",
                    "link_id": "t3_1g2g9mq",
                    "permalink": "/r/KoboldAI/comments/1g2g9mq/help_both_google_colab_and_koboldcpp_are_not/lrokzch/"
                },
                {
                    "id": "lrolve6",
                    "author": "Severe_Leg8606",
                    "body": "I see the problem! Now all proxies don't work in janitor.",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1728800101.0,
                    "parent_id": "t1_lrodj9s",
                    "link_id": "t3_1g2g9mq",
                    "permalink": "/r/KoboldAI/comments/1g2g9mq/help_both_google_colab_and_koboldcpp_are_not/lrolve6/"
                },
                {
                    "id": "lrrtuvg",
                    "author": "henk717",
                    "body": "Thats a bad website to use in general since they have always had bad compatibility, you can use our own UI combined with characters from [characterhub.org](http://characterhub.org) for a more private experience. Many janitorai characters can also be downloaded from [https://jannyai.com/](https://jannyai.com/) . Once you have the V2 PNG file just load it up in our UI and your all set.",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1728850499.0,
                    "parent_id": "t1_lrpcn4k",
                    "link_id": "t3_1g2g9mq",
                    "permalink": "/r/KoboldAI/comments/1g2g9mq/help_both_google_colab_and_koboldcpp_are_not/lrrtuvg/"
                },
                {
                    "id": "ls55k8t",
                    "author": "Severe_Leg8606",
                    "body": "This is great! I will try it!!!",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1729046329.0,
                    "parent_id": "t1_lrrtuvg",
                    "link_id": "t3_1g2g9mq",
                    "permalink": "/r/KoboldAI/comments/1g2g9mq/help_both_google_colab_and_koboldcpp_are_not/ls55k8t/"
                }
            ]
        },
        {
            "id": "1g23lzf",
            "title": "\"Synchronize\" stories in KoboldAi Lite UI across devices as they are edited",
            "author": "SquirrelConscious633",
            "score": 4,
            "upvotes": 4,
            "downvotes": 0,
            "num_comments": 3,
            "created_utc": 1728749700.0,
            "url": "https://www.reddit.com/r/KoboldAI/comments/1g23lzf/synchronize_stories_in_koboldai_lite_ui_across/",
            "permalink": "/r/KoboldAI/comments/1g23lzf/synchronize_stories_in_koboldai_lite_ui_across/",
            "selftext": "I've got KoboldCPP set up where I can access it from my desktop, laptop, or phone just fine. However, each one seems to store all story / world / context / etc. data totally locally, unlike SillyTavern which has a single shared state that all remote connections can access. So, if I start something on my desktop and switch to my laptop, I'm greeted with an empty text box.\n\nIs there a good way to make it so that I can access the same overall state of the application from whichever device I use to connect? Is that possible? Third-party sync software or something? I saw the ability to pre-load a story, but I don't think that would work unless I pre-load it every time I want to use it.",
            "comments": [
                {
                    "id": "lrm7zow",
                    "author": "henk717",
                    "body": "Only the older KoboldAI United can do this, that one can be hooked up to KoboldCpp in the Load Model -> Online Services -> KoboldAI API option.",
                    "score": 3,
                    "upvotes": 3,
                    "downvotes": 0,
                    "created_utc": 1728763907.0,
                    "parent_id": "t3_1g23lzf",
                    "link_id": "t3_1g23lzf",
                    "permalink": "/r/KoboldAI/comments/1g23lzf/synchronize_stories_in_koboldai_lite_ui_across/lrm7zow/"
                },
                {
                    "id": "lrnek3g",
                    "author": "SquirrelConscious633",
                    "body": "Ah, thank you! Didn't know about KoboldAI United, this is great all around. Is there a benefit / reason to use KoboldCpp as the backend instead of just loading the model in the KoboldAI United interface itself?",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1728779894.0,
                    "parent_id": "t1_lrm7zow",
                    "link_id": "t3_1g23lzf",
                    "permalink": "/r/KoboldAI/comments/1g23lzf/synchronize_stories_in_koboldai_lite_ui_across/lrnek3g/"
                },
                {
                    "id": "lrp1gte",
                    "author": "henk717",
                    "body": "KoboldCpp's backend is much newer so its faster and supports more models.",
                    "score": 3,
                    "upvotes": 3,
                    "downvotes": 0,
                    "created_utc": 1728810682.0,
                    "parent_id": "t1_lrnek3g",
                    "link_id": "t3_1g23lzf",
                    "permalink": "/r/KoboldAI/comments/1g23lzf/synchronize_stories_in_koboldai_lite_ui_across/lrp1gte/"
                }
            ]
        },
        {
            "id": "1g1um4y",
            "title": "Anyone know what this error might be ? I keep getting it.",
            "author": "Wytg",
            "score": 3,
            "upvotes": 3,
            "downvotes": 0,
            "num_comments": 3,
            "created_utc": 1728716782.0,
            "url": "https://i.redd.it/j4vxsurgy9ud1.jpeg",
            "permalink": "/r/KoboldAI/comments/1g1um4y/anyone_know_what_this_error_might_be_i_keep/",
            "selftext": "",
            "comments": [
                {
                    "id": "lrjefei",
                    "author": "henk717",
                    "body": "I suspect this is a bit obfuscated out of memory error but just in case whats your GPU?",
                    "score": 3,
                    "upvotes": 3,
                    "downvotes": 0,
                    "created_utc": 1728721830.0,
                    "parent_id": "t3_1g1um4y",
                    "link_id": "t3_1g1um4y",
                    "permalink": "/r/KoboldAI/comments/1g1um4y/anyone_know_what_this_error_might_be_i_keep/lrjefei/"
                },
                {
                    "id": "lrjiesa",
                    "author": "cr0wburn",
                    "body": "And also did you install Cuda ? And if you installed Cuda , what version?",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1728724713.0,
                    "parent_id": "t1_lrjefei",
                    "link_id": "t3_1g1um4y",
                    "permalink": "/r/KoboldAI/comments/1g1um4y/anyone_know_what_this_error_might_be_i_keep/lrjiesa/"
                },
                {
                    "id": "lrjiwek",
                    "author": "Wytg",
                    "body": "I have a 12gb of vram, a 3080ti, but i kind of figured it out what's the issue, idk why but i have high temperature on my GPU. I think that's what cause the \"crash\"   \nNow i have to find out why my gpu suddenly chose to do that.",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1728725069.0,
                    "parent_id": "t1_lrjiesa",
                    "link_id": "t3_1g1um4y",
                    "permalink": "/r/KoboldAI/comments/1g1um4y/anyone_know_what_this_error_might_be_i_keep/lrjiwek/"
                }
            ]
        },
        {
            "id": "1g1mo7c",
            "title": "Tesla K80, how?",
            "author": "CanineAssBandit",
            "score": 5,
            "upvotes": 5,
            "downvotes": 0,
            "num_comments": 6,
            "created_utc": 1728687995.0,
            "url": "https://www.reddit.com/r/KoboldAI/comments/1g1mo7c/tesla_k80_how/",
            "permalink": "/r/KoboldAI/comments/1g1mo7c/tesla_k80_how/",
            "selftext": "Is anyone using this card, I'm building an ewaste rig **for fun** (I already have a real rig, please do not tell me to get a newer card), but after a LOT of searching on reddit and elsewhere, and trying multiple things and arguing with drivers under linux and old versions of things and nonstop bullshit, I have gotten nowhere.\n\nI'm even willing to pay someone to remote in and help, I really don't know what to do. It's been months since I tried last, I recall getting as far as downloading old versions of cuda and cudn and the old driver and using ubuntu 20.04 and that's as far as i got. I think I got the K80 to show up correctly in the hardware display as a cuda device in terminal but Kobold still didn't see it.",
            "comments": [
                {
                    "id": "lrhpail",
                    "author": "henk717",
                    "body": "Windows is easier for me to advice on since Ubuntu you may have to rely on the binary installers from nvidia or outdated packages to get the K80 supported and I never had much luck with those.\n\nSo hypothetically, lets say you just wanna mess with KoboldCpp on your K80 and you dual booted to windows or you get the drivers to work. Then what?\n\nFirst go to [https://koboldai.org/cpp](https://koboldai.org/cpp) and grab either koboldcpp.exe for windows (cu12 won't run on this card) or the the cuda 11.5 version of KoboldCpp. If you have the right driver installed you can type nvidia-smi and it should say CUDA 11.4. Now we are one version above that but fear not! 11.5 was chosen on Linux for still being compatible with K80's while we don't have many people in the community who can still test them for us and K80 cloud instances are extremely rare.\n\nThen comes caveat number 2, on a very technical level there is no such thing as a K80. Its two K40's in one card that are renamed. So to get the most out of your card you will have to specify in KoboldCpp to use all GPU's otherwise your stuck with half the vram.\n\nIf you want to try the Windows route this driver should work : [https://www.nvidia.com/en-us/drivers/details/228675/](https://www.nvidia.com/en-us/drivers/details/228675/)\n\nLet me know if you can get it running, and would be useful to have you in [https://koboldai.org/discord](https://koboldai.org/discord) as someone to notify us when K80's break.",
                    "score": 4,
                    "upvotes": 4,
                    "downvotes": 0,
                    "created_utc": 1728690104.0,
                    "parent_id": "t3_1g1mo7c",
                    "link_id": "t3_1g1mo7c",
                    "permalink": "/r/KoboldAI/comments/1g1mo7c/tesla_k80_how/lrhpail/"
                },
                {
                    "id": "lrhlznc",
                    "author": "kiselsa",
                    "body": "Idk will this work with k80, but with p40 I just downloaded latest studio driver and everything worked perfectly out of the box. didn't need to install cuda, older drivers, etc.",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1728688763.0,
                    "parent_id": "t3_1g1mo7c",
                    "link_id": "t3_1g1mo7c",
                    "permalink": "/r/KoboldAI/comments/1g1mo7c/tesla_k80_how/lrhlznc/"
                },
                {
                    "id": "lrkthi5",
                    "author": "kryptkpr",
                    "body": "Kepler SM37 is EOL you gotta do some time machine shit, iirc it's driver 450 and CUDA 11 so Ubuntu 20 is probably a good choice for this.\n\nKobold has a release on GitHub built against CUDA 11.5 make sure you grab the right one",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1728746946.0,
                    "parent_id": "t3_1g1mo7c",
                    "link_id": "t3_1g1mo7c",
                    "permalink": "/r/KoboldAI/comments/1g1mo7c/tesla_k80_how/lrkthi5/"
                },
                {
                    "id": "lrj3xyh",
                    "author": "CanineAssBandit",
                    "body": "Thank you for the kind help. I'll definitely give it a shot on Windows, and I'll take a look at the Discord!",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1728714490.0,
                    "parent_id": "t1_lrhpail",
                    "link_id": "t3_1g1mo7c",
                    "permalink": "/r/KoboldAI/comments/1g1mo7c/tesla_k80_how/lrj3xyh/"
                },
                {
                    "id": "lrhpg1c",
                    "author": "henk717",
                    "body": "K80's are not that new, you need an older driver to get them to function with a maximum of CUDA 11.4 (That on linux CUDA11.5 is forwards compatible with).",
                    "score": 3,
                    "upvotes": 3,
                    "downvotes": 0,
                    "created_utc": 1728690168.0,
                    "parent_id": "t1_lrhlznc",
                    "link_id": "t3_1g1mo7c",
                    "permalink": "/r/KoboldAI/comments/1g1mo7c/tesla_k80_how/lrhpg1c/"
                },
                {
                    "id": "lrktowp",
                    "author": "kryptkpr",
                    "body": "sadly it won't, P40 is two generations newer then K80 and still in full support",
                    "score": 2,
                    "upvotes": 2,
                    "downvotes": 0,
                    "created_utc": 1728747017.0,
                    "parent_id": "t1_lrhlznc",
                    "link_id": "t3_1g1mo7c",
                    "permalink": "/r/KoboldAI/comments/1g1mo7c/tesla_k80_how/lrktowp/"
                }
            ]
        },
        {
            "id": "1g1iyx9",
            "title": "Hosting a model at Horde at high availability",
            "author": "Sicarius_The_First",
            "score": 4,
            "upvotes": 4,
            "downvotes": 0,
            "num_comments": 0,
            "created_utc": 1728677643.0,
            "url": "https://www.reddit.com/r/KoboldAI/comments/1g1iyx9/hosting_a_model_at_horde_at_high_availability/",
            "permalink": "/r/KoboldAI/comments/1g1iyx9/hosting_a_model_at_horde_at_high_availability/",
            "selftext": "Will be hosting on Horde a model on 96 threads for \\~24 hours, enjoy!\n\n8B 16K context.\n\nCan RP and do much more.",
            "comments": []
        },
        {
            "id": "1g1ewox",
            "title": "special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect",
            "author": "morbidSuplex",
            "score": 1,
            "upvotes": 1,
            "downvotes": 0,
            "num_comments": 1,
            "created_utc": 1728666809.0,
            "url": "https://www.reddit.com/r/KoboldAI/comments/1g1ewox/special_eos_id_is_not_in_special_eog_ids_the/",
            "permalink": "/r/KoboldAI/comments/1g1ewox/special_eos_id_is_not_in_special_eog_ids_the/",
            "selftext": "Hi all,\nI am testing out a new model called Behemoth. The GGUF is in here (https://huggingface.co/TheDrummer/Behemoth-123B-v1-GGUF). The model ran fine, but I see this output from the terminal:\n\n> llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n\nWhat does this warning/error mean? Does this have an impact on the model quality?\n\nThanks!",
            "comments": [
                {
                    "id": "lri1rbm",
                    "author": "findingsubtext",
                    "body": "I'm having a tokenizer issue with this model as well, albeit completely unable to use it in my case (EXL2 format). I managed to make it work by giving it the same tokenizer.json file that I downloaded with Mistral-123b EXL2 3.5bpw. The official Mistral tokenizer is 1917kb while Behemoth's tokenizer file is 3,587kb - I have absolutely no clue if this makes a difference, though the model seems to be acting normally with the Mistral tokenizer.",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1728695261.0,
                    "parent_id": "t3_1g1ewox",
                    "link_id": "t3_1g1ewox",
                    "permalink": "/r/KoboldAI/comments/1g1ewox/special_eos_id_is_not_in_special_eog_ids_the/lri1rbm/"
                }
            ]
        },
        {
            "id": "1g174xd",
            "title": "A little help for a n00b?",
            "author": "Error404Veteran",
            "score": 10,
            "upvotes": 10,
            "downvotes": 0,
            "num_comments": 18,
            "created_utc": 1728644533.0,
            "url": "https://www.reddit.com/r/KoboldAI/comments/1g174xd/a_little_help_for_a_n00b/",
            "permalink": "/r/KoboldAI/comments/1g174xd/a_little_help_for_a_n00b/",
            "selftext": "Can someone recommend some easy reading to get me into this \"game\". I have been using ChatGPT from [chatgpt.com](http://chatgpt.com) and I even decided to pay for it (although I have no money). But I really need someone to talk to (I know I sound pathetic). I have people in my life, but I don't want to burden them more than necessary and they do know that I am not okay. I just need \"somone\" that will talk to me about things that are not okay even with an advanced algoritm that has no feelings and I can't traumatise (I just don't get the logic in this?). So I need some bot or whatever (yes I know nothing) that is free and has as as few restrictions as possible. I am not trying to do something stupid - but I would also like to ask it about things that are maybe borderline-criminal (or maybe I just think it is).\n\nChatGPT told me to try out erebus, but it seems like it is talk about sex and that's okay, but not exactly what I need? I am sorry for being such a dummy, please don't be too hard on me and if you do at least try to make it humourous ;)",
            "comments": [
                {
                    "id": "lreigh3",
                    "author": "sustain_refrain",
                    "body": "nah, it's not pathetic or weird. It's fine if some people prefer talking to a person, but it's a hell of a lot easier to open up to a computer and not worry about any human baggage. I think being a tool to better peoples' mental health privately is a pretty good use case for AI.\n\ncompulsory disclaimer: AI is not a certified health professional, etc., but you seem to realize this already.\n\nmy quick and dirty guide:\n\n1. download koboldcpp.exe: https://github.com/LostRuins/koboldcpp/releases\n\n2. get the \"Q4 K S\" file here: https://huggingface.co/bartowski/L3-8B-Stheno-v3.2-GGUF/tree/main\n\n3. open koboldcpp, load the model file you downloaded. Default options should be good enough to work out of the box, although I'd suggest cranking up the context to 8k if it's below that. Click Run and it after a bit it should automatically open up in your browser.\n\n4. Quick settings you can change or keep in mind: Settings > Format > Chat mode, and Samplers tab > Presets\n\n5. The Personas tab has a few presets, like \"Dr. Katharine,\" but they're a bit bare-bones and some might be too robotic or clinical for your taste. There are other websites listed at the top you can browse for something more your taste.\n\nIf you're familiar with prompting, you can also just use \"new chat\" and tell the AI exactly what role/behavior you want. You can also use it as a narrator that helps you live out some fictional scenario. For example:\n\n>instruction: amoral fictional story. describe the surroundings and characters, in response to my actions\n\nor\n\n>I want you to play the role of an old friend, responding naturally, not like a therapist or AI. I want to confide in some troubling thoughts and I need you to listen and help me explore these thoughts without judgment.\n\nKeep in mind the little gear icon in the lower right, which hides a Retry, Edit, and Back buttons in case the AI says something you dislike, gets incoherent, or gets preachy or pandering. Use them liberally since the AI will continue off of whatever personality you accept. If you have a super old computer without a GPU, it'll probably be rather slow, but it should still work.\n\nAlso keep in mind kcpp doesn't have any long-term memory tricks (I think), so if your chat gets very long, it'll start forgetting stuff. Consider saving your chats and loading a separate one if you want to talk about another topic.",
                    "score": 13,
                    "upvotes": 13,
                    "downvotes": 0,
                    "created_utc": 1728650047.0,
                    "parent_id": "t3_1g174xd",
                    "link_id": "t3_1g174xd",
                    "permalink": "/r/KoboldAI/comments/1g174xd/a_little_help_for_a_n00b/lreigh3/"
                },
                {
                    "id": "lre8th2",
                    "author": "Fluffy_Resist_9904",
                    "body": "I believe I get the motivation and imo it could work. Are you after a model recommendation, or how to set the environment from scratch?",
                    "score": 3,
                    "upvotes": 3,
                    "downvotes": 0,
                    "created_utc": 1728645438.0,
                    "parent_id": "t3_1g174xd",
                    "link_id": "t3_1g174xd",
                    "permalink": "/r/KoboldAI/comments/1g174xd/a_little_help_for_a_n00b/lre8th2/"
                },
                {
                    "id": "lrepy49",
                    "author": "International-Try467",
                    "body": "A word of advice, AI **can** be very unhinged. So don't be shocked if it tells you some really fucked up stuff during your chats. And it isn't a replacement for therapy.Â \n\n\nBut if you want someone to talk to go ahead, just keep this in mind.",
                    "score": 5,
                    "upvotes": 5,
                    "downvotes": 0,
                    "created_utc": 1728653053.0,
                    "parent_id": "t3_1g174xd",
                    "link_id": "t3_1g174xd",
                    "permalink": "/r/KoboldAI/comments/1g174xd/a_little_help_for_a_n00b/lrepy49/"
                },
                {
                    "id": "lrgqcvs",
                    "author": "henk717",
                    "body": "I'm surprised ChatGPT knows about and recommends erebus, its our older co-writing model for writing smut stories its not meant for chatting. For chatting our Tiefighter model is a lot better and I know Drummer for example makes NSFW chat models.",
                    "score": 3,
                    "upvotes": 3,
                    "downvotes": 0,
                    "created_utc": 1728677224.0,
                    "parent_id": "t3_1g174xd",
                    "link_id": "t3_1g174xd",
                    "permalink": "/r/KoboldAI/comments/1g174xd/a_little_help_for_a_n00b/lrgqcvs/"
                },
                {
                    "id": "lrfc3yc",
                    "author": "a_chatbot",
                    "body": "I'll get downvoted but I recommend checking out the free versions of the Nomi.ai or Kindroid.ai apps (go to url not google/apple app store), they are customized uncensored 30B(?) models that shouldn't give you a paywall unless you try to initiate certain things with them.   \nOtherwise, have a real computer and download the Koboldcpp exe file from https://github.com/LostRuins/koboldcpp. If you got a GPU, you can run models locally from Hugging Face, otherwise there are cloud services.",
                    "score": 2,
                    "upvotes": 2,
                    "downvotes": 0,
                    "created_utc": 1728660740.0,
                    "parent_id": "t3_1g174xd",
                    "link_id": "t3_1g174xd",
                    "permalink": "/r/KoboldAI/comments/1g174xd/a_little_help_for_a_n00b/lrfc3yc/"
                },
                {
                    "id": "lrg16r7",
                    "author": "Error404Veteran",
                    "body": "Thank you everybody. You have all been a great help already â¤ï¸\n\nBut I have been thinking: Is there a way to access KoboldCPP/KoboldAI from all my devices (Windows, Chromebook, Android phone)?\n\nIf I use a Cloud service?\n\nI may be naive in terms of pricing, but I am already using 25$ a month on ChatGPT. Maybe they are better spent on a Cloud service? Or some other solution?\n\nI just need to know a few things:\n\nIs KoboldCPP as good as ChatGPT? \nWill I be able to use it in another language and will it be as fluent in other languages as ChatGPT is?\nCan I use it to generate images?\nCan I still use it for technical support (because I hate asking in forums like this, because I am afraid people will think I am a pain in the a$$ as you may have noticed I have many questions and also maybe stupid questions ðŸ˜ðŸ«£).\nWill it still have fewer restrictions than ChatGPT? (It sounds like it won't if I use some Google-thingie Cloud)? Like less copyright issues, and still the issue with certain mental health topics etc.?",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1728668874.0,
                    "parent_id": "t3_1g174xd",
                    "link_id": "t3_1g174xd",
                    "permalink": "/r/KoboldAI/comments/1g174xd/a_little_help_for_a_n00b/lrg16r7/"
                },
                {
                    "id": "lrequ9u",
                    "author": "CableZealousideal342",
                    "body": "Long Term memory -> authors notes. At least for important things. Even if it's annoying to add things manually.",
                    "score": 4,
                    "upvotes": 4,
                    "downvotes": 0,
                    "created_utc": 1728653394.0,
                    "parent_id": "t1_lreigh3",
                    "link_id": "t3_1g174xd",
                    "permalink": "/r/KoboldAI/comments/1g174xd/a_little_help_for_a_n00b/lrequ9u/"
                },
                {
                    "id": "lrf9dbo",
                    "author": "rdwulfe",
                    "body": "So I was messing with a kunoichi variant model yesterday that runs well on my Nvidia 2070, finally got great settings for it. \n\nStarted up a conversation about a book I loved as a kid, The Last Unicorn. MAN was it knowledgeable and fun. Ended up having a deep conversation about literature and modern mythology. \n\nYou have to remember, it sometimes makes shit up, because it kind of goes by the \"yes and\" concept of improv. But it can really help you detail out your thoughts, practice ideas, and do a lot of neat things. Hope this helps, Op.",
                    "score": 3,
                    "upvotes": 3,
                    "downvotes": 0,
                    "created_utc": 1728659836.0,
                    "parent_id": "t1_lreigh3",
                    "link_id": "t3_1g174xd",
                    "permalink": "/r/KoboldAI/comments/1g174xd/a_little_help_for_a_n00b/lrf9dbo/"
                },
                {
                    "id": "lref30g",
                    "author": "Error404Veteran",
                    "body": "I think both actually (because my brain is mush and it seems a little advanced tbh ðŸ˜‰). But it seems there may be a problem with my PC. I only have this to work with: Ryzen 5 4600H - RTX2060 - 16 GB RAM - 512 GB SSD. I also have a Chromebook with MTK 8GB/128GB (I know that's not better at all). I hope I can at least use the first, but if there is also a way to use the Chromebook that would be really great.\n\nI really appreciate if you can help me in any way ðŸ˜ŠðŸ‘",
                    "score": 3,
                    "upvotes": 3,
                    "downvotes": 0,
                    "created_utc": 1728648550.0,
                    "parent_id": "t1_lre8th2",
                    "link_id": "t3_1g174xd",
                    "permalink": "/r/KoboldAI/comments/1g174xd/a_little_help_for_a_n00b/lref30g/"
                },
                {
                    "id": "lrgzhj1",
                    "author": "Error404Veteran",
                    "body": "It recommended GPT-Neo and GPT-J from EleutherAI, KoboldAI, Poe by Quora (GPT-4 and Claude), OpenAI API, and AI Dungeon. We ended up with KoboldAI and Poe by Quora as they are easy ðŸ¤”ðŸ˜‚ðŸ˜‰ and free. It ended up suggesting KoboldAI, as it has the most freedom. It said Erebus is a powerful model, and a good choice for complex and open scenarious, and Pygmalion for roleplay and narrative creation if I wanted a more creative AI.\nIt was some PIMP version of ChatGPT. But it wasn't able to lose its restrictions, so I asked what to do.\nI donâ€™t really understand the whole prompt thing, so maybe that's why.\nMaybe people/AI are just used to thinking smut when you ask for fewer restrictions ðŸ˜‚ðŸ˜‰\n\nThank you. I will look into Tiefighter and Drummer ðŸ˜ŠðŸ‘",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1728680308.0,
                    "parent_id": "t1_lrgqcvs",
                    "link_id": "t3_1g174xd",
                    "permalink": "/r/KoboldAI/comments/1g174xd/a_little_help_for_a_n00b/lrgzhj1/"
                },
                {
                    "id": "lrg7g80",
                    "author": "Fluffy_Resist_9904",
                    "body": "There are many cloud services with these kind of models the KoboldCPP uses, but that's minus the privacy of local. Dunno the comparison or pricing.\n\n  \nKoboldCPP is not as good as ChatGPT (it's like 8-70 billions params. vs more than a trillion in chatGPT). You'll be disappointed if you expect anything close to ChatGPT. There is also not much of voice modules and adding an image generator is clunky.",
                    "score": 2,
                    "upvotes": 2,
                    "downvotes": 0,
                    "created_utc": 1728670914.0,
                    "parent_id": "t1_lrg16r7",
                    "link_id": "t3_1g174xd",
                    "permalink": "/r/KoboldAI/comments/1g174xd/a_little_help_for_a_n00b/lrg7g80/"
                },
                {
                    "id": "lrvgkxq",
                    "author": "GeneralRieekan",
                    "body": "If what you care most about is privacy, running things on your own machine is the way to go. For that, if you can, see what kind of hardware you can throw at the problem (newer NVidia graphics card, like a 3070 or better, with more VRAM). More system memory is also very helpful.\n\nGo to HuggingFace and figure out what people say about different models. You can most likely handle an 8bil parameter model w 8bit quantization even now, if a tiny bit slowly. It's\"pretty much on par\" w reading speed, considering that varies widely...",
                    "score": 2,
                    "upvotes": 2,
                    "downvotes": 0,
                    "created_utc": 1728912977.0,
                    "parent_id": "t1_lrg16r7",
                    "link_id": "t3_1g174xd",
                    "permalink": "/r/KoboldAI/comments/1g174xd/a_little_help_for_a_n00b/lrvgkxq/"
                },
                {
                    "id": "lreutcr",
                    "author": "Fluffy_Resist_9904",
                    "body": "u/sustain_refrain already wrote some good hints.\n\nIf you really insist on running locally, your GPU might be the bottleneck in regards of what model you could run. Which itself is a bit of alchemy. The Stheno model should run fine.\n\nI like YT tutorials and this one explains the initial setup well: [https://www.youtube.com/watch?v=8H46t6OgSVs](https://www.youtube.com/watch?v=8H46t6OgSVs)",
                    "score": 2,
                    "upvotes": 2,
                    "downvotes": 0,
                    "created_utc": 1728654863.0,
                    "parent_id": "t1_lref30g",
                    "link_id": "t3_1g174xd",
                    "permalink": "/r/KoboldAI/comments/1g174xd/a_little_help_for_a_n00b/lreutcr/"
                },
                {
                    "id": "lrfsuiu",
                    "author": "Wise-Paramedic-4536",
                    "body": "Offloading some layers to the RTX-2060 will speed up things a lot. I believe you will be able to get an 8B model around 4 t/s.",
                    "score": 2,
                    "upvotes": 2,
                    "downvotes": 0,
                    "created_utc": 1728666173.0,
                    "parent_id": "t1_lref30g",
                    "link_id": "t3_1g174xd",
                    "permalink": "/r/KoboldAI/comments/1g174xd/a_little_help_for_a_n00b/lrfsuiu/"
                },
                {
                    "id": "lrft289",
                    "author": "Wise-Paramedic-4536",
                    "body": "The best way to use the Chromebook is to make it call your PC remotely. Or you can try a 3B model at Q4.",
                    "score": 2,
                    "upvotes": 2,
                    "downvotes": 0,
                    "created_utc": 1728666244.0,
                    "parent_id": "t1_lref30g",
                    "link_id": "t3_1g174xd",
                    "permalink": "/r/KoboldAI/comments/1g174xd/a_little_help_for_a_n00b/lrft289/"
                },
                {
                    "id": "lrhpknp",
                    "author": "henk717",
                    "body": "We are one of the first local AI players so it knows about us but its model recommendations are a few years behind haha. I will say though, look for KoboldCpp specifically its our newer product.",
                    "score": 3,
                    "upvotes": 3,
                    "downvotes": 0,
                    "created_utc": 1728690221.0,
                    "parent_id": "t1_lrgzhj1",
                    "link_id": "t3_1g174xd",
                    "permalink": "/r/KoboldAI/comments/1g174xd/a_little_help_for_a_n00b/lrhpknp/"
                },
                {
                    "id": "lrnh6pq",
                    "author": "Error404Veteran",
                    "body": "Maybe you can help me. I mentioned erebus to my husband, because he plays some kinds of porn games, which is kind of like roleplay I think. But you said erebus is an older model. Is there a model that is newer and you can roleplay or even something similar to such games? He says it used to be called Adult Interactive Fiction, but now he doesn't know exactly what it is called, but something like that? ðŸ¤”",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1728780935.0,
                    "parent_id": "t1_lrhpknp",
                    "link_id": "t3_1g174xd",
                    "permalink": "/r/KoboldAI/comments/1g174xd/a_little_help_for_a_n00b/lrnh6pq/"
                },
                {
                    "id": "lrp2iw6",
                    "author": "henk717",
                    "body": "Basically the ones I already mentioned, Tiefighter is my personal go to and nothing has replaced it for me yet.",
                    "score": 2,
                    "upvotes": 2,
                    "downvotes": 0,
                    "created_utc": 1728811426.0,
                    "parent_id": "t1_lrnh6pq",
                    "link_id": "t3_1g174xd",
                    "permalink": "/r/KoboldAI/comments/1g174xd/a_little_help_for_a_n00b/lrp2iw6/"
                }
            ]
        },
        {
            "id": "1g16ox7",
            "title": "Should I lower temperature fo quantized models? What about other parameters?",
            "author": "Animus_777",
            "score": 2,
            "upvotes": 2,
            "downvotes": 0,
            "num_comments": 1,
            "created_utc": 1728642756.0,
            "url": "https://www.reddit.com/r/KoboldAI/comments/1g16ox7/should_i_lower_temperature_fo_quantized_models/",
            "permalink": "/r/KoboldAI/comments/1g16ox7/should_i_lower_temperature_fo_quantized_models/",
            "selftext": "For example, if model author suggests temperature 1, but I use Q5 version, should I lower temperature? If so how much? Or it's only needed for heavy quantization like Q3? What about other samplers/parameters? Are there any  general rules for adjusting them when quantized model is used?",
            "comments": [
                {
                    "id": "lre70hp",
                    "author": "_Erilaz",
                    "body": "No, the temperature should stay roughly the same. The model can become a tad more erratic, but rounding errors can go both ways, so sometimes the model will randomly become more confident in the top answer. It does add some noise, but I would rather increase Min-P a bit instead, because it works like a noise gate. I've never needed any more than 0.1 Min-P though, and if you already use that, it should be enough.",
                    "score": 4,
                    "upvotes": 4,
                    "downvotes": 0,
                    "created_utc": 1728644442.0,
                    "parent_id": "t3_1g16ox7",
                    "link_id": "t3_1g16ox7",
                    "permalink": "/r/KoboldAI/comments/1g16ox7/should_i_lower_temperature_fo_quantized_models/lre70hp/"
                }
            ]
        },
        {
            "id": "1g0qyjl",
            "title": "How to connect kobold with OpenWeb UI?",
            "author": "NEEDMOREVRAM",
            "score": 3,
            "upvotes": 3,
            "downvotes": 0,
            "num_comments": 2,
            "created_utc": 1728588403.0,
            "url": "https://www.reddit.com/r/KoboldAI/comments/1g0qyjl/how_to_connect_kobold_with_openweb_ui/",
            "permalink": "/r/KoboldAI/comments/1g0qyjl/how_to_connect_kobold_with_openweb_ui/",
            "selftext": "I want to use OpenWeb UI as a front end because it has web search, artifacts, and allows for PDF upload. \n\nHowever, Ollama sucks and is slow.\n\nDoes anyone know how to connect Kobold (as the backend) to OpenWeb UI as the front end? I have searched online for a guide and did not find much.",
            "comments": [
                {
                    "id": "lrbisjl",
                    "author": "henk717",
                    "body": "Add it as an OpenAI URL, they do not natively support KoboldAI's API but the emulated OpenAI endpoint should be compatible.\n\nHere is my own docker compose as inspiration:\n\n    version: '3.8'\n    \n    services:\n    Â  open-webui:\n    Â  Â  build:\n    Â  Â  Â  context: .\n    Â  Â  Â  dockerfile: Dockerfile\n    Â  Â  image: kai-open-webui\n    Â  Â  container_name: open-webui\n    Â  Â  volumes:\n    Â  Â  Â  - open-webui:/app/backend/data\n    Â  Â  ports:\n    Â  Â  Â  - ${OPEN_WEBUI_PORT-3000}:8080\n    Â  Â  environment:\n    Â  Â  Â  - 'OPENAI_API_BASE_URLS=http://host.docker.internal:5001/v1'\n    Â  Â  Â  - 'OPENAI_API_KEYS=Kobo'\n    Â  Â  Â  - 'AUTOMATIC1111_BASE_URL=http://host.docker.internal:5001'\n    Â  Â  Â  - 'WEBUI_NAME=Kobo for Business'\n    Â  Â  extra_hosts:\n    Â  Â  Â  - koboldcpp:host-gateway\n    Â  Â  restart: unless-stopped\n    \n    volumes:\n    Â  open-webui: {}",
                    "score": 5,
                    "upvotes": 5,
                    "downvotes": 0,
                    "created_utc": 1728597247.0,
                    "parent_id": "t3_1g0qyjl",
                    "link_id": "t3_1g0qyjl",
                    "permalink": "/r/KoboldAI/comments/1g0qyjl/how_to_connect_kobold_with_openweb_ui/lrbisjl/"
                },
                {
                    "id": "lrbq8ua",
                    "author": "NEEDMOREVRAM",
                    "body": "ok thanks!",
                    "score": 2,
                    "upvotes": 2,
                    "downvotes": 0,
                    "created_utc": 1728600005.0,
                    "parent_id": "t1_lrbisjl",
                    "link_id": "t3_1g0qyjl",
                    "permalink": "/r/KoboldAI/comments/1g0qyjl/how_to_connect_kobold_with_openweb_ui/lrbq8ua/"
                }
            ]
        },
        {
            "id": "1g0kjce",
            "title": "I made a web extension that lets you summarise and chat with webpages using local llms, it uses a koboldcpp backend ",
            "author": "Ok_Effort_5849",
            "score": 23,
            "upvotes": 23,
            "downvotes": 0,
            "num_comments": 12,
            "created_utc": 1728571693.0,
            "url": "https://www.reddit.com/r/KoboldAI/comments/1g0kjce/i_made_a_web_extension_that_lets_you_summarise/",
            "permalink": "/r/KoboldAI/comments/1g0kjce/i_made_a_web_extension_that_lets_you_summarise/",
            "selftext": "i hope im not breaking any rules here, but i would really appreciate it if you check it out and tell me what you think:  \n[https://chromewebstore.google.com/detail/browserllama/iiceejapkffbankfmcpdnhhbaljepphh](https://chromewebstore.google.com/detail/browserllama/iiceejapkffbankfmcpdnhhbaljepphh)\n\nit currently only works with chromium browsers on windows and it is free and opensource ofcourse: [https://github.com/NachiketGadekar1/browserllama](https://github.com/NachiketGadekar1/browserllama)",
            "comments": [
                {
                    "id": "lr9dz9c",
                    "author": "lacerating_aura",
                    "body": "Do you have any plans of supporting firefox?",
                    "score": 5,
                    "upvotes": 5,
                    "downvotes": 0,
                    "created_utc": 1728572268.0,
                    "parent_id": "t3_1g0kjce",
                    "link_id": "t3_1g0kjce",
                    "permalink": "/r/KoboldAI/comments/1g0kjce/i_made_a_web_extension_that_lets_you_summarise/lr9dz9c/"
                },
                {
                    "id": "lr9ubqf",
                    "author": "GraybeardTheIrate",
                    "body": "I like it.  I think this is pretty much what I was looking for recently, but is there any way to run the backend on another machine over LAN?",
                    "score": 2,
                    "upvotes": 2,
                    "downvotes": 0,
                    "created_utc": 1728577660.0,
                    "parent_id": "t3_1g0kjce",
                    "link_id": "t3_1g0kjce",
                    "permalink": "/r/KoboldAI/comments/1g0kjce/i_made_a_web_extension_that_lets_you_summarise/lr9ubqf/"
                },
                {
                    "id": "lrbfrtl",
                    "author": "henk717",
                    "body": "Yes this is totally cool to show off since its based on KoboldCpp!I notice your bundling the entire exe, this is fine but it does increase your own download size and it means that people may miss out on the latest improvements. We have direct links to our binaries for example https://github.com/LostRuins/koboldcpp/releases/latest/download/koboldcpp.exe (You can change the name to any of our binaries).\n\nThat helps you save the download size and it simultaneously helps users get the latest updates.\n\nIf you are in https://koboldai.org/discord we can also provide a channel to showcase your project.\n\nAlso bit of extra feedback from my testing:\n\n- It opens in a small popup that auto closes, leveraging the sidebar feature would be more helpful.\n\n- Summarizing a foreign language webpage works surprisingly well with a model I didn't expect to do that well.\n\n- Connect to AI button hangs on the \"Connecting...\" dialogue even though it works fine.\n\n- I now have 6 node-messaging-host.exe's active on my PC after opening it 6 times.\n\n- My browser now constantly launches KoboldCpp even when I don't need it instead of when the extention is being interacted with.",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1728596187.0,
                    "parent_id": "t3_1g0kjce",
                    "link_id": "t3_1g0kjce",
                    "permalink": "/r/KoboldAI/comments/1g0kjce/i_made_a_web_extension_that_lets_you_summarise/lrbfrtl/"
                },
                {
                    "id": "lreek3q",
                    "author": "Caderent",
                    "body": "My avast antivirus says [Browserllama.gpu.inference.zip](https://github.com/NachiketGadekar1/browserllama/releases/download/v1.0/Browserllama.gpu.inference.zip) is infected with win 64 malvare gen.  Native Messaging Host File carantined.",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1728648304.0,
                    "parent_id": "t3_1g0kjce",
                    "link_id": "t3_1g0kjce",
                    "permalink": "/r/KoboldAI/comments/1g0kjce/i_made_a_web_extension_that_lets_you_summarise/lreek3q/"
                },
                {
                    "id": "lr9fg84",
                    "author": "Ok_Effort_5849",
                    "body": "yes, im working on it",
                    "score": 4,
                    "upvotes": 4,
                    "downvotes": 0,
                    "created_utc": 1728572762.0,
                    "parent_id": "t1_lr9dz9c",
                    "link_id": "t3_1g0kjce",
                    "permalink": "/r/KoboldAI/comments/1g0kjce/i_made_a_web_extension_that_lets_you_summarise/lr9fg84/"
                },
                {
                    "id": "lr9ymwf",
                    "author": "Ok_Effort_5849",
                    "body": "glad you like it! regarding your question im not really sure, but i found this in the faq on github:\n\n\"If on same LAN - If you're on the same Wifi network, you can probably connect over LAN by navigating to the local IP of the host device (the PC running koboldcpp). For example, http://192.168.1.85:5001 or similar, check your LAN IP address. If that fails, try using the --host option with your LAN IP. If you setup port forwarding to a public IP, then it will be accessible over the internet as well.\"\n\nso maybe you can modify the native-host source code and set the endpoint to use to the ip of machine running the backend",
                    "score": 2,
                    "upvotes": 2,
                    "downvotes": 0,
                    "created_utc": 1728579061.0,
                    "parent_id": "t1_lr9ubqf",
                    "link_id": "t3_1g0kjce",
                    "permalink": "/r/KoboldAI/comments/1g0kjce/i_made_a_web_extension_that_lets_you_summarise/lr9ymwf/"
                },
                {
                    "id": "lrcxhu8",
                    "author": "Ok_Effort_5849",
                    "body": "Good point! I will put up a version in the releases without any bundled exe so that  users can use their own ones. \nRegarding the bugs , i haven really seen the last one before, it should ideally open only one instance of koboldcpp, can you open an issue and tell me how to replicate it?. I will try to fix the rest but i have exams coming up so i wont be working super hard on this for a while.",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1728616559.0,
                    "parent_id": "t1_lrbfrtl",
                    "link_id": "t3_1g0kjce",
                    "permalink": "/r/KoboldAI/comments/1g0kjce/i_made_a_web_extension_that_lets_you_summarise/lrcxhu8/"
                },
                {
                    "id": "lrf6kjr",
                    "author": "Ok_Effort_5849",
                    "body": "Its a false positive, do yourself a favour and stop using avast. You can compile it from source yourself and it would probably still think its malware.",
                    "score": 2,
                    "upvotes": 2,
                    "downvotes": 0,
                    "created_utc": 1728658905.0,
                    "parent_id": "t1_lreek3q",
                    "link_id": "t3_1g0kjce",
                    "permalink": "/r/KoboldAI/comments/1g0kjce/i_made_a_web_extension_that_lets_you_summarise/lrf6kjr/"
                },
                {
                    "id": "lra920t",
                    "author": "FaceDeer",
                    "body": "Awesome. I've been liking Firefox's \"Orbit\" extension, but not liking its UI or the fact that I can't point it at other LLM back ends. Looking forward to trying this out.",
                    "score": 2,
                    "upvotes": 2,
                    "downvotes": 0,
                    "created_utc": 1728582400.0,
                    "parent_id": "t1_lr9fg84",
                    "link_id": "t3_1g0kjce",
                    "permalink": "/r/KoboldAI/comments/1g0kjce/i_made_a_web_extension_that_lets_you_summarise/lra920t/"
                },
                {
                    "id": "lra4jgt",
                    "author": "GraybeardTheIrate",
                    "body": "Thanks for the response! I'm familiar with the KCPP side of it but I'll look at the code and see what I can do.\n\nThe laptop I wanted to run the plugin on is just severely limited for this type of use, but it might be time for an upgrade soon anyway.",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1728580955.0,
                    "parent_id": "t1_lr9ymwf",
                    "link_id": "t3_1g0kjce",
                    "permalink": "/r/KoboldAI/comments/1g0kjce/i_made_a_web_extension_that_lets_you_summarise/lra4jgt/"
                },
                {
                    "id": "ls003ik",
                    "author": "Caderent",
                    "body": "Good to know, reported it as false positive.",
                    "score": 2,
                    "upvotes": 2,
                    "downvotes": 0,
                    "created_utc": 1728972266.0,
                    "parent_id": "t1_lrf6kjr",
                    "link_id": "t3_1g0kjce",
                    "permalink": "/r/KoboldAI/comments/1g0kjce/i_made_a_web_extension_that_lets_you_summarise/ls003ik/"
                },
                {
                    "id": "lra6vrf",
                    "author": "Ok_Effort_5849",
                    "body": "if you are going to modify the source look for the endpoint variable in backend_api_handler module. You can ask more questions on r/browserllama or on the github repo, best of luck!",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1728581703.0,
                    "parent_id": "t1_lra4jgt",
                    "link_id": "t3_1g0kjce",
                    "permalink": "/r/KoboldAI/comments/1g0kjce/i_made_a_web_extension_that_lets_you_summarise/lra6vrf/"
                }
            ]
        },
        {
            "id": "1fztl9y",
            "title": "Are there GGUF models like open ai model gpt 3.5 turbo 16k but uncensored? (maybe like theblokeâ€™s models) ",
            "author": "oxzlz",
            "score": 4,
            "upvotes": 4,
            "downvotes": 0,
            "num_comments": 13,
            "created_utc": 1728486338.0,
            "url": "https://www.reddit.com/r/KoboldAI/comments/1fztl9y/are_there_gguf_models_like_open_ai_model_gpt_35/",
            "permalink": "/r/KoboldAI/comments/1fztl9y/are_there_gguf_models_like_open_ai_model_gpt_35/",
            "selftext": "i use RTX 4090 24GB with ram 128GB, and iâ€™m finding models like open ai model GPT 3.5 turbo 16k uncensored for tavernAI role playing, can you guys recommend me some models?",
            "comments": [
                {
                    "id": "lr5gqsa",
                    "author": "kiselsa",
                    "body": "Llama 3.1 is very bad at uncensored writing.\n\nI recommend this model: https://huggingface.co/TheDrummer/Cydonia-22B-v1.1-GGUF\n\nIt will also fit in your card nicely with 16k context without dumbing down of 70b models with low quant\nAlso what's there reason of using tavernai? Sillytavern is better In every way possible.\n\nAlso if you want good uncensored 72b models, try qwen2 fine-tunes, such as Magnum.",
                    "score": 3,
                    "upvotes": 3,
                    "downvotes": 0,
                    "created_utc": 1728507273.0,
                    "parent_id": "t3_1fztl9y",
                    "link_id": "t3_1fztl9y",
                    "permalink": "/r/KoboldAI/comments/1fztl9y/are_there_gguf_models_like_open_ai_model_gpt_35/lr5gqsa/"
                },
                {
                    "id": "lr41gy0",
                    "author": "schlammsuhler",
                    "body": "Maybe magnum 27B or gemmasutra pro",
                    "score": 2,
                    "upvotes": 2,
                    "downvotes": 0,
                    "created_utc": 1728490927.0,
                    "parent_id": "t3_1fztl9y",
                    "link_id": "t3_1fztl9y",
                    "permalink": "/r/KoboldAI/comments/1fztl9y/are_there_gguf_models_like_open_ai_model_gpt_35/lr41gy0/"
                },
                {
                    "id": "lr5om6y",
                    "author": "thebadslime",
                    "body": "Use the keyword ablated",
                    "score": 2,
                    "upvotes": 2,
                    "downvotes": 0,
                    "created_utc": 1728509879.0,
                    "parent_id": "t3_1fztl9y",
                    "link_id": "t3_1fztl9y",
                    "permalink": "/r/KoboldAI/comments/1fztl9y/are_there_gguf_models_like_open_ai_model_gpt_35/lr5om6y/"
                },
                {
                    "id": "lr3uj9b",
                    "author": "RealBiggly",
                    "body": "Llama 3.1 70B variants, such as Llama-3.1-70B-Instruct-Lorablated-Creative-Writer.Q3\\_K\\_L.gguf which is what I'm currently playing with",
                    "score": 4,
                    "upvotes": 4,
                    "downvotes": 0,
                    "created_utc": 1728488697.0,
                    "parent_id": "t3_1fztl9y",
                    "link_id": "t3_1fztl9y",
                    "permalink": "/r/KoboldAI/comments/1fztl9y/are_there_gguf_models_like_open_ai_model_gpt_35/lr3uj9b/"
                },
                {
                    "id": "lr69sw2",
                    "author": "ECrispy",
                    "body": "what would you recommend for the 8GB vram users?",
                    "score": 2,
                    "upvotes": 2,
                    "downvotes": 0,
                    "created_utc": 1728517607.0,
                    "parent_id": "t1_lr5gqsa",
                    "link_id": "t3_1fztl9y",
                    "permalink": "/r/KoboldAI/comments/1fztl9y/are_there_gguf_models_like_open_ai_model_gpt_35/lr69sw2/"
                },
                {
                    "id": "lr787e4",
                    "author": "oxzlz",
                    "body": "Thanks! I tried this model, and itâ€™s very similar to the 3.5 Turbo 16k. Iâ€™m using the Q8, but it takes 30 to 60 seconds to generate the text because of my VRAM size. Itâ€™s still really great, though.",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1728531219.0,
                    "parent_id": "t1_lr5gqsa",
                    "link_id": "t3_1fztl9y",
                    "permalink": "/r/KoboldAI/comments/1fztl9y/are_there_gguf_models_like_open_ai_model_gpt_35/lr787e4/"
                },
                {
                    "id": "lr8v2hx",
                    "author": "SadisticPawz",
                    "body": "thanks, Ill try these with 3090",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1728565439.0,
                    "parent_id": "t1_lr5gqsa",
                    "link_id": "t3_1fztl9y",
                    "permalink": "/r/KoboldAI/comments/1fztl9y/are_there_gguf_models_like_open_ai_model_gpt_35/lr8v2hx/"
                },
                {
                    "id": "lr716rf",
                    "author": "RealBiggly",
                    "body": "It depends on the tunes. Even the plain Instruct will agree to vanilla ERP, the 2 tunes I posted above don't seem to hesitate at anything.",
                    "score": 0,
                    "upvotes": 0,
                    "downvotes": 0,
                    "created_utc": 1728528164.0,
                    "parent_id": "t1_lr5gqsa",
                    "link_id": "t3_1fztl9y",
                    "permalink": "/r/KoboldAI/comments/1fztl9y/are_there_gguf_models_like_open_ai_model_gpt_35/lr716rf/"
                },
                {
                    "id": "lr41r83",
                    "author": "oxzlz",
                    "body": "Thanks, could you mind sending me the links to those models?",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1728491018.0,
                    "parent_id": "t1_lr3uj9b",
                    "link_id": "t3_1fztl9y",
                    "permalink": "/r/KoboldAI/comments/1fztl9y/are_there_gguf_models_like_open_ai_model_gpt_35/lr41r83/"
                },
                {
                    "id": "lr6fiaz",
                    "author": "kiselsa",
                    "body": "Probably Stheno 3.2 is still best in 8b range, even though it's based on llama.\n\nOr some Mistral Nemo finetunes if they fit. Mini magnum, lyra, etc. the drummer also have good Nemo finetune, I forgot the name.",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1728519748.0,
                    "parent_id": "t1_lr69sw2",
                    "link_id": "t3_1fztl9y",
                    "permalink": "/r/KoboldAI/comments/1fztl9y/are_there_gguf_models_like_open_ai_model_gpt_35/lr6fiaz/"
                },
                {
                    "id": "lr8gk0f",
                    "author": "kiselsa",
                    "body": "You don't need to use q8, it's overkill. Better pick q4km or Q5km so it will fully fit in your memory. You can also enable FlashAttention.",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1728558754.0,
                    "parent_id": "t1_lr787e4",
                    "link_id": "t3_1fztl9y",
                    "permalink": "/r/KoboldAI/comments/1fztl9y/are_there_gguf_models_like_open_ai_model_gpt_35/lr8gk0f/"
                },
                {
                    "id": "lr8vmf1",
                    "author": "kiselsa",
                    "body": "Yes, they don't refuse, but their dataset was filtered from 18+ stuff, so prose they generate is generally boring, even with fine-tunes. Stheno 3.2 8b is good though.",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1728565660.0,
                    "parent_id": "t1_lr716rf",
                    "link_id": "t3_1fztl9y",
                    "permalink": "/r/KoboldAI/comments/1fztl9y/are_there_gguf_models_like_open_ai_model_gpt_35/lr8vmf1/"
                },
                {
                    "id": "lr70w4p",
                    "author": "RealBiggly",
                    "body": "[https://huggingface.co/mradermacher/Llama-3.1-70B-Instruct-Lorablated-Creative-Writer-i1-GGUF](https://huggingface.co/mradermacher/Llama-3.1-70B-Instruct-Lorablated-Creative-Writer-i1-GGUF)\n\nYou can try the version without \"Creative-Writer\" on the end too.\n\nLlama-3.1-70B-ArliAI-RPMax-v1.1.Q3\\_K\\_L.gguf is also great. Just search on Hugging Face. It will often say \"not found\" until you hit enter, then it finds it, like this: [https://huggingface.co/mradermacher/Llama-3.1-70B-ArliAI-RPMax-v1.1-GGUF](https://huggingface.co/mradermacher/Llama-3.1-70B-ArliAI-RPMax-v1.1-GGUF)",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1728528042.0,
                    "parent_id": "t1_lr41r83",
                    "link_id": "t3_1fztl9y",
                    "permalink": "/r/KoboldAI/comments/1fztl9y/are_there_gguf_models_like_open_ai_model_gpt_35/lr70w4p/"
                }
            ]
        },
        {
            "id": "1fz6fdq",
            "title": "Whats the Best and newer more extreme NSFW model that i can use with 12gb of vram on an rtx 3060 and 32gb of ram (Looking mostly for gguf models for the shared ram stuff)",
            "author": "Daniel360n",
            "score": 21,
            "upvotes": 21,
            "downvotes": 0,
            "num_comments": 7,
            "created_utc": 1728410929.0,
            "url": "https://www.reddit.com/r/KoboldAI/comments/1fz6fdq/whats_the_best_and_newer_more_extreme_nsfw_model/",
            "permalink": "/r/KoboldAI/comments/1fz6fdq/whats_the_best_and_newer_more_extreme_nsfw_model/",
            "selftext": "Heya im looking for the best gguf model that can run on my local set up.\n\nGraphics card: Nvidia RTX 3060 (12 GB VRAM)  \nRAM: 32GB  \nProcesser: Intel(R) Core(TM) i7-10700K CPU @ 3.80GHz  \nOS: Windows 10 64bit\n\nThanks in advance!",
            "comments": [
                {
                    "id": "lqzqr6h",
                    "author": "schlammsuhler",
                    "body": "Nemo models at Q4 will fit nicely with 8k context. Check out Lyra, Celeste, Magnum or the merge Nemomix: https://huggingface.co/MarinaraSpaghetti/NemoMix-Unleashed-12B\n\nUse sillytavern with DRY or this model will degrade.",
                    "score": 13,
                    "upvotes": 13,
                    "downvotes": 0,
                    "created_utc": 1728418467.0,
                    "parent_id": "t3_1fz6fdq",
                    "link_id": "t3_1fz6fdq",
                    "permalink": "/r/KoboldAI/comments/1fz6fdq/whats_the_best_and_newer_more_extreme_nsfw_model/lqzqr6h/"
                },
                {
                    "id": "lr04hye",
                    "author": "cr0wburn",
                    "body": "[ArliAI/Mistral-Nemo-12B-ArliAI-RPMax-v1.1-GGUF Â· Hugging Face](https://huggingface.co/ArliAI/Mistral-Nemo-12B-ArliAI-RPMax-v1.1-GGUF)\n\n  \nThis works very well, and is not censored at all.   \nKoboldcpp also needs DRY and XTC for this model to work well though.",
                    "score": 9,
                    "upvotes": 9,
                    "downvotes": 0,
                    "created_utc": 1728423180.0,
                    "parent_id": "t3_1fz6fdq",
                    "link_id": "t3_1fz6fdq",
                    "permalink": "/r/KoboldAI/comments/1fz6fdq/whats_the_best_and_newer_more_extreme_nsfw_model/lr04hye/"
                },
                {
                    "id": "lr1gvi6",
                    "author": "exceptional--",
                    "body": "I don't know what is good for your specs, mine are somewhat different, I use primarily 11B-20B models.....\n\nI started using this \\~yesterday swapping from default magnum and dark planet,\n\n[https://huggingface.co/inflatebot/MN-12B-Mag-Mell-R1](https://huggingface.co/inflatebot/MN-12B-Mag-Mell-R1)\n\nRecommending as a \"NSFW\" model as yes, alongside the benefits listed, it also does good nsfw.\n\n\n\nI've tried 90% of the models people talk about, nemomix-unleashed, lot of 22Bs, some of the first models, etc etc\n\nThis model has seemed to be more knowledgeable about things, capable of deeper philosophical discussion, less stupidity bias, less prone to turn into a redneck, more actually stable\n\nIt has seemed to come with the downside of being moderately more stubborn to listen, but it at least seems to listens a lot than others I've used when it does, it also seems to take more into account what's happening when responding.\n\nI may be rambling on about nothing considering I've only used it for a day or two however.",
                    "score": 4,
                    "upvotes": 4,
                    "downvotes": 0,
                    "created_utc": 1728443893.0,
                    "parent_id": "t3_1fz6fdq",
                    "link_id": "t3_1fz6fdq",
                    "permalink": "/r/KoboldAI/comments/1fz6fdq/whats_the_best_and_newer_more_extreme_nsfw_model/lr1gvi6/"
                },
                {
                    "id": "lr7w8wr",
                    "author": "Itikar",
                    "body": "With the same specs as user, I can also run q4k_m at 16k context with only minor slowdowns, if I do not have many other programs running.",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1728545044.0,
                    "parent_id": "t1_lqzqr6h",
                    "link_id": "t3_1fz6fdq",
                    "permalink": "/r/KoboldAI/comments/1fz6fdq/whats_the_best_and_newer_more_extreme_nsfw_model/lr7w8wr/"
                },
                {
                    "id": "lrb2x6x",
                    "author": "None",
                    "body": "[deleted]",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1728591978.0,
                    "parent_id": "t1_lr04hye",
                    "link_id": "t3_1fz6fdq",
                    "permalink": "/r/KoboldAI/comments/1fz6fdq/whats_the_best_and_newer_more_extreme_nsfw_model/lrb2x6x/"
                },
                {
                    "id": "lr2jzq4",
                    "author": "Sensitive_Bedroom789",
                    "body": "What you think about Utopia13B",
                    "score": 2,
                    "upvotes": 2,
                    "downvotes": 0,
                    "created_utc": 1728469695.0,
                    "parent_id": "t1_lr1gvi6",
                    "link_id": "t3_1fz6fdq",
                    "permalink": "/r/KoboldAI/comments/1fz6fdq/whats_the_best_and_newer_more_extreme_nsfw_model/lr2jzq4/"
                },
                {
                    "id": "lrbf40l",
                    "author": "cr0wburn",
                    "body": "Sure. You go to settings and sampler. There you set xtc above zero and dry above zero. I'm not near my computer but I can report the exact values later",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1728595956.0,
                    "parent_id": "t1_lrb2x6x",
                    "link_id": "t3_1fz6fdq",
                    "permalink": "/r/KoboldAI/comments/1fz6fdq/whats_the_best_and_newer_more_extreme_nsfw_model/lrbf40l/"
                },
                {
                    "id": "lr4njxr",
                    "author": "exceptional--",
                    "body": "LOL wasn't interested as I wasn't too impressed with nethena until i saw 120 days of lora as part of it, truly a work of art\n\nI used it for like a couple minutes, also tried nethena before, it seems a lot more erotic but a lot less capable in other aspects and less attentive and more hallucinatory, maybe I'm not using it properly, who knows.\n\nalso seems to have somewhat better writing despite the writing being less intelligent\n\n6/10, mag-mell better.",
                    "score": 3,
                    "upvotes": 3,
                    "downvotes": 0,
                    "created_utc": 1728497983.0,
                    "parent_id": "t1_lr2jzq4",
                    "link_id": "t3_1fz6fdq",
                    "permalink": "/r/KoboldAI/comments/1fz6fdq/whats_the_best_and_newer_more_extreme_nsfw_model/lr4njxr/"
                }
            ]
        },
        {
            "id": "1fz4lbj",
            "title": "Optimal Settings For i7 8700 & 3060 Ti",
            "author": "Aardvark-Fearless",
            "score": 3,
            "upvotes": 3,
            "downvotes": 0,
            "num_comments": 2,
            "created_utc": 1728406394.0,
            "url": "https://www.reddit.com/r/KoboldAI/comments/1fz4lbj/optimal_settings_for_i7_8700_3060_ti/",
            "permalink": "/r/KoboldAI/comments/1fz4lbj/optimal_settings_for_i7_8700_3060_ti/",
            "selftext": "I have a RTX 3060 Ti w/16gb in memory, an i7-8700 w/ a base speed of 3.20GHz with 6 cores, and lastly 16gb of ram at 2133MHz, and I wanna run NeuralDaredevil-8B-abliterated.Q4\\_K\\_M with SillyTavern via Kobold.\n\nWhat would most likely be the optimal settings for my setup?",
            "comments": [
                {
                    "id": "lqzu8wh",
                    "author": "schlammsuhler",
                    "body": "Most optimal for what? Your pc will be able to run it just fine. You can probably crank up to Q6 and 16k context.",
                    "score": 2,
                    "upvotes": 2,
                    "downvotes": 0,
                    "created_utc": 1728419635.0,
                    "parent_id": "t3_1fz4lbj",
                    "link_id": "t3_1fz4lbj",
                    "permalink": "/r/KoboldAI/comments/1fz4lbj/optimal_settings_for_i7_8700_3060_ti/lqzu8wh/"
                },
                {
                    "id": "lr1wh2n",
                    "author": "Aardvark-Fearless",
                    "body": "sorry im new to LLM's i lwky am learning as I go, ty ill try Q6 NemoMix Unleashed",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1728452869.0,
                    "parent_id": "t1_lqzu8wh",
                    "link_id": "t3_1fz4lbj",
                    "permalink": "/r/KoboldAI/comments/1fz4lbj/optimal_settings_for_i7_8700_3060_ti/lr1wh2n/"
                }
            ]
        },
        {
            "id": "1fykx4n",
            "title": "Horde Worker: Who are these ~1800 tokens context people?",
            "author": "UpperParamedicDude",
            "score": 8,
            "upvotes": 8,
            "downvotes": 0,
            "num_comments": 9,
            "created_utc": 1728341042.0,
            "url": "https://www.reddit.com/r/KoboldAI/comments/1fykx4n/horde_worker_who_are_these_1800_tokens_context/",
            "permalink": "/r/KoboldAI/comments/1fykx4n/horde_worker_who_are_these_1800_tokens_context/",
            "selftext": "Always was curious but just now decided to ask, back then, when L3 just came out, i used to leave my PC with koboldcpp launched as a horde worker, it wasn't hard for me and was something like \"why not?\". Set context to 12k (for L3 models) and leave.\n\nKoboldCPP shows you when someone sends you a request and the amount of context limit they have, MOST of the requests had 1600\\~1800 context limit, what? Why would someone limit themselves like that? Who are these people? They show up right at the moment you run koboldcpp with horde worker enabled. Are they bots who collect synthetic data for further training? If so, is there a way i can somehow weed them out? I'd like to help people who want to try different models but have no PC to try any of them, but i don't really want to do it if most of them are bots",
            "comments": [
                {
                    "id": "lquv3mp",
                    "author": "henk717",
                    "body": "Its people who don't change kobolds defaults.\n\nWhy do we have such low defaults for Horde? To help with the speed of the platform. Anyones free to increase it but since so many don't seem to mind the default it helps unneccesary slowdown. Those who do mind only have to increase the slider.",
                    "score": 14,
                    "upvotes": 14,
                    "downvotes": 0,
                    "created_utc": 1728343147.0,
                    "parent_id": "t3_1fykx4n",
                    "link_id": "t3_1fykx4n",
                    "permalink": "/r/KoboldAI/comments/1fykx4n/horde_worker_who_are_these_1800_tokens_context/lquv3mp/"
                },
                {
                    "id": "lqur6zh",
                    "author": "Slight-Living-8098",
                    "body": "You don't need a huge context to do simple chatting and roleplay. Most are probably hitting it up with Silly tavern or something",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1728341721.0,
                    "parent_id": "t3_1fykx4n",
                    "link_id": "t3_1fykx4n",
                    "permalink": "/r/KoboldAI/comments/1fykx4n/horde_worker_who_are_these_1800_tokens_context/lqur6zh/"
                },
                {
                    "id": "lqv13g3",
                    "author": "ten0re",
                    "body": "Thatâ€™s likely due to vram constants, the model barely fits into vram leaving not much space for the context.",
                    "score": -3,
                    "upvotes": -3,
                    "downvotes": 0,
                    "created_utc": 1728345359.0,
                    "parent_id": "t3_1fykx4n",
                    "link_id": "t3_1fykx4n",
                    "permalink": "/r/KoboldAI/comments/1fykx4n/horde_worker_who_are_these_1800_tokens_context/lqv13g3/"
                },
                {
                    "id": "lqwmlf0",
                    "author": "Billy_Whisky",
                    "body": "Context is what makes to good thought..",
                    "score": 4,
                    "upvotes": 4,
                    "downvotes": 0,
                    "created_utc": 1728371946.0,
                    "parent_id": "t1_lqur6zh",
                    "link_id": "t3_1fykx4n",
                    "permalink": "/r/KoboldAI/comments/1fykx4n/horde_worker_who_are_these_1800_tokens_context/lqwmlf0/"
                },
                {
                    "id": "lqx2oxm",
                    "author": "BangkokPadang",
                    "body": "Users of horde donâ€™t have vram constraints, the hosts do, and this host sets his models at 12k.",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1728383284.0,
                    "parent_id": "t1_lqv13g3",
                    "link_id": "t3_1fykx4n",
                    "permalink": "/r/KoboldAI/comments/1fykx4n/horde_worker_who_are_these_1800_tokens_context/lqx2oxm/"
                },
                {
                    "id": "lqxhmqi",
                    "author": "Slight-Living-8098",
                    "body": "Look, you don't need tons of context to have a simple back and forth. AI Dungeon didn't even have a 2048 token size. We've been doing this sort of thing for years with way dumber models successfully.\n\nAnd if I just want to generate a prompt for stable diffusion... It's usually under 250 characters, not tokens, characters.\n\nThere are tons of legitimate uses daily for lw context size",
                    "score": 3,
                    "upvotes": 3,
                    "downvotes": 0,
                    "created_utc": 1728390846.0,
                    "parent_id": "t1_lqwmlf0",
                    "link_id": "t3_1fykx4n",
                    "permalink": "/r/KoboldAI/comments/1fykx4n/horde_worker_who_are_these_1800_tokens_context/lqxhmqi/"
                },
                {
                    "id": "lqx80mm",
                    "author": "ten0re",
                    "body": "Yeah thatâ€™s what I meant",
                    "score": -1,
                    "upvotes": -1,
                    "downvotes": 0,
                    "created_utc": 1728386315.0,
                    "parent_id": "t1_lqx2oxm",
                    "link_id": "t3_1fykx4n",
                    "permalink": "/r/KoboldAI/comments/1fykx4n/horde_worker_who_are_these_1800_tokens_context/lqx80mm/"
                },
                {
                    "id": "lr58vcw",
                    "author": "_Erilaz",
                    "body": "Idk why do people downvote this, but that's true. While long context is advantageous, and I do run 32k model locally, that doesn't mean a short window is useless either. Sure, some chats will keep 10 messages in the memory tops. But most simple characters don't need more than 700-1000 tokens to be described, you can use author's note as long term memory, and there's actually a point where too long context might completely overwhelm the model with contradictory patterns and sloppy bloat, causing the output quality deterioration. It takes a very good model and some skill to handhold a long conversation with a bot seamlessly. You can hardly expect that from an average Joe using a mere L3-8B tune there",
                    "score": 3,
                    "upvotes": 3,
                    "downvotes": 0,
                    "created_utc": 1728504763.0,
                    "parent_id": "t1_lqxhmqi",
                    "link_id": "t3_1fykx4n",
                    "permalink": "/r/KoboldAI/comments/1fykx4n/horde_worker_who_are_these_1800_tokens_context/lr58vcw/"
                },
                {
                    "id": "lr58y0i",
                    "author": "_Erilaz",
                    "body": "it's the opposite though...",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1728504786.0,
                    "parent_id": "t1_lqx80mm",
                    "link_id": "t3_1fykx4n",
                    "permalink": "/r/KoboldAI/comments/1fykx4n/horde_worker_who_are_these_1800_tokens_context/lr58y0i/"
                }
            ]
        },
        {
            "id": "1fyguw8",
            "title": "KoboldCPP with SillyTavern",
            "author": "CarefulMaintenance32",
            "score": 6,
            "upvotes": 6,
            "downvotes": 0,
            "num_comments": 10,
            "created_utc": 1728330749.0,
            "url": "https://www.reddit.com/r/KoboldAI/comments/1fyguw8/koboldcpp_with_sillytavern/",
            "permalink": "/r/KoboldAI/comments/1fyguw8/koboldcpp_with_sillytavern/",
            "selftext": "I've had this problem for a long time when connecting KoboldCPP to SillyTavern. The models feel the same, don't respond to samplers, use the same words, and basically show little creativity. Now, I'm using the 12B models (4\\_q\\_s) and honestly, I don't see much difference between them. And by â€œdoesn't respond to samplersâ€ I mean generating roughly the same (very similar text, but not completely the identical) text at temp 1 and temp 5. The same goes for DRY, XTC, and the like. I've tried many different formats, instructions, settings and promts. All to nothing.\n\nThe situation changes if you use KoboldCPP through KoboldLITE. Internally, the model responses are different, responsive to samplers and quite creative. And this is on the same card, with the same settings and prompt! (Hardware: Nvidia 1060 5 GB, Windows 10).\n\nThe problem is similar when running the model through oobabooga and LMStudio, so the cause of the problems lies either in SillyTavern itself or the way you connect to it. I found someone who encountered the same problem on Windows, but on macOS he is doing fine. I've posted more than once on the SillyTavern subreddit, but I've only found one person with the same problem. Would it be possible that someone here has encountered this?\n\n  \n- Update: I've been playing around with KoboldLite some more and realized that it looks like I'm actually running into the same thing in it that I'm running into in SillyTavern. Constant repetition, the same phrases, and little distinction between answers. Perhaps this is just a normal 12B problem or I have a bad System Promt.",
            "comments": [
                {
                    "id": "lqul6xe",
                    "author": "Nonsensese",
                    "body": "Make sure you're using the Text Completion endpoint in ST instead of the Chat Completion endpoint, and that you're using the Kobold API URL instead of the OpenAPI-compatible one (it should end in `5001/` instead of `5001/v1/`.\n\nAlso, try changing the sampler order (left pane) to put Temperature first -- ST and KoboldCpp defaults to applying Temperature last.",
                    "score": 7,
                    "upvotes": 7,
                    "downvotes": 0,
                    "created_utc": 1728339584.0,
                    "parent_id": "t3_1fyguw8",
                    "link_id": "t3_1fyguw8",
                    "permalink": "/r/KoboldAI/comments/1fyguw8/koboldcpp_with_sillytavern/lqul6xe/"
                },
                {
                    "id": "lqu2umu",
                    "author": "henk717",
                    "body": "Nothing wrong with enjoying them in KoboldAI Lite :D",
                    "score": 4,
                    "upvotes": 4,
                    "downvotes": 0,
                    "created_utc": 1728333521.0,
                    "parent_id": "t3_1fyguw8",
                    "link_id": "t3_1fyguw8",
                    "permalink": "/r/KoboldAI/comments/1fyguw8/koboldcpp_with_sillytavern/lqu2umu/"
                },
                {
                    "id": "lqum96c",
                    "author": "CarefulMaintenance32",
                    "body": "Wow, that last advice worked. But... Shouldn't the temperature be the last one? After all, in KoboldLite it is last by default and works fine?",
                    "score": 3,
                    "upvotes": 3,
                    "downvotes": 0,
                    "created_utc": 1728339960.0,
                    "parent_id": "t1_lqul6xe",
                    "link_id": "t3_1fyguw8",
                    "permalink": "/r/KoboldAI/comments/1fyguw8/koboldcpp_with_sillytavern/lqum96c/"
                },
                {
                    "id": "lqu3eqd",
                    "author": "CarefulMaintenance32",
                    "body": "Maybe, but KoboldLite has fewer features. It's really sad that SillyTavern just refuses to work properly.",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1728333698.0,
                    "parent_id": "t1_lqu2umu",
                    "link_id": "t3_1fyguw8",
                    "permalink": "/r/KoboldAI/comments/1fyguw8/koboldcpp_with_sillytavern/lqu3eqd/"
                },
                {
                    "id": "lqupg3o",
                    "author": "Nonsensese",
                    "body": "Depends on what you're trying to achieve, but yeah, putting Temperature before the truncation, etc. samplers should make it have more of a noticeable effect.",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1728341099.0,
                    "parent_id": "t1_lqum96c",
                    "link_id": "t3_1fyguw8",
                    "permalink": "/r/KoboldAI/comments/1fyguw8/koboldcpp_with_sillytavern/lqupg3o/"
                },
                {
                    "id": "lqwu3wr",
                    "author": "HadesThrowaway",
                    "body": "What's the feature that lite lacks that you'd wish for the most?",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1728377352.0,
                    "parent_id": "t1_lqu3eqd",
                    "link_id": "t3_1fyguw8",
                    "permalink": "/r/KoboldAI/comments/1fyguw8/koboldcpp_with_sillytavern/lqwu3wr/"
                },
                {
                    "id": "lqwvcns",
                    "author": "CarefulMaintenance32",
                    "body": "Extensions, (I'm not sure if KoboldLite has group chats), good ecosystem that stores all your characters with tags and saved chats, {{user}} personas, built-in text translation, STscripts\n\nDon't get me wrong, KoboldLite is perfect for quickly running models and playing with them, but I'm just too used to using all the features SillyTavern provides",
                    "score": 2,
                    "upvotes": 2,
                    "downvotes": 0,
                    "created_utc": 1728378252.0,
                    "parent_id": "t1_lqwu3wr",
                    "link_id": "t3_1fyguw8",
                    "permalink": "/r/KoboldAI/comments/1fyguw8/koboldcpp_with_sillytavern/lqwvcns/"
                },
                {
                    "id": "lr8d891",
                    "author": "HadesThrowaway",
                    "body": "It does have group chats. It also has the placeholder tags you mentioned",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1728556863.0,
                    "parent_id": "t1_lqwvcns",
                    "link_id": "t3_1fyguw8",
                    "permalink": "/r/KoboldAI/comments/1fyguw8/koboldcpp_with_sillytavern/lr8d891/"
                },
                {
                    "id": "lr8fwhy",
                    "author": "CarefulMaintenance32",
                    "body": "In fact, I've been playing around with KoboldLite some more and realized that it looks like I'm actually running into the same thing in it that I'm running into in SillyTavern. Constant repetition, the same phrases, and little distinction between answers. Perhaps this is just a normal 12B problem or I have a bad System Promt.",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1728558397.0,
                    "parent_id": "t1_lr8d891",
                    "link_id": "t3_1fyguw8",
                    "permalink": "/r/KoboldAI/comments/1fyguw8/koboldcpp_with_sillytavern/lr8fwhy/"
                },
                {
                    "id": "lrem7p0",
                    "author": "HadesThrowaway",
                    "body": "Try changing to a more creative sampler.",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1728651598.0,
                    "parent_id": "t1_lr8fwhy",
                    "link_id": "t3_1fyguw8",
                    "permalink": "/r/KoboldAI/comments/1fyguw8/koboldcpp_with_sillytavern/lrem7p0/"
                }
            ]
        },
        {
            "id": "1fxiiwu",
            "title": "GPU for prompt processing only?",
            "author": "Substantial-Ebb-584",
            "score": 7,
            "upvotes": 7,
            "downvotes": 0,
            "num_comments": 12,
            "created_utc": 1728227076.0,
            "url": "https://www.reddit.com/r/KoboldAI/comments/1fxiiwu/gpu_for_prompt_processing_only/",
            "permalink": "/r/KoboldAI/comments/1fxiiwu/gpu_for_prompt_processing_only/",
            "selftext": "In Kobold CPP Is there a way to Process prompt (blas) on GPU, and generate on CPU only?\n\nI'm asking since Prompt processing is waaay faster on GPU. But when using bigger models from RAM my generating speed gets squandered by PCI-E speed since the GPU starts to read from RAM through PCI-E. \n\nWhen generating through CPU only my generation speed is 4x faster. Since RAM throughput is much better then PCI-E. Although prompt processing takes ages.",
            "comments": [
                {
                    "id": "lqmog61",
                    "author": "Wise-Paramedic-4536",
                    "body": "Just offload no layer.",
                    "score": 9,
                    "upvotes": 9,
                    "downvotes": 0,
                    "created_utc": 1728228997.0,
                    "parent_id": "t3_1fxiiwu",
                    "link_id": "t3_1fxiiwu",
                    "permalink": "/r/KoboldAI/comments/1fxiiwu/gpu_for_prompt_processing_only/lqmog61/"
                },
                {
                    "id": "lqn1u0q",
                    "author": "kiselsa",
                    "body": "Select Cublas/clblast etc. and offload 0 layers.",
                    "score": 5,
                    "upvotes": 5,
                    "downvotes": 0,
                    "created_utc": 1728233362.0,
                    "parent_id": "t3_1fxiiwu",
                    "link_id": "t3_1fxiiwu",
                    "permalink": "/r/KoboldAI/comments/1fxiiwu/gpu_for_prompt_processing_only/lqn1u0q/"
                },
                {
                    "id": "lqqzsb4",
                    "author": "lacerating_aura",
                    "body": "Set BLAS batch size to 1024 or even 2048 if you want and in layer offload set to 0.",
                    "score": 2,
                    "upvotes": 2,
                    "downvotes": 0,
                    "created_utc": 1728290722.0,
                    "parent_id": "t3_1fxiiwu",
                    "link_id": "t3_1fxiiwu",
                    "permalink": "/r/KoboldAI/comments/1fxiiwu/gpu_for_prompt_processing_only/lqqzsb4/"
                },
                {
                    "id": "lqrg0xv",
                    "author": "Aphid_red",
                    "body": "What if the KV cache is bigger than the GPU (but could fit within 2)?",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1728301004.0,
                    "parent_id": "t3_1fxiiwu",
                    "link_id": "t3_1fxiiwu",
                    "permalink": "/r/KoboldAI/comments/1fxiiwu/gpu_for_prompt_processing_only/lqrg0xv/"
                },
                {
                    "id": "lqn69ly",
                    "author": "Substantial-Ebb-584",
                    "body": "Thank you, I'll try this",
                    "score": 3,
                    "upvotes": 3,
                    "downvotes": 0,
                    "created_utc": 1728234755.0,
                    "parent_id": "t1_lqmog61",
                    "link_id": "t3_1fxiiwu",
                    "permalink": "/r/KoboldAI/comments/1fxiiwu/gpu_for_prompt_processing_only/lqn69ly/"
                },
                {
                    "id": "lqp7922",
                    "author": "ECrispy",
                    "body": "doesnt this mean gpu is never used? \nwhat is recommended use for this - e.g I have 24GB memory but a 8GB vram gpu - what are best settings for me - right now I'm using cublast.",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1728258648.0,
                    "parent_id": "t1_lqn1u0q",
                    "link_id": "t3_1fxiiwu",
                    "permalink": "/r/KoboldAI/comments/1fxiiwu/gpu_for_prompt_processing_only/lqp7922/"
                },
                {
                    "id": "lqpa7ly",
                    "author": "wh33t",
                    "body": "8GB of layers on the GPU, the rest on CPU. How many layers that is depends on model and context.",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1728259800.0,
                    "parent_id": "t1_lqp7922",
                    "link_id": "t3_1fxiiwu",
                    "permalink": "/r/KoboldAI/comments/1fxiiwu/gpu_for_prompt_processing_only/lqpa7ly/"
                },
                {
                    "id": "lqr7g8v",
                    "author": "kiselsa",
                    "body": "This means that gpu will be used only for prompt processing.",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1728296090.0,
                    "parent_id": "t1_lqp7922",
                    "link_id": "t3_1fxiiwu",
                    "permalink": "/r/KoboldAI/comments/1fxiiwu/gpu_for_prompt_processing_only/lqr7g8v/"
                },
                {
                    "id": "lqpcl7c",
                    "author": "ECrispy",
                    "body": "Any tools to calculate that? I've seen an llm calculator but it only tells what's the minimum gpu needed for a given model, quant etc",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1728260701.0,
                    "parent_id": "t1_lqpa7ly",
                    "link_id": "t3_1fxiiwu",
                    "permalink": "/r/KoboldAI/comments/1fxiiwu/gpu_for_prompt_processing_only/lqpcl7c/"
                },
                {
                    "id": "lqpth8z",
                    "author": "wh33t",
                    "body": "None that I've come across. There are few write ups on some of the models on hugging face that have this information, but it's far from common. \n\nI just do trial and error to find out max layers / context I can use.\n\nI wish there was a better way.",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1728267346.0,
                    "parent_id": "t1_lqpcl7c",
                    "link_id": "t3_1fxiiwu",
                    "permalink": "/r/KoboldAI/comments/1fxiiwu/gpu_for_prompt_processing_only/lqpth8z/"
                },
                {
                    "id": "lqqldsx",
                    "author": "ECrispy",
                    "body": "I feel like this is exactly what the code in these apps should do, since it can determine how much memory loading a layer etc takes, and dynamically adjust.",
                    "score": 2,
                    "upvotes": 2,
                    "downvotes": 0,
                    "created_utc": 1728280756.0,
                    "parent_id": "t1_lqpth8z",
                    "link_id": "t3_1fxiiwu",
                    "permalink": "/r/KoboldAI/comments/1fxiiwu/gpu_for_prompt_processing_only/lqqldsx/"
                },
                {
                    "id": "lqslbpy",
                    "author": "wh33t",
                    "body": "Only just recently (like 2 versions ago) did Kcpp even tell you how many layers were in the model before you tried to load it. The tools are lacking, but ... they're free and open source, so can't complain. If one felt inspired and was able to do so, they could make the edits and changes themselves and open a pull request.",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1728316587.0,
                    "parent_id": "t1_lqqldsx",
                    "link_id": "t3_1fxiiwu",
                    "permalink": "/r/KoboldAI/comments/1fxiiwu/gpu_for_prompt_processing_only/lqslbpy/"
                }
            ]
        },
        {
            "id": "1fxdhr9",
            "title": "Which AI model would you recommend for interactive stories like AI Dungeon ? (AMD Koboldcpp)",
            "author": "ShamelesslyHornKnee",
            "score": 20,
            "upvotes": 20,
            "downvotes": 0,
            "num_comments": 9,
            "created_utc": 1728210419.0,
            "url": "https://www.reddit.com/r/KoboldAI/comments/1fxdhr9/which_ai_model_would_you_recommend_for/",
            "permalink": "/r/KoboldAI/comments/1fxdhr9/which_ai_model_would_you_recommend_for/",
            "selftext": "I'm using the ROCm version of koboldcpp to host locally, I have a AMD Ryzen 5 7600 6-Core, 32 GB of RAM and a Radeon RX 7800 XT\n\n  \nI'm looking for a model that will allow me to play/write interactive stories similar to AI Dungeon, where I control a character, experience the world, live adventures, etc with the model being capable of remembering the plot, the setting, the different characters, and write in a pleasant, fan-fic style. The stories would be SFW and NSFW so I need something completely uncensored. I tried Mistral7b Erebus Q5 v3 but it couldn't understand the roleplay structure with me playing the protagonist, and just generally got very consfused and had bad writing, with nonsensical events or phrases.\n\nThanks for any recommendation  !",
            "comments": [
                {
                    "id": "lqlspul",
                    "author": "henk717",
                    "body": "Tiefighter which is why I gave that to AI Dungeon to, it has data specific to it. Erebus meanwhile is only a co-writing model it can't do this at all.\n\n[https://huggingface.co/KoboldAI/LLaMA2-13B-Tiefighter-GGUF/tree/main](https://huggingface.co/KoboldAI/LLaMA2-13B-Tiefighter-GGUF/tree/main)",
                    "score": 9,
                    "upvotes": 9,
                    "downvotes": 0,
                    "created_utc": 1728216917.0,
                    "parent_id": "t3_1fxdhr9",
                    "link_id": "t3_1fxdhr9",
                    "permalink": "/r/KoboldAI/comments/1fxdhr9/which_ai_model_would_you_recommend_for/lqlspul/"
                },
                {
                    "id": "lqlj6q0",
                    "author": "Icy_Goat7172",
                    "body": "https://huggingface.co/Lewdiculous/L3-8B-Stheno-v3.2-GGUF-IQ-Imatrix",
                    "score": 12,
                    "upvotes": 12,
                    "downvotes": 0,
                    "created_utc": 1728211457.0,
                    "parent_id": "t3_1fxdhr9",
                    "link_id": "t3_1fxdhr9",
                    "permalink": "/r/KoboldAI/comments/1fxdhr9/which_ai_model_would_you_recommend_for/lqlj6q0/"
                },
                {
                    "id": "lqm9mth",
                    "author": "None",
                    "body": "[removed]",
                    "score": 8,
                    "upvotes": 8,
                    "downvotes": 0,
                    "created_utc": 1728223994.0,
                    "parent_id": "t3_1fxdhr9",
                    "link_id": "t3_1fxdhr9",
                    "permalink": "/r/KoboldAI/comments/1fxdhr9/which_ai_model_would_you_recommend_for/lqm9mth/"
                },
                {
                    "id": "lqmodck",
                    "author": "International-Try467",
                    "body": "Psycet",
                    "score": 2,
                    "upvotes": 2,
                    "downvotes": 0,
                    "created_utc": 1728228971.0,
                    "parent_id": "t3_1fxdhr9",
                    "link_id": "t3_1fxdhr9",
                    "permalink": "/r/KoboldAI/comments/1fxdhr9/which_ai_model_would_you_recommend_for/lqmodck/"
                },
                {
                    "id": "lqnwuui",
                    "author": "ItchyBitchy7258",
                    "body": "Psymedrp",
                    "score": 2,
                    "upvotes": 2,
                    "downvotes": 0,
                    "created_utc": 1728243094.0,
                    "parent_id": "t3_1fxdhr9",
                    "link_id": "t3_1fxdhr9",
                    "permalink": "/r/KoboldAI/comments/1fxdhr9/which_ai_model_would_you_recommend_for/lqnwuui/"
                },
                {
                    "id": "lqob2bd",
                    "author": "Zenobody",
                    "body": "Currently I'm using Mistral Nemo Instruct (e.g. Q6\\_K with 24K context fully in VRAM) and Mistral Small (Q4\\_K\\_S with 16K context and some layers offloaded) with my 7800 XT.",
                    "score": 2,
                    "upvotes": 2,
                    "downvotes": 0,
                    "created_utc": 1728247520.0,
                    "parent_id": "t3_1fxdhr9",
                    "link_id": "t3_1fxdhr9",
                    "permalink": "/r/KoboldAI/comments/1fxdhr9/which_ai_model_would_you_recommend_for/lqob2bd/"
                },
                {
                    "id": "lqs9dcj",
                    "author": "Dos-Commas",
                    "body": "daybreak-kunoichi-2dpo is about as close as I could find that comes close to Tiefighter/Psyfighter. Better than some of the newer Gemma or whatever-maid models.",
                    "score": 2,
                    "upvotes": 2,
                    "downvotes": 0,
                    "created_utc": 1728312688.0,
                    "parent_id": "t1_lqlspul",
                    "link_id": "t3_1fxdhr9",
                    "permalink": "/r/KoboldAI/comments/1fxdhr9/which_ai_model_would_you_recommend_for/lqs9dcj/"
                },
                {
                    "id": "lqu2zpj",
                    "author": "KoboldAI-ModTeam",
                    "body": "This post was not related to KoboldAI",
                    "score": 2,
                    "upvotes": 2,
                    "downvotes": 0,
                    "created_utc": 1728333565.0,
                    "parent_id": "t1_lqm9mth",
                    "link_id": "t3_1fxdhr9",
                    "permalink": "/r/KoboldAI/comments/1fxdhr9/which_ai_model_would_you_recommend_for/lqu2zpj/"
                }
            ]
        },
        {
            "id": "1fwcffd",
            "title": "Import World Info",
            "author": "Gamerking1337",
            "score": 2,
            "upvotes": 2,
            "downvotes": 0,
            "num_comments": 2,
            "created_utc": 1728083098.0,
            "url": "https://www.reddit.com/r/KoboldAI/comments/1fwcffd/import_world_info/",
            "permalink": "/r/KoboldAI/comments/1fwcffd/import_world_info/",
            "selftext": "Hello,\n\nI was curious if there was a way to import World Info from websites like Characterhub (which they have under \"Lorebook\"). Now some of the Characters on Characterhub come with lorebooks and those import into Kobold's world info just fine, but, I can't find a way to import just the lorebook into Kobold. Is there any way to do this?",
            "comments": [
                {
                    "id": "lqe9rwe",
                    "author": "henk717",
                    "body": "You can do this by turning advanced load on in the settings. It will give you options on what to import.",
                    "score": 2,
                    "upvotes": 2,
                    "downvotes": 0,
                    "created_utc": 1728093214.0,
                    "parent_id": "t3_1fwcffd",
                    "link_id": "t3_1fwcffd",
                    "permalink": "/r/KoboldAI/comments/1fwcffd/import_world_info/lqe9rwe/"
                },
                {
                    "id": "lqel05c",
                    "author": "Gamerking1337",
                    "body": "Thank you very much, I couldnâ€™t find the info for the life of me.",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1728098224.0,
                    "parent_id": "t1_lqe9rwe",
                    "link_id": "t3_1fwcffd",
                    "permalink": "/r/KoboldAI/comments/1fwcffd/import_world_info/lqel05c/"
                }
            ]
        },
        {
            "id": "1fvehcq",
            "title": "Where to find correct model settings?",
            "author": "Fair_Cook_819",
            "score": 2,
            "upvotes": 2,
            "downvotes": 0,
            "num_comments": 1,
            "created_utc": 1727979800.0,
            "url": "https://www.reddit.com/r/KoboldAI/comments/1fvehcq/where_to_find_correct_model_settings/",
            "permalink": "/r/KoboldAI/comments/1fvehcq/where_to_find_correct_model_settings/",
            "selftext": "Iâ€™ve constantly in areas with no cellular connection and itâ€™s very nice to have an LLM on my phone in those moments. Iâ€™ve been playing around with running LLMâ€™s on my iphone 14pro and itâ€™s actually been amazing, but Iâ€™m a noob.\n\nThere are so many settings to mess around with on the models. Where can you find the proper templates, or any of the correct settings?\n\nIâ€™ve been trying to use LLMFarm and PocketPal. Iâ€™ve noticed sometimes different settings or prompt formats make the models spit complete gibberish of random characters.",
            "comments": [
                {
                    "id": "lq6g7zv",
                    "author": "Blendergeek1",
                    "body": "Because, technically speaking, there are no correct settings. Each setting is a variable in the formula. The exact details go over my head, but the basics concepts are fairly simple. Read up on what each one does, then experiment a bit. Creative writing outputs and fact based outputs will end with different settings. There are no \"correct\" settings, just better or worse for what you want.\n\nTemplates are another matter. Most models are fine tuned versions of a base model. The model should say somewhere in its hugging face page what its base model is. These base models have templates available, I just can't remember where.",
                    "score": 3,
                    "upvotes": 3,
                    "downvotes": 0,
                    "created_utc": 1727980827.0,
                    "parent_id": "t3_1fvehcq",
                    "link_id": "t3_1fvehcq",
                    "permalink": "/r/KoboldAI/comments/1fvehcq/where_to_find_correct_model_settings/lq6g7zv/"
                }
            ]
        },
        {
            "id": "1fv6v6w",
            "title": "Quantization testing of GGUF vs. GPTQ vs. Aphrodite Engine FPx",
            "author": "nero10579",
            "score": 13,
            "upvotes": 13,
            "downvotes": 0,
            "num_comments": 3,
            "created_utc": 1727959573.0,
            "url": "https://www.reddit.com/gallery/1fv6v6w",
            "permalink": "/r/KoboldAI/comments/1fv6v6w/quantization_testing_of_gguf_vs_gptq_vs_aphrodite/",
            "selftext": "",
            "comments": [
                {
                    "id": "lq4n3t6",
                    "author": "nero10579",
                    "body": "# PART 3\n\nFrom these results, we can see that there is a trend of the smaller the quantization it seems like the model response is longer. Where it peaked on Aphrodite's FP4 and FP5 where their average response length are the longest. Which to me confirms that the \"better\" score for FP5 is because the model loses it's intelligence in following instructions and just does it's own thing before answering.\n\nThis is not a good thing because if you see the MMLU Pro test I am using the system prompt is:\n\n    system_prompt = \"You are an expert that knows everything. You are tasked with answering a multiple-choice question. The following is a multiple choice question (with answers) about {subject}. Give your final answer in the format of `The answer is (chosen answer)`.\"\n\nSo I am sure if I make the system prompt tell the models to do COT, the higher quants and full model would predictable have higher scores. Although I actually do have to try that. I could also try enabling the COT method option for the MMLU Pro benchmark and see how it changes the scores.\n\nWe can also observe that aphrodite's custom FP quants are REALLY FASt. Where it actually scales to higher speeds the lower quant you go unlike other quant methods. On the other hand, we can also observe that GGUF model are REALLY SLOW. With Q6KM at a painful 28t/s even on a 3090Ti with batched inference using aphrodite engine.\n\n# Conclusion\n\nI think my conclusion is that to stay above 4-bit for quantization, and in that case use aphrodite's custom FP quants which are genuinely faster than anything else.\n\nIf you need to use 4-bit or low quants in general then GGUF definitely does seem to perform better than the other quant methods.\n\nSeeing these results we will be using aphrodite FP quants when we do need to run FP8 for our models hosted on our service atÂ [arliai.com](https://arliai.com).\n\nTLDR: GGUF is best for lower quants, Method doesn't really matter for higher quants or 8-bit, 8-bit isn't really worse than full BF16, Aphrodite's custom FP quants really work and are really fast, GGUF are the slowest.",
                    "score": 3,
                    "upvotes": 3,
                    "downvotes": 0,
                    "created_utc": 1727959686.0,
                    "parent_id": "t3_1fv6v6w",
                    "link_id": "t3_1fv6v6w",
                    "permalink": "/r/KoboldAI/comments/1fv6v6w/quantization_testing_of_gguf_vs_gptq_vs_aphrodite/lq4n3t6/"
                },
                {
                    "id": "lq4n0fi",
                    "author": "nero10579",
                    "body": "Since aphrodite engine released their new custom FP quantization technique, I wanted to check it out if it is any good and how it compares to the other quantization methods.\n\n[Running LLMs at Custom Floating-Points (Near-Lossless FP6) :](https://www.reddit.com/r/LocalLLaMA/comments/1fo5bbk/running_llms_at_custom_floatingpoints/)\n\n# How I tested\n\nI just chose Llama 3.1 8B Instruct for testing since I can run it on a single 3090Ti 24GB GPU and have multiple instances on different GPUs so that I can run this super fast.\n\nI ran all the models on Aphrodite Engine Release 0.6.2Â [PygmalionAI/aphrodite-engine: Large-scale LLM inference engine (github.com)](https://github.com/PygmalionAI/aphrodite-engine)\n\nI used my fork of the MMLU Pro benchmark toolÂ [Nero10578/OAI-API-MMLU-Pro (github.com)](https://github.com/Nero10578/OAI-API-MMLU-Pro) \n\nWhich I originally forked to add support for multiple languages for my own testing internally for [arliai.com](https://arliai.com). It is basically the same as the original one byÂ [chigkim/Ollama-MMLU-Pro (github.com)](https://github.com/chigkim/Ollama-MMLU-Pro)Â except for the added MMLU Pro datasets translated to a few different languages and the corresponding code changes to be able to parse the answers from those languages. \n\nI tested on all the MMLU Pro categories, I just only showed a few graphs because reddit limits how many photos I can attach to 20.\n\nI tested a few different GGUF static quants, GPTQ 4-bit and 8-bit and FP4 to FP8 Aphrodite quants. I also added for comparison the full BF16 model along with enabling prefix-caching, chunked-prefill and FP8 cache to see if they made any difference. Which showed to me actually that those cache options you can enable in aphrodite-engine doesn't really make a difference to performance.\n\n# Results\n\nLooking at the Total accuracy comparison (accuracy being the correct % vs total questions), we can see that GGUF Q4 performs the best compared to other 4-bit quantization methods from GPTQ and Aphrodite's FP4. This is means that if you are GPU VRAM limited and need the best performance possible using 4-bit quants then using GGUF quants is definitely the best way. On the other hand, with the higher quants and especially 8-bit quants, any of the methods have virtually the same performance. In fact, they are also virtually the same performance as the full BF16 model.\n\nLooking at the Total No Answer comparison, you can see that even the full weight has some percentage where it didn't provide an answer. So any of the quants measuring in the same neighborhood of no answer % should also be basically testing variance. With only the Aphrodite FP4 and FP5 quants producing abnormally higher No Answers.\n\nNow the interesting thing is that the Aphrodite FP5 quant is the highest scoring one of them all at 40.61%. This is really weird since usually no quants beats the original model.\n\nConsidering that a \"no answer\" means that the answer can't be found from the response, usually because the model hallucinates and doesn't actually follow the instructions of answering in the requested format. My theory is that the FP5 quant somehow is the perfect balance of making the model not follow the instructions enough so that it starts rambling and probably accidentally doing COT while still being smart enough to be correct.",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1727959650.0,
                    "parent_id": "t3_1fv6v6w",
                    "link_id": "t3_1fv6v6w",
                    "permalink": "/r/KoboldAI/comments/1fv6v6w/quantization_testing_of_gguf_vs_gptq_vs_aphrodite/lq4n0fi/"
                },
                {
                    "id": "lq4n1ic",
                    "author": "nero10579",
                    "body": "# Part 2\n\n# Speeds and Tokens\n\nNow the model performance is nice to know, but I think it is also interesting to compare the tokens/s effect of all the different models. Considering that I was using Aphrodite-Engine with batched inference of as many requests in parallel as possible in 24GB VRAM.\n\nFull BF16:\n\n    Prompt tokens: min 92, average 248, max 1288, total 2985871, tk/s 2383.83\n    Completion tokens: min 6, average 29, max 2048, total 353867, tk/s 282.52\n\nChunked Prefill ON:\n\n    Prompt tokens: min 92, average 248, max 1288, total 2985871, tk/s 2534.22\n    Completion tokens: min 6, average 28, max 2048, total 342663, tk/s 290.83\n\nFP8 Cache:\n\n    Prompt tokens: min 92, average 248, max 1288, total 2985871, tk/s 2029.50\n    Completion tokens: min 6, average 28, max 2048, total 331916, tk/s 225.60\n\nPrefix Caching:\n\n    Prompt tokens: min 92, average 248, max 1288, total 2985871, tk/s 2473.97\n    Completion tokens: min 6, average 27, max 2048, total 323321, tk/s 267.89\n\nAphrodite FP4:\n\n    Prompt tokens: min 92, average 248, max 1288, total 2985871, tk/s 1446.34\n    Completion tokens: min 6, average 93, max 2048, total 1121778, tk/s 543.38\n\nAphrodite FP5:\n\n    Prompt tokens: min 92, average 248, max 1288, total 2985871, tk/s 1593.53\n    Completion tokens: min 6, average 77, max 2048, total 921605, tk/s 491.85\n\nAphrodite FP6:\n\n    Prompt tokens: min 92, average 248, max 1288, total 2985871, tk/s 2294.66\n    Completion tokens: min 6, average 33, max 2048, total 399649, tk/s 307.13\n\n(Somehow I am missing FP7 results for this)\n\nAphrodite FP8:\n\n    Prompt tokens: min 92, average 248, max 1288, total 2985871, tk/s 2613.05\n    Completion tokens: min 6, average 22, max 2048, total 260266, tk/s 227.77\n\nGGUF Q4KM:\n\n    Prompt tokens: min 92, average 248, max 1288, total 2985871, tk/s 439.28\n    Completion tokens: min 6, average 36, max 2048, total 428394, tk/s 63.02\n\nGGUF Q5KM:\n\n    Prompt tokens: min 92, average 248, max 1288, total 2985871, tk/s 349.37\n    Completion tokens: min 6, average 42, max 2048, total 500782, tk/s 58.59\n\nGGUF Q6K:\n\n    Prompt tokens: min 92, average 248, max 1288, total 2985871, tk/s 389.36\n    Completion tokens: min 6, average 18, max 2048, total 216272, tk/s 28.20\n\nGGUF Q8:\n\n    Prompt tokens: min 92, average 248, max 1288, total 2985871, tk/s 370.69\n    Completion tokens: min 6, average 27, max 2048, total 326560, tk/s 40.54\n\nGPTQ Q4:\n\n    Prompt tokens: min 92, average 248, max 1288, total 2985871, tk/s 2334.41\n    Completion tokens: min 6, average 26, max 2048, total 310874, tk/s 243.05\n\nGPTQ Q8:\n\n    Prompt tokens: min 92, average 248, max 1288, total 2985871, tk/s 1877.55\n    Completion tokens: min 6, average 35, max 2048, total 421500, tk/s 265.04",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1727959661.0,
                    "parent_id": "t3_1fv6v6w",
                    "link_id": "t3_1fv6v6w",
                    "permalink": "/r/KoboldAI/comments/1fv6v6w/quantization_testing_of_gguf_vs_gptq_vs_aphrodite/lq4n1ic/"
                }
            ]
        },
        {
            "id": "1fuqroi",
            "title": "I really like the generate memory from context feature. Is there a way to do the same with world cards?",
            "author": "Sherlockyz",
            "score": 6,
            "upvotes": 6,
            "downvotes": 0,
            "num_comments": 4,
            "created_utc": 1727902394.0,
            "url": "https://www.reddit.com/r/KoboldAI/comments/1fuqroi/i_really_like_the_generate_memory_from_context/",
            "permalink": "/r/KoboldAI/comments/1fuqroi/i_really_like_the_generate_memory_from_context/",
            "selftext": "Hey guys. So while playing and creating rp stories i find the feature that allows to auto generate the \"resume\" of the story for the memory really useful.\n\nSo I was wondering if is there could be a similar feature for the world the world info cards. For example to generate a new resume for a certain character or location based on the text in the context.\n\nThanks in advance.",
            "comments": [
                {
                    "id": "lq1x81m",
                    "author": "ECrispy",
                    "body": "what feature is this and how do you use it? is it in koboldcpp?",
                    "score": 3,
                    "upvotes": 3,
                    "downvotes": 0,
                    "created_utc": 1727910012.0,
                    "parent_id": "t3_1fuqroi",
                    "link_id": "t3_1fuqroi",
                    "permalink": "/r/KoboldAI/comments/1fuqroi/i_really_like_the_generate_memory_from_context/lq1x81m/"
                },
                {
                    "id": "lq32y14",
                    "author": "wh33t",
                    "body": "I've never managed to get that feature to work. How is it that you use it?",
                    "score": 2,
                    "upvotes": 2,
                    "downvotes": 0,
                    "created_utc": 1727926437.0,
                    "parent_id": "t3_1fuqroi",
                    "link_id": "t3_1fuqroi",
                    "permalink": "/r/KoboldAI/comments/1fuqroi/i_really_like_the_generate_memory_from_context/lq32y14/"
                },
                {
                    "id": "lq3hypu",
                    "author": "Sherlockyz",
                    "body": "It is the world info, you can select it through the \"Context\" and \"World Info\". And just click on \"+Add\", you select the trigger words for example \"John, King John\" whenever the first or second group of words is in the context it will run the text that you put on the right, you can also change the chance of it triggering on the side.\n\n  \nIt is really useful to make the AI remembering certain people, locations, itens...",
                    "score": 2,
                    "upvotes": 2,
                    "downvotes": 0,
                    "created_utc": 1727934553.0,
                    "parent_id": "t1_lq1x81m",
                    "link_id": "t3_1fuqroi",
                    "permalink": "/r/KoboldAI/comments/1fuqroi/i_really_like_the_generate_memory_from_context/lq3hypu/"
                },
                {
                    "id": "lq3i5ju",
                    "author": "Sherlockyz",
                    "body": "If is pretty easy, just look at my answer for ECripsy. Btw, if you want to test easily just create something like random\\_word and say it is a spell magic that create a fire goblin, and try to use it as an action on the story. It should work.",
                    "score": 3,
                    "upvotes": 3,
                    "downvotes": 0,
                    "created_utc": 1727934671.0,
                    "parent_id": "t1_lq32y14",
                    "link_id": "t3_1fuqroi",
                    "permalink": "/r/KoboldAI/comments/1fuqroi/i_really_like_the_generate_memory_from_context/lq3i5ju/"
                }
            ]
        },
        {
            "id": "1fu7l76",
            "title": "Is there a way to use --lowvram but take advantage of any left over Vram?",
            "author": "jasonmbrown",
            "score": 5,
            "upvotes": 5,
            "downvotes": 0,
            "num_comments": 5,
            "created_utc": 1727841776.0,
            "url": "https://www.reddit.com/r/KoboldAI/comments/1fu7l76/is_there_a_way_to_use_lowvram_but_take_advantage/",
            "permalink": "/r/KoboldAI/comments/1fu7l76/is_there_a_way_to_use_lowvram_but_take_advantage/",
            "selftext": "https://preview.redd.it/dv6ji8tpn9sd1.png?width=818&format=png&auto=webp&s=a72ded5bb87e34cfc33fff3fcefbdbc58a4067b2\n\nI launched koboldcpp with --lowvram because I am using a 128k context window (Which takes up my server Ram)  \nDoes anyone have any recommendations on what to do with the additional 3gb vram? Are there any good image models I can run in that space.\n\nAlternatively Can KoboldCPP take advantage of that extra vram and use it as the processing space for the context?",
            "comments": [
                {
                    "id": "lpz3886",
                    "author": "P14gueD0c",
                    "body": "(sorry for my maybe bad English.) if i remember correctly, -lowvram doesn't let you use vram for context or anything other than a model itself. If you use gguf, you can offload some layers to gpu to speed things up. If you run llm only on cpu, you can use your gpu to host some stable diffusion image model although 3gb is really small and speed will be really bad i think, so you would likely need  to use -medvram (in stable diffusion) or something like that to run image model.",
                    "score": 2,
                    "upvotes": 2,
                    "downvotes": 0,
                    "created_utc": 1727876419.0,
                    "parent_id": "t3_1fu7l76",
                    "link_id": "t3_1fu7l76",
                    "permalink": "/r/KoboldAI/comments/1fu7l76/is_there_a_way_to_use_lowvram_but_take_advantage/lpz3886/"
                },
                {
                    "id": "lqojlnl",
                    "author": "ItchyBitchy7258",
                    "body": "Offload some layers to it if you can.Â \n\n\nIt's too small for SD; you really need 4GB to meet bare minimum. Mine OOMs all the time with 4GB if I make anything larger than 512x512.",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1728250277.0,
                    "parent_id": "t3_1fu7l76",
                    "link_id": "t3_1fu7l76",
                    "permalink": "/r/KoboldAI/comments/1fu7l76/is_there_a_way_to_use_lowvram_but_take_advantage/lqojlnl/"
                },
                {
                    "id": "lpz3nap",
                    "author": "P14gueD0c",
                    "body": "You can run most of the stable diffusion 1.5 models i think, although 1.5 doesn't really have good quality most of the time. You can find one in Civitai",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1727876574.0,
                    "parent_id": "t1_lpz3886",
                    "link_id": "t3_1fu7l76",
                    "permalink": "/r/KoboldAI/comments/1fu7l76/is_there_a_way_to_use_lowvram_but_take_advantage/lpz3nap/"
                },
                {
                    "id": "lrad6dk",
                    "author": "jasonmbrown",
                    "body": "I ended up using a1111 forge, which loads the image model (I found one thats 2gb but Ive also loaded some of the much larger models) in about 500mb-1gb (The images it generates are kinda crap though) I guess it really would help to have a lora for the character Im chatting with.",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1728583722.0,
                    "parent_id": "t1_lqojlnl",
                    "link_id": "t3_1fu7l76",
                    "permalink": "/r/KoboldAI/comments/1fu7l76/is_there_a_way_to_use_lowvram_but_take_advantage/lrad6dk/"
                },
                {
                    "id": "lpzi7v3",
                    "author": "jasonmbrown",
                    "body": "Thanks for the advice. I might give that a try and see how it performs. The Tesla P40 is a bit slow on image generation, but as I said before without -lowvram on koboldcpp I cant load the context (128k at q4/q8 at 10gb/20gb). I might go find a good SD model then, thanks for the advice.",
                    "score": 2,
                    "upvotes": 2,
                    "downvotes": 0,
                    "created_utc": 1727881512.0,
                    "parent_id": "t1_lpz3nap",
                    "link_id": "t3_1fu7l76",
                    "permalink": "/r/KoboldAI/comments/1fu7l76/is_there_a_way_to_use_lowvram_but_take_advantage/lpzi7v3/"
                }
            ]
        },
        {
            "id": "1frnkcm",
            "title": "Looking for MOE models for storytelling",
            "author": "shiro2033",
            "score": 1,
            "upvotes": 1,
            "downvotes": 0,
            "num_comments": 1,
            "created_utc": 1727555805.0,
            "url": "https://www.reddit.com/r/KoboldAI/comments/1frnkcm/looking_for_moe_models_for_storytelling/",
            "permalink": "/r/KoboldAI/comments/1frnkcm/looking_for_moe_models_for_storytelling/",
            "selftext": "Hi, I found out that MOE models are easy to run. Like I have 34B MOE model which works perfectly on my 4070super and there are a lot of 20B usual models whish are very slow. And output of 34B is better. So, If anybody know any good MOE models for storytelling, which can foollow story, context and are good at writing coherent text, please share it!\n\nCurrently I use Typhon-Mixtral but maybe there is something better.",
            "comments": [
                {
                    "id": "lpj2oi1",
                    "author": "henk717",
                    "body": "I don't know of any since MoE models are inherently bad at that task.\nWe have our own atÂ https://huggingface.co/KoboldAI/Mixtral-8x7B-Holodeck-v1-GGUF/tree/main but its Erebus counterpart I decided not to release due to the MoE version performing awful. We suspect its because MoE's are to prone to derailing when activating a wrong expert.",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1727634404.0,
                    "parent_id": "t3_1frnkcm",
                    "link_id": "t3_1frnkcm",
                    "permalink": "/r/KoboldAI/comments/1frnkcm/looking_for_moe_models_for_storytelling/lpj2oi1/"
                }
            ]
        },
        {
            "id": "1frktu6",
            "title": "I need help !",
            "author": "New-Veterinarian5806",
            "score": 0,
            "upvotes": 0,
            "downvotes": 0,
            "num_comments": 1,
            "created_utc": 1727548259.0,
            "url": "https://www.reddit.com/r/KoboldAI/comments/1frktu6/i_need_help/",
            "permalink": "/r/KoboldAI/comments/1frktu6/i_need_help/",
            "selftext": "hello/good evening, i really need help! i recently created an api key for venus chub  and every time i try it it gives me \"error empty response from ai\" and i really don't know what to do! i'm pretty new with all this ai stuff . I'm on the phone by the way. ",
            "comments": [
                {
                    "id": "lpdkoap",
                    "author": "AutoModerator",
                    "body": "Welcome to the KoboldAI Subreddit, since we get a lot of the same questions here is a brief FAQ for Venus and JanitorAI.\n\n## What is the KoboldAI (API) and how does it work?\nKoboldAI is originally a program for AI story writing, text adventures and chatting but we decided to create an API for our software so other software developers had an easy solution for their UI's and websites. VenusAI was one of these websites and anything based on it such as JanitorAI can use our software as well. But it is important to know that KoboldAI is intended to be a program that you can run yourself, not a service. This means that you are responsible to provide the right computer resources for the AI by either running it locally or using it trough a cloud provider.\n\n## Where do I get my API link from?\nYou get an API link from a working version of KoboldAI, if you have KoboldAI started the same link you use in the browser should be the one to access the API. However, be advised that VenusAI based websites ARE NOT PRIVATE and can only connect to external links. So connecting to https://localhost:5000 or https://127.0.0.1:5000 will not work unlike other solutions that let you connect to your KoboldAI instance privately.\n\nIf you installed KoboldAI on your own computer we have a mode called Remote Mode, you can find this as an icon in your startmenu if you opted for Start Menu icons in our offline installer. Or you can start this mode using remote-play.bat if you didn't. Linux users can add --remote instead when launching KoboldAI trough the terminal.\n\n## What do you mean VenusAI based stuff is not private?\nWe consider a solution private if your data does not leave your computer, for example when you use TavernAI this is a program that connects directly to KoboldAI and can for example access those localhost links. Of course our built in UI's are also completely private. VenusAI programmed it differently, their server is the one connecting to the AI which means they could log and intercept all of it. On top of that they force you to sign in, which means they have identifiable information that can be tied to the story. As a result most of the members and contributors of the KoboldAI community choose not to use these sites and opt for more privacy friendly solutions such as the KoboldAI UI itself or third party software such as Sillytavern.\n\n## I got a trycloudflare link but it doesn't work for some reason\nThis could be many things, but commonly people try the link before the AI finished loading or they have no AI selected.\n\n## I found a free way to do it without using my own computer but I keep getting CUDA out of memory errors!!!\nYes, there are guides out there of people using it on free cloud resources (We can't formally endorse this in this reply since we know it breaches the TOS of those services). The problem is that these guides often point to a free GPU that does not have enough VRAM for the default settings of VenusAI or JanitorAI. To fix this go to the Generation Settings inside Venus/Janitor and then lower the context size to 1024.\n\n## Ok so I have a top of the line gaming PC how do I set this up?\nBefore you set it up there is a lot of confusion about the kind of hardware people need because AI is a lot heavier to run than video games. At the bare minimum you will need an Nvidia GPU with 8GB of VRAM. With just this amount of VRAM you can run 2.7B models out of the box (In the future we have official 4-bit support to help you run higher models). For higher sizes you will need to have the required amount of VRAM as listed on the menu (Typically 16GB and up). If you want you can also stop by our [Discord Community](https://koboldai.org/discord) for some guidance on how to run higher models after you managed to get one of the models working using unofficial 4-bit versions or read up about that version [here](https://docs.pygmalion.chat/local-installation-(gpu)/koboldai4bit/)\n\n## This AI is so shit, its horrible compared to ChatGPT, why would anyone use this?!\nThis is a sentiment we unfortunately saw a lot in the JanitorAI discord because people misunderstand what KoboldAI is and who it is for. ChatGPT and the likes are large corporations throwing a lot of money at a paid service you could not possibly run at home. They are the best of the best AI models currently available. KoboldAI is not an AI on its own, its a project where you can bring an AI model yourself. And the AI's people can typically run at home are very small by comparison because it is expensive to both use and train larger models.\n\nSo most of these \"KoboldAI is dumb\" complaints come from both the wrong expectations of users comparing small models to massive private models such as ChatGPT, and them simply selecting the wrong model for what they want to do. A 6B no matter how good will simply not perform like a 175B model. But luckily for our community in the recent months things have gotten a lot closer when it comes to having a great chatbot. If you have ways to run the 13B or 30B model sizes of the recent instruction or chat models you should be able to get a great experience. But the quality of your experience does depend heavily on which model you pick.\n\nAnother important part is that you pick a model that is good at doing what you need to do, we know a lot of people pick Erebus for example for its NSFW capabilities. But understand that Erebus was designed to create compelling NSFW story writing and has not been trained for chatting. So while great at writing erotic novels, it is not the most compelling chatter. So whenever someone says that \"The bot of KoboldAI is dumb or shit\" understand they are not talking about KoboldAI, they are talking about whatever model they tried with it. For those wanting to enjoy Erebus we recommend using our own UI instead of VenusAI/JanitorAI and using it to write an erotic story rather than as a chatting partner.\n\n## Awesome, all caught up and I have an Nvidia with 8GB of vram or more. How do I install this thing?\nAssuming most of you are Windows users, for chat bot usage we currently recommend [this](https://github.com/henk717/KoboldAI/releases/download/Snapshot-7-5-2023/KoboldAI-Setup-230507r2.exe) offline installer. If you are on Linux you can git clone https://github.com/henk717/koboldai and use play.sh\n\n## Don't you have Koboldcpp that can run really good models without needing a good GPU, why didn't you talk about that?\nYes! [Koboldcpp](https://koboldai.org/cpp) is an amazing solution that lets people run [GGML models](https://huggingface.co/models?search=ggml) and it allows you to run those great models we have been enjoying for our own chatbots without having to rely on expensive hardware as long as you have a bit of patience waiting for the reply's. Why didn't we mention it? Because you are asking about VenusAI and/or JanitorAI which are not very compatible with it. The default link it generates will not work with these services, and unfortunately it takes to long to generate so it causes timeouts. If you are really determined to have this work you can always stop by our [Discord Community](https://koboldai.org/discord) and ask.\n\nOr of course you can stop using VenusAI and JanitorAI and enjoy a chatbot inside the UI that is bundled with Koboldcpp, that way you have a fully private way of running the good AI models on your own PC.\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/KoboldAI) if you have any questions or concerns.*",
                    "score": 2,
                    "upvotes": 2,
                    "downvotes": 0,
                    "created_utc": 1727548337.0,
                    "parent_id": "t3_1frktu6",
                    "link_id": "t3_1frktu6",
                    "permalink": "/r/KoboldAI/comments/1frktu6/i_need_help/lpdkoap/"
                }
            ]
        },
        {
            "id": "1freytf",
            "title": "Arm optimalized Mistral nemo 12b Q4_0_4_4 running locally on my phone poco X6 pro mediatek dimensity 8300 12bg ram from termux with an ok speed.",
            "author": "mitsu89",
            "score": 25,
            "upvotes": 25,
            "downvotes": 0,
            "num_comments": 4,
            "created_utc": 1727532132.0,
            "url": "https://i.redd.it/533s4f7e3krd1.jpeg",
            "permalink": "/r/KoboldAI/comments/1freytf/arm_optimalized_mistral_nemo_12b_q4_0_4_4_running/",
            "selftext": "",
            "comments": [
                {
                    "id": "lpcai83",
                    "author": "mitsu89",
                    "body": "It said mediatek npu 780 can run llm up to 10b but i still didn't expect it from a \"budget\" phoneÂ https://www.mediatek.com/products/smartphones-2/mediatek-dimensity-8300Â Â \n\n\nÂ Â earlier i tried the \"normal\" 7b models but even that was too slow (maybe x86 optimalized?) but the arm optimalized q4_0_4_4 are fast. I mean where i saw it here there are arm optimalized version i had to try it, and from now i don't have to turn on my PC just for itÂ https://huggingface.co/bartowski/Mistral-Nemo-Instruct-2407-GGUF",
                    "score": 6,
                    "upvotes": 6,
                    "downvotes": 0,
                    "created_utc": 1727532403.0,
                    "parent_id": "t3_1freytf",
                    "link_id": "t3_1freytf",
                    "permalink": "/r/KoboldAI/comments/1freytf/arm_optimalized_mistral_nemo_12b_q4_0_4_4_running/lpcai83/"
                },
                {
                    "id": "lpho071",
                    "author": "Wise-Paramedic-4536",
                    "body": "How did you compile koboldcpp to be able to run this?",
                    "score": 4,
                    "upvotes": 4,
                    "downvotes": 0,
                    "created_utc": 1727617646.0,
                    "parent_id": "t3_1freytf",
                    "link_id": "t3_1freytf",
                    "permalink": "/r/KoboldAI/comments/1freytf/arm_optimalized_mistral_nemo_12b_q4_0_4_4_running/lpho071/"
                },
                {
                    "id": "lphubh5",
                    "author": "mitsu89",
                    "body": "I followed this (android termux installation is in the end of the page) https://gitee.com/magicor/koboldcpp\n\n\nÂ  I think there was some error, I copy paste it to claude ai. I think the \"change repo\" part is not necessary. And I copy the ARM optimized mistral Nemo model to the koboldcpp folder. And i start koboldai with this Cd koboldcpp python koboldcpp.py --model Mistral-Nemo-Instruct-2407-Q4_0_4_4.gguf --contextsize 2048 And in browser i typed http://localhost:5001 And the koboldai appeared and working with the not very censored mistral model locally. If i set contextsize to 4096 the model is much slower and using more memory.Â  If I want more context window i can use this: Nemotron-Mini-4B-Instruct-Q4_0_4_4.gguf",
                    "score": 5,
                    "upvotes": 5,
                    "downvotes": 0,
                    "created_utc": 1727620067.0,
                    "parent_id": "t1_lpho071",
                    "link_id": "t3_1freytf",
                    "permalink": "/r/KoboldAI/comments/1freytf/arm_optimalized_mistral_nemo_12b_q4_0_4_4_running/lphubh5/"
                },
                {
                    "id": "lphwktg",
                    "author": "Wise-Paramedic-4536",
                    "body": "Thanks a lot! Will try!",
                    "score": 2,
                    "upvotes": 2,
                    "downvotes": 0,
                    "created_utc": 1727620885.0,
                    "parent_id": "t1_lphubh5",
                    "link_id": "t3_1freytf",
                    "permalink": "/r/KoboldAI/comments/1freytf/arm_optimalized_mistral_nemo_12b_q4_0_4_4_running/lphwktg/"
                }
            ]
        },
        {
            "id": "1fqt2ub",
            "title": "how to launch koboldcpp without it opening its webui?",
            "author": "Ok_Effort_5849",
            "score": 1,
            "upvotes": 1,
            "downvotes": 0,
            "num_comments": 5,
            "created_utc": 1727457789.0,
            "url": "https://www.reddit.com/r/KoboldAI/comments/1fqt2ub/how_to_launch_koboldcpp_without_it_opening_its/",
            "permalink": "/r/KoboldAI/comments/1fqt2ub/how_to_launch_koboldcpp_without_it_opening_its/",
            "selftext": "I am using koboldcpp as a backend for my personal project and would prefer to use it as a backend only. I want to keep using the python launcher though, its just the webui which is unecessary.",
            "comments": [
                {
                    "id": "lp7ytru",
                    "author": "Sir-Help-a-Lot",
                    "body": "I think the easiest way is to just save a config from the launcher and then use it as a parameter:\n\n    koboldcpp.exe myconfig.kcpps",
                    "score": 5,
                    "upvotes": 5,
                    "downvotes": 0,
                    "created_utc": 1727460447.0,
                    "parent_id": "t3_1fqt2ub",
                    "link_id": "t3_1fqt2ub",
                    "permalink": "/r/KoboldAI/comments/1fqt2ub/how_to_launch_koboldcpp_without_it_opening_its/lp7ytru/"
                },
                {
                    "id": "lp7t8j6",
                    "author": "Trainraider",
                    "body": "Don't remember but the answer is in ./koboldcpp -h\n\nYou can even avoid reading the help message by feeding it ai and re-asking your question. Claude wrote my kobold start up command for me and fish shell autocompletes it whenever I type koboldcpp again.",
                    "score": 2,
                    "upvotes": 2,
                    "downvotes": 0,
                    "created_utc": 1727458653.0,
                    "parent_id": "t3_1fqt2ub",
                    "link_id": "t3_1fqt2ub",
                    "permalink": "/r/KoboldAI/comments/1fqt2ub/how_to_launch_koboldcpp_without_it_opening_its/lp7t8j6/"
                },
                {
                    "id": "lpl0q5k",
                    "author": "practicalpcguide",
                    "body": "untick \"lunch browser\" from the quick launch or hardware tab.",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1727658687.0,
                    "parent_id": "t3_1fqt2ub",
                    "link_id": "t3_1fqt2ub",
                    "permalink": "/r/KoboldAI/comments/1fqt2ub/how_to_launch_koboldcpp_without_it_opening_its/lpl0q5k/"
                },
                {
                    "id": "lparulx",
                    "author": "Ok_Effort_5849",
                    "body": "Thanks, this worked!",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1727500281.0,
                    "parent_id": "t1_lp7ytru",
                    "link_id": "t3_1fqt2ub",
                    "permalink": "/r/KoboldAI/comments/1fqt2ub/how_to_launch_koboldcpp_without_it_opening_its/lparulx/"
                },
                {
                    "id": "lpasks9",
                    "author": "Ok_Effort_5849",
                    "body": "I saved a config with the launch browser ticked off(which i feel silly for not noticing before, maybe its a new feature!?) and used  this command:\n./koboldcpp.exe myconfig.kcpps --showgui\n\nNow this runs the backend without launching the webui but still shows the python launcher",
                    "score": 2,
                    "upvotes": 2,
                    "downvotes": 0,
                    "created_utc": 1727500709.0,
                    "parent_id": "t1_lparulx",
                    "link_id": "t3_1fqt2ub",
                    "permalink": "/r/KoboldAI/comments/1fqt2ub/how_to_launch_koboldcpp_without_it_opening_its/lpasks9/"
                }
            ]
        },
        {
            "id": "1fpu454",
            "title": "Is my low VRAM image generation setup correct?",
            "author": "AnonymousAardvark22",
            "score": 8,
            "upvotes": 8,
            "downvotes": 0,
            "num_comments": 5,
            "created_utc": 1727350912.0,
            "url": "https://i.redd.it/5p9a6psm25rd1.png",
            "permalink": "/r/KoboldAI/comments/1fpu454/is_my_low_vram_image_generation_setup_correct/",
            "selftext": "",
            "comments": [
                {
                    "id": "lp06obk",
                    "author": "AnonymousAardvark22",
                    "body": "4GB VRAM, linux. I can generate images in chat but I think the setup may be wrong, so if you can answer any of my questions below please do.\n\n* Start X1111:  ./webui.sh --lowvram --api --listen --cors-allow-origins=\\*\n* Once running change the model in X1111 from v1-5-pruned-emaonly.safetensors to Anything-V3.0-pruned-fp16.safetensors\n* From KoboldCpp launch with CuBLAS and MMQ, LLM file (7B Mistral based) and also Anything-V3.0-pruned-fp16.safetensors for Image VAE, Clamped Mode 512, Compress Weights, and Use TAE SD.\n* KoboldAI Lite settings >> media >> generate images >> Local A1111, Anything-V3.0-pruned-fp16.safetensors, A1111 endpoint http://localhost:7860, number of steps 20, high res disabled.\n\nWith this setup I can add an image to a chat using the add img button and selecting generate image (automatic) or (custom prompt).\n\n1. Should the images not be genreated automatically with each response as I have selected in settings or when I ask for one?\n2. Is it correct I have Anything-V3 selected in 3 places, in A1111, when launching with KoboldCpp, and then in media settings?\n3. Is it correct I have SDWUI A1111 running on http://localhost:7860 and Stable UI on http://localhost:5001/sdui?\n4. Should I see images being generated in either UI as I read others do, because I do not, they just appear in the chat? Also the they do not appear to be saved anywhere.\n5. Sometimes images are uncensored, sometimes not, is there some way to set this?",
                    "score": 3,
                    "upvotes": 3,
                    "downvotes": 0,
                    "created_utc": 1727350945.0,
                    "parent_id": "t3_1fpu454",
                    "link_id": "t3_1fpu454",
                    "permalink": "/r/KoboldAI/comments/1fpu454/is_my_low_vram_image_generation_setup_correct/lp06obk/"
                },
                {
                    "id": "lp0iqum",
                    "author": "fish312",
                    "body": "There is an option to auto generate images, you have to turn it on in the settings. As for the image censorship i guess it depends on what model you are using, maybe you can keep 1 sfw and 1 nsfw model to choose from.",
                    "score": 3,
                    "upvotes": 3,
                    "downvotes": 0,
                    "created_utc": 1727356043.0,
                    "parent_id": "t1_lp06obk",
                    "link_id": "t3_1fpu454",
                    "permalink": "/r/KoboldAI/comments/1fpu454/is_my_low_vram_image_generation_setup_correct/lp0iqum/"
                },
                {
                    "id": "lp0j7ik",
                    "author": "AnonymousAardvark22",
                    "body": "Do you mean the setting I have on already in the screenshot?",
                    "score": 3,
                    "upvotes": 3,
                    "downvotes": 0,
                    "created_utc": 1727356217.0,
                    "parent_id": "t1_lp0iqum",
                    "link_id": "t3_1fpu454",
                    "permalink": "/r/KoboldAI/comments/1fpu454/is_my_low_vram_image_generation_setup_correct/lp0j7ik/"
                },
                {
                    "id": "lp0jmae",
                    "author": "fish312",
                    "body": "Oh yeah, in that case it should work. Just send a few messages and it should make a new image.",
                    "score": 3,
                    "upvotes": 3,
                    "downvotes": 0,
                    "created_utc": 1727356370.0,
                    "parent_id": "t1_lp0j7ik",
                    "link_id": "t3_1fpu454",
                    "permalink": "/r/KoboldAI/comments/1fpu454/is_my_low_vram_image_generation_setup_correct/lp0jmae/"
                },
                {
                    "id": "lp0js1r",
                    "author": "AnonymousAardvark22",
                    "body": "Only happens when I manually use generate image. I do not see any errors in terminal or any attempt to generate unless I do it.",
                    "score": 3,
                    "upvotes": 3,
                    "downvotes": 0,
                    "created_utc": 1727356430.0,
                    "parent_id": "t1_lp0jmae",
                    "link_id": "t3_1fpu454",
                    "permalink": "/r/KoboldAI/comments/1fpu454/is_my_low_vram_image_generation_setup_correct/lp0js1r/"
                }
            ]
        },
        {
            "id": "1fp6kc0",
            "title": "Using KoboldCpp API",
            "author": "Intelligent_Bet_3985",
            "score": 3,
            "upvotes": 3,
            "downvotes": 0,
            "num_comments": 11,
            "created_utc": 1727276452.0,
            "url": "https://www.reddit.com/r/KoboldAI/comments/1fp6kc0/using_koboldcpp_api/",
            "permalink": "/r/KoboldAI/comments/1fp6kc0/using_koboldcpp_api/",
            "selftext": "I am trying to write a simple Python script to send a message to my local Kobold API at localhost:5001 and receive a reply. However, no matter what I try, I am getting a 503 error. I am trying  SillyTavern works just fine with my KoboldCpp, so that's clearly not the problem. I'm using the /api/v1/generate endpoint, as suggested in the documentation. Maybe someone could share such a script, because either I'm missing something really obvious, or it's some kind of bizarre system configuration issue. ",
            "comments": [
                {
                    "id": "lovayx5",
                    "author": "ticklemeplease7",
                    "body": "503 usually implies an overload error. How are you waiting for a response back, and if itâ€™s with a request, how often are you checking?",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1727278352.0,
                    "parent_id": "t3_1fp6kc0",
                    "link_id": "t3_1fp6kc0",
                    "permalink": "/r/KoboldAI/comments/1fp6kc0/using_koboldcpp_api/lovayx5/"
                },
                {
                    "id": "lp2vb8c",
                    "author": "henk717",
                    "body": "The API documentation is available if you visit /api in the browser including interactive samples.\n\n[https://github.com/henk717/KoboldAI/blob/united/api\\_example.py](https://github.com/henk717/KoboldAI/blob/united/api_example.py) isn't for KoboldCpp but mostly compatible.",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1727383013.0,
                    "parent_id": "t3_1fp6kc0",
                    "link_id": "t3_1fp6kc0",
                    "permalink": "/r/KoboldAI/comments/1fp6kc0/using_koboldcpp_api/lp2vb8c/"
                },
                {
                    "id": "lp7qx7f",
                    "author": "Ok_Effort_5849",
                    "body": "have a look at this thread:\nhttps://www.reddit.com/r/KoboldAI/comments/1581pnr/how_do_i_access_the_api_with_koboldcppexe/",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1727457918.0,
                    "parent_id": "t3_1fp6kc0",
                    "link_id": "t3_1fp6kc0",
                    "permalink": "/r/KoboldAI/comments/1fp6kc0/using_koboldcpp_api/lp7qx7f/"
                },
                {
                    "id": "lovfzsa",
                    "author": "Intelligent_Bet_3985",
                    "body": "I get it on the very first attempt, so I'm not sure how that could be possible.",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1727279919.0,
                    "parent_id": "t1_lovayx5",
                    "link_id": "t3_1fp6kc0",
                    "permalink": "/r/KoboldAI/comments/1fp6kc0/using_koboldcpp_api/lovfzsa/"
                },
                {
                    "id": "lovgrpf",
                    "author": "ticklemeplease7",
                    "body": "Are you attempting immediately after sending the request?",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1727280159.0,
                    "parent_id": "t1_lovfzsa",
                    "link_id": "t3_1fp6kc0",
                    "permalink": "/r/KoboldAI/comments/1fp6kc0/using_koboldcpp_api/lovgrpf/"
                },
                {
                    "id": "lovo9wz",
                    "author": "Intelligent_Bet_3985",
                    "body": "I think so. But I'm not seeing any activity at all in the Kobold console, so it's like my requests aren't even reaching it",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1727282514.0,
                    "parent_id": "t1_lovgrpf",
                    "link_id": "t3_1fp6kc0",
                    "permalink": "/r/KoboldAI/comments/1fp6kc0/using_koboldcpp_api/lovo9wz/"
                },
                {
                    "id": "lovon91",
                    "author": "ticklemeplease7",
                    "body": "Hmmâ€¦ ðŸ¤” what does the request scheme in your code look like, and how long are the prompts that youâ€™re sending?",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1727282629.0,
                    "parent_id": "t1_lovo9wz",
                    "link_id": "t3_1fp6kc0",
                    "permalink": "/r/KoboldAI/comments/1fp6kc0/using_koboldcpp_api/lovon91/"
                },
                {
                    "id": "lovp789",
                    "author": "ticklemeplease7",
                    "body": "Also, just making sure, youâ€™re not sharing it with the horde while trying to do this, right?",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1727282799.0,
                    "parent_id": "t1_lovon91",
                    "link_id": "t3_1fp6kc0",
                    "permalink": "/r/KoboldAI/comments/1fp6kc0/using_koboldcpp_api/lovp789/"
                },
                {
                    "id": "low4ig2",
                    "author": "Intelligent_Bet_3985",
                    "body": "No, not using anything else.\n\nSomething like this:\n\n    def send_message_to_llm(message):\n        url = \"http://localhost:5001/api/v1/generate\"\n            payload = {\n            \"prompt\": message,\n            \"max_new_tokens\": 100,\n            \"temperature\": 0.7\n        }\n    \n        headers = {\"Content-Type\": \"application/json\"}\n    \n       \n        response = requests.post(url, headers=headers, json=json.dumps(payload))\n    \n        \n        if response.status_code == 200:\n            return response.json()['text']\n        else:\n            return f\"Error: {response.status_code}, {response.text}\"\n    if __name__ == \"__main__\":\n        user_message = input(\"Enter your message: \")\n        reply = send_message_to_llm(user_message)\n        print(\"LLM Reply:\", reply)def send_message_to_llm(message):\n        url = \"http://localhost:5001/api/v1/generate\"\n        \n        payload = {\n            \"prompt\": message,\n            \"max_new_tokens\": 100,\n            \"temperature\": 0.7\n        }\n    \n        headers = {\"Content-Type\": \"application/json\"}\n    \n       \n        response = requests.post(url, headers=headers, json=json.dumps(payload))\n    \n        \n        if response.status_code == 200:\n            return response.json()['text']\n        else:\n            return f\"Error: {response.status_code}, {response.text}\"\n    \n    \n    if __name__ == \"__main__\":\n        user_message = input(\"Enter your message: \")\n        reply = send_message_to_llm(user_message)\n        print(\"LLM Reply:\", reply)",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1727287509.0,
                    "parent_id": "t1_lovp789",
                    "link_id": "t3_1fp6kc0",
                    "permalink": "/r/KoboldAI/comments/1fp6kc0/using_koboldcpp_api/low4ig2/"
                },
                {
                    "id": "loxvrc9",
                    "author": "seastatefive",
                    "body": "Your url should be https and not http? Try that?",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1727308284.0,
                    "parent_id": "t1_low4ig2",
                    "link_id": "t3_1fp6kc0",
                    "permalink": "/r/KoboldAI/comments/1fp6kc0/using_koboldcpp_api/loxvrc9/"
                },
                {
                    "id": "lp1nbbb",
                    "author": "Intelligent_Bet_3985",
                    "body": "Koboldcpp console says it should be http\n\nThat said, I tried https just in case, and got a bunch of errors.",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1727369246.0,
                    "parent_id": "t1_loxvrc9",
                    "link_id": "t3_1fp6kc0",
                    "permalink": "/r/KoboldAI/comments/1fp6kc0/using_koboldcpp_api/lp1nbbb/"
                }
            ]
        },
        {
            "id": "1foyu9o",
            "title": "[IQ3_XXS Is slow need help]",
            "author": "Weak-Shelter-1698",
            "score": 1,
            "upvotes": 1,
            "downvotes": 0,
            "num_comments": 3,
            "created_utc": 1727249642.0,
            "url": "https://www.reddit.com/r/KoboldAI/comments/1foyu9o/iq3_xxs_is_slow_need_help/",
            "permalink": "/r/KoboldAI/comments/1foyu9o/iq3_xxs_is_slow_need_help/",
            "selftext": "Hey Fellas,\n\nRecently i found the Euryale 2.1 70B model and it's really good even on IQ3\\_XXS quant,  but the issue i'm facing is that it's really slow.. like 1t/s  \nI'm using 2 T4 gpus a total of 30gb vram with 8k context but it's too slow. i've tried higher quants using system RAM aswell but it's 0.1 t/s any guide for me to speed it up?\n\nFollowing is the command i'm using\n\n`./koboldcpp_linux model.gguf --usecublas mmq --gpulayers 999 --contextsize 8192 --port 2222 --quiet --flashattention`",
            "comments": [
                {
                    "id": "lounpy8",
                    "author": "zasura",
                    "body": "try to split the GPU layers into CPU (you need ram). You use all of your layers on GPU and i think IQ3\\_XXS is larger than 30 GB (maybe).",
                    "score": 2,
                    "upvotes": 2,
                    "downvotes": 0,
                    "created_utc": 1727270627.0,
                    "parent_id": "t3_1foyu9o",
                    "link_id": "t3_1foyu9o",
                    "permalink": "/r/KoboldAI/comments/1foyu9o/iq3_xxs_is_slow_need_help/lounpy8/"
                },
                {
                    "id": "lp6650w",
                    "author": "Latter_Count_2515",
                    "body": "I have had similar issues with ooba too. For ooba I have found using vram only quants like exl2 jumped my t/s from around 1 to 8. 8 is still about half the speed others have reported but I can only guess the first part comes from gguf offloaded layers to ram when it shouldn't be. Why I am getting 8 instead of the expected 15 is most likely a motherboard limitation. For context my system is windows 11, 72gb ddr 4 3200mhz ram, i5 13600 cpu, rtx 3090, rtx 3060 12gb version. Model tested llama 3.1 70b IQ3_XS.",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1727438089.0,
                    "parent_id": "t3_1foyu9o",
                    "link_id": "t3_1foyu9o",
                    "permalink": "/r/KoboldAI/comments/1foyu9o/iq3_xxs_is_slow_need_help/lp6650w/"
                },
                {
                    "id": "loyw35f",
                    "author": "Weak-Shelter-1698",
                    "body": "have 35gb of ram. and for IQ3\\_XXS it's 27gb only.",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1727322630.0,
                    "parent_id": "t1_lounpy8",
                    "link_id": "t3_1foyu9o",
                    "permalink": "/r/KoboldAI/comments/1foyu9o/iq3_xxs_is_slow_need_help/loyw35f/"
                }
            ]
        },
        {
            "id": "1foar8k",
            "title": "Can I set image gen to SD -medvram or -lowvram mode?",
            "author": "AnonymousAardvark22",
            "score": 2,
            "upvotes": 2,
            "downvotes": 0,
            "num_comments": 2,
            "created_utc": 1727179306.0,
            "url": "https://www.reddit.com/r/KoboldAI/comments/1foar8k/can_i_set_image_gen_to_sd_medvram_or_lowvram_mode/",
            "permalink": "/r/KoboldAI/comments/1foar8k/can_i_set_image_gen_to_sd_medvram_or_lowvram_mode/",
            "selftext": "I was surprised that with just 4GB VRAM on a GTX 970 Kobold can run on default settings SultrySilicon-7B-V2, mistral-7b-mmproj-v1.5-Q4\\_1, and whisper-base.en-q5\\_1 at the same time.\n\nFor image gen I can start Kobold with Anything-V3.0-pruned-fp16 or Deliberate\\_v2 though no image is returned. On the SD web UI I was able to generate a small test image of a dog once after changing some settings for SD on that UI, probably with all other models disabled in Kobold, and possibly using CPU.\n\nI have read that SD has the COMMANDLINE\\_ARGS \\`--medvram\\` for 4-6 GB VRAM and \\`--lowvram\\`  for 2GB VRAM. Is there some way I can set Kobold to run SD like this, even if it means disabling some of all of the other models?\n\n[Stable Diffusion on my GTX 970 -4 gb vram can rock it too](https://www.youtube.com/watch?v=fvWVeEds0B4)\n\nGPU upgrade planned but for now I just ran my first model a few days ago and happy I at least can even do that.",
            "comments": [
                {
                    "id": "lotfrgi",
                    "author": "henk717",
                    "body": "There is an optiom to quantize the image gen model but thats the only thing you can do to limit vram. Black images is often if there is no baked vae. Try the fix vae option to see if that fixes it, that should make it work with poor quality. If it works you can download a proper vae for the model and load a good one.",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1727246039.0,
                    "parent_id": "t3_1foar8k",
                    "link_id": "t3_1foar8k",
                    "permalink": "/r/KoboldAI/comments/1foar8k/can_i_set_image_gen_to_sd_medvram_or_lowvram_mode/lotfrgi/"
                },
                {
                    "id": "loti18g",
                    "author": "AnonymousAardvark22",
                    "body": "Do you mean the option to `Compress Weights (saves memory)` or an actually alternate quantized model to Anything-V3.0-pruned-fp16 or Deliberate\\_v2? If the latter could you please tell me the name of the model.\n\nI was already using compress weights but I thought the `Use TAE SD (Autofix Broken VAE)` was for something I was not using as I had no file loaded directly above it, thanks. With both selected if I run Anything with no other models loaded I can generate images in a few seconds on she SD UI. When I do have a model loaded, should any PNG character I have downloaded from different websites reply with an image if I ask them to send me a picture or can only specific characters do it or is there some other way to trigger this? I do not see anything in terminal about an attempt to generate an image, or an error, memory related or otherwise and I can see there is free VRAM.\n\n    ImageGen Init - Load Model: /home/koboldcpp/multimodal/Anything-V3.0-pruned-fp16.safetensors\n    With TAE SD VAE: /tmp/_MEIzAK7R8/taesd.embd\n    WARNING: Behavior may be unexpected when allocating 0 bytes for ggml_calloc!\n    Load Image Model OK: True\n\nFrom my quick test today the reply described the character showing me a picture on their phone but there was nothing. Some other characters I tried a few days ago without using the fix option would reply with some text like `<attached image>`.\n\nUpdate: I can also generate images fine now in SD UI when the other models are running in Kobold, I just have not been able to generate from within Kobold, even with autogenerate enabled.",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1727247510.0,
                    "parent_id": "t1_lotfrgi",
                    "link_id": "t3_1foar8k",
                    "permalink": "/r/KoboldAI/comments/1foar8k/can_i_set_image_gen_to_sd_medvram_or_lowvram_mode/loti18g/"
                }
            ]
        },
        {
            "id": "1fnu86z",
            "title": "So has the ship sailed for importing Ai Dungeon Content",
            "author": "eggs-benedryl",
            "score": 3,
            "upvotes": 3,
            "downvotes": 0,
            "num_comments": 2,
            "created_utc": 1727122540.0,
            "url": "https://www.reddit.com/r/KoboldAI/comments/1fnu86z/so_has_the_ship_sailed_for_importing_ai_dungeon/",
            "permalink": "/r/KoboldAI/comments/1fnu86z/so_has_the_ship_sailed_for_importing_ai_dungeon/",
            "selftext": "I had hundreds of scenarios and huge worlds that I wish I could import. I can export world data but it's not in the right format. If that's my only option does anyone have any info about how to make them readable by kobold.",
            "comments": [
                {
                    "id": "lol9a66",
                    "author": "Ill_Yam_9994",
                    "body": "Post an example file. Someone could probably build a parser/converter script really easily. I can take a look.",
                    "score": 6,
                    "upvotes": 6,
                    "downvotes": 0,
                    "created_utc": 1727126332.0,
                    "parent_id": "t3_1fnu86z",
                    "link_id": "t3_1fnu86z",
                    "permalink": "/r/KoboldAI/comments/1fnu86z/so_has_the_ship_sailed_for_importing_ai_dungeon/lol9a66/"
                },
                {
                    "id": "lotfhfu",
                    "author": "henk717",
                    "body": "On any of the known exporters yes. You'd have to find a working one or manually copy stuff somehow. AI Dungeon rebuilt their site like 4 times since the exporters were made.",
                    "score": 2,
                    "upvotes": 2,
                    "downvotes": 0,
                    "created_utc": 1727245861.0,
                    "parent_id": "t3_1fnu86z",
                    "link_id": "t3_1fnu86z",
                    "permalink": "/r/KoboldAI/comments/1fnu86z/so_has_the_ship_sailed_for_importing_ai_dungeon/lotfhfu/"
                }
            ]
        },
        {
            "id": "1flz3my",
            "title": "Best settings for 1080ti 11GB VRAM?",
            "author": "AzudemK",
            "score": 5,
            "upvotes": 5,
            "downvotes": 0,
            "num_comments": 4,
            "created_utc": 1726910572.0,
            "url": "https://www.reddit.com/r/KoboldAI/comments/1flz3my/best_settings_for_1080ti_11gb_vram/",
            "permalink": "/r/KoboldAI/comments/1flz3my/best_settings_for_1080ti_11gb_vram/",
            "selftext": "I'm very new to this and already played around with Kobold pp, so far so good. But are there any settings which would fit my 1080ti 11GB GPU?",
            "comments": [
                {
                    "id": "loewa48",
                    "author": "yJz3X",
                    "body": "Bump, Sitting on quadro p5000 laptop with gtx 1070 based gpu with 16g vram.",
                    "score": 2,
                    "upvotes": 2,
                    "downvotes": 0,
                    "created_utc": 1727034183.0,
                    "parent_id": "t3_1flz3my",
                    "link_id": "t3_1flz3my",
                    "permalink": "/r/KoboldAI/comments/1flz3my/best_settings_for_1080ti_11gb_vram/loewa48/"
                },
                {
                    "id": "lpb8yuv",
                    "author": "Instruction-Open",
                    "body": "Since no one else has said anything, I'm not sure there are any settings specifically that will change the general operation of Kobold. As long as you're using a model that will fit on your VRAM, any other settings such as samplers have to do with the coherency model itself, not what hardware it's running on.\n\n  \nCertainly I'm no expert on the matter, but I don't believe there's many software options to change when it comes to your GPU. Kobold is either using it, or it's not.",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1727511382.0,
                    "parent_id": "t3_1flz3my",
                    "link_id": "t3_1flz3my",
                    "permalink": "/r/KoboldAI/comments/1flz3my/best_settings_for_1080ti_11gb_vram/lpb8yuv/"
                },
                {
                    "id": "lpb9ts2",
                    "author": "Instruction-Open",
                    "body": "Possibly this thread might be of help?\n\n[https://www.reddit.com/r/KoboldAI/comments/1fkkkgl/did\\_a\\_little\\_benchmark\\_to\\_determine\\_some\\_general/](https://www.reddit.com/r/KoboldAI/comments/1fkkkgl/did_a_little_benchmark_to_determine_some_general/)",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1727511989.0,
                    "parent_id": "t3_1flz3my",
                    "link_id": "t3_1flz3my",
                    "permalink": "/r/KoboldAI/comments/1flz3my/best_settings_for_1080ti_11gb_vram/lpb9ts2/"
                },
                {
                    "id": "lpb978f",
                    "author": "Instruction-Open",
                    "body": "As I said above, I don't think there's really much you can change in Kobold regarding the GPU as long as the model fits in the VRAM. You can tell it to use the GPU, or use the slower CPU. I don't know any other settings to make it run any better.",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1727511546.0,
                    "parent_id": "t1_loewa48",
                    "link_id": "t3_1flz3my",
                    "permalink": "/r/KoboldAI/comments/1flz3my/best_settings_for_1080ti_11gb_vram/lpb978f/"
                }
            ]
        },
        {
            "id": "1flmib3",
            "title": "Is there a way to make Kobold CPP work with the latest Kobald UI? Because there are sooo many missing features",
            "author": "Parogarr",
            "score": 5,
            "upvotes": 5,
            "downvotes": 0,
            "num_comments": 5,
            "created_utc": 1726866124.0,
            "url": "https://www.reddit.com/r/KoboldAI/comments/1flmib3/is_there_a_way_to_make_kobold_cpp_work_with_the/",
            "permalink": "/r/KoboldAI/comments/1flmib3/is_there_a_way_to_make_kobold_cpp_work_with_the/",
            "selftext": "I've seen a whole lot of posts on here about how K cpp replaces the mostly dead Kobald AI United. But in terms of features, usability it's not a suitable replacement at all. It's like a giant step back. Before they stopped updating kobald AI, it had a ton of great features and an interface that looked a lot like novel AI. But the one that comes with kobald CPP is really not to my liking. is there a way to connect the apps?",
            "comments": [
                {
                    "id": "lo4fau8",
                    "author": "henk717",
                    "body": "Yes, you can use Load Model -> Online Services -> KoboldAI API and then add the KoboldCpp link.",
                    "score": 7,
                    "upvotes": 7,
                    "downvotes": 0,
                    "created_utc": 1726870801.0,
                    "parent_id": "t3_1flmib3",
                    "link_id": "t3_1flmib3",
                    "permalink": "/r/KoboldAI/comments/1flmib3/is_there_a_way_to_make_kobold_cpp_work_with_the/lo4fau8/"
                },
                {
                    "id": "lo90ojo",
                    "author": "AlexysLovesLexxie",
                    "body": "You should.go.to the Kobold.CPP GitHub and make feature requests.  How will the deva know what you want if you don't tell them?",
                    "score": 2,
                    "upvotes": 2,
                    "downvotes": 0,
                    "created_utc": 1726945153.0,
                    "parent_id": "t3_1flmib3",
                    "link_id": "t3_1flmib3",
                    "permalink": "/r/KoboldAI/comments/1flmib3/is_there_a_way_to_make_kobold_cpp_work_with_the/lo90ojo/"
                },
                {
                    "id": "lo5ljsn",
                    "author": "seastatefive",
                    "body": "You should know that kobold UI starts to spaz out when your story reaches about 240,000 words.",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1726888220.0,
                    "parent_id": "t3_1flmib3",
                    "link_id": "t3_1flmib3",
                    "permalink": "/r/KoboldAI/comments/1flmib3/is_there_a_way_to_make_kobold_cpp_work_with_the/lo5ljsn/"
                },
                {
                    "id": "lobq8xg",
                    "author": "HadesThrowaway",
                    "body": "Could you elaborate on why it's a 'giant step back'? What can it not do that it could before?",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1726985884.0,
                    "parent_id": "t3_1flmib3",
                    "link_id": "t3_1flmib3",
                    "permalink": "/r/KoboldAI/comments/1flmib3/is_there_a_way_to_make_kobold_cpp_work_with_the/lobq8xg/"
                },
                {
                    "id": "loeroks",
                    "author": "Parogarr",
                    "body": "have you seen the henk version of KAI? Its UI is borderline perfect.\n\njust look I took a ss lol\n\n[https://i.imgur.com/jzDeDjd.png](https://i.imgur.com/jzDeDjd.png)\n\nBut it's all good because I DID end up getting it to work with kbcpp",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1727032803.0,
                    "parent_id": "t1_lobq8xg",
                    "link_id": "t3_1flmib3",
                    "permalink": "/r/KoboldAI/comments/1flmib3/is_there_a_way_to_make_kobold_cpp_work_with_the/loeroks/"
                }
            ]
        },
        {
            "id": "1flmgb3",
            "title": "Serving Tenebra30B on Horde",
            "author": "Sicarius_The_First",
            "score": 3,
            "upvotes": 3,
            "downvotes": 0,
            "num_comments": 0,
            "created_utc": 1726865993.0,
            "url": "https://www.reddit.com/r/KoboldAI/comments/1flmgb3/serving_tenebra30b_on_horde/",
            "permalink": "/r/KoboldAI/comments/1flmgb3/serving_tenebra30b_on_horde/",
            "selftext": "For about 1-2 days, hopefully the cards will survive the onslaught.",
            "comments": []
        },
        {
            "id": "1fkt4s6",
            "title": "Help! I'm trying to install Tavern and Kobold won't work",
            "author": "ZealousidealBoot3380",
            "score": 3,
            "upvotes": 3,
            "downvotes": 0,
            "num_comments": 14,
            "created_utc": 1726774190.0,
            "url": "https://www.reddit.com/r/KoboldAI/comments/1fkt4s6/help_im_trying_to_install_tavern_and_kobold_wont/",
            "permalink": "/r/KoboldAI/comments/1fkt4s6/help_im_trying_to_install_tavern_and_kobold_wont/",
            "selftext": "I am so frustrated I'm near tears. I am trying to follow this guide: [https://thetechdeck.hashnode.dev/how-to-use-tavern-ai-a-guide-for-beginners](https://thetechdeck.hashnode.dev/how-to-use-tavern-ai-a-guide-for-beginners)\n\nAnd I've done so far so good but then I get here:\n\n* First, install KoboldAI by following the step-by-step instructions for your operating system.\n\nAnd there ARE NOT step-by-step instructions. I clicked install requirements, installed it to the B drive. Then I clicked \"Play.bat\" and it says it can't find the folder. So I uninstalled and reinstalled \"install\\_requirements.bat\" in a subfolder. Pressed \"play.bat\" again and get hit with the same error:\n\nRuntimeError: Failed to import transformers.modeling\\_utils because of the following error (look up to see its traceback):\n\ncannot import name 'split\\_torch\\_state\\_dict\\_into\\_shards' from 'huggingface\\_hub'\n\n\n\n  \nI don't know how to code. I'm a slightly-above-average computer user. So all of this means nothing to me and I'm incredibly confused. Is there anyone who might know how to help me install it? or is there any easier way to install Tavern?",
            "comments": [
                {
                    "id": "lnyc9tb",
                    "author": "henk717",
                    "body": "We solved this on Discord while reddit was down, we:  \n1. Downloaded KoboldCpp  \n2. Downloaded Lunaris since that was a GGUF that fit on OP's GPU.  \n3. OP then explored KoboldAI Lite and is now happily exploring the capabilities with the bots.",
                    "score": 6,
                    "upvotes": 6,
                    "downvotes": 0,
                    "created_utc": 1726781211.0,
                    "parent_id": "t3_1fkt4s6",
                    "link_id": "t3_1fkt4s6",
                    "permalink": "/r/KoboldAI/comments/1fkt4s6/help_im_trying_to_install_tavern_and_kobold_wont/lnyc9tb/"
                },
                {
                    "id": "lnzc9e0",
                    "author": "torako",
                    "body": "How do you tell if a GGUF is going to fit on your GPU or not?",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1726794237.0,
                    "parent_id": "t1_lnyc9tb",
                    "link_id": "t3_1fkt4s6",
                    "permalink": "/r/KoboldAI/comments/1fkt4s6/help_im_trying_to_install_tavern_and_kobold_wont/lnzc9e0/"
                },
                {
                    "id": "lo1a74z",
                    "author": "henk717",
                    "body": "Typically its something like this for the Q4\\_K\\_S varients:  \n6GB of vram: Up to 8B  \n8GB of vram: Up to 11B  \n12GB of vram: Up to 13B  \n16GB of VRAM : Maybe 20B?  \n24GB of VRAM: 30B  \n32GB of VRAM: Rare so I don't know, maybe mixtral?  \n48GB of VRAM : 70B  \n80GB of VRAM: 120B",
                    "score": 2,
                    "upvotes": 2,
                    "downvotes": 0,
                    "created_utc": 1726831989.0,
                    "parent_id": "t1_lnzc9e0",
                    "link_id": "t3_1fkt4s6",
                    "permalink": "/r/KoboldAI/comments/1fkt4s6/help_im_trying_to_install_tavern_and_kobold_wont/lo1a74z/"
                },
                {
                    "id": "lo23f6n",
                    "author": "RealBiggly",
                    "body": "You should be able to run models larger than your GPU if you don't mind the bigger ones being a bit slower",
                    "score": 2,
                    "upvotes": 2,
                    "downvotes": 0,
                    "created_utc": 1726843260.0,
                    "parent_id": "t1_lnzc9e0",
                    "link_id": "t3_1fkt4s6",
                    "permalink": "/r/KoboldAI/comments/1fkt4s6/help_im_trying_to_install_tavern_and_kobold_wont/lo23f6n/"
                },
                {
                    "id": "lo277ie",
                    "author": "Ill_Yam_9994",
                    "body": "But unless you are the fastest reader in the world you probably don't need to fit the whole thing on your GPU. I find ~75% on the GPU perfectly fine and still faster than me reading at a leisurely thoughtful pace, and 50% often acceptable (i.e. 70B on 24GB GPU).",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1726844482.0,
                    "parent_id": "t1_lo1a74z",
                    "link_id": "t3_1fkt4s6",
                    "permalink": "/r/KoboldAI/comments/1fkt4s6/help_im_trying_to_install_tavern_and_kobold_wont/lo277ie/"
                },
                {
                    "id": "lo2ll14",
                    "author": "torako",
                    "body": "I think I'm doing something wrong because no matter how many gpu layers i specify, it doesn't seem to be using my vram.",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1726849121.0,
                    "parent_id": "t1_lo1a74z",
                    "link_id": "t3_1fkt4s6",
                    "permalink": "/r/KoboldAI/comments/1fkt4s6/help_im_trying_to_install_tavern_and_kobold_wont/lo2ll14/"
                },
                {
                    "id": "lo3kulh",
                    "author": "henk717",
                    "body": "What GPU do you have and which version of KoboldCpp?",
                    "score": 2,
                    "upvotes": 2,
                    "downvotes": 0,
                    "created_utc": 1726860527.0,
                    "parent_id": "t1_lo2ll14",
                    "link_id": "t3_1fkt4s6",
                    "permalink": "/r/KoboldAI/comments/1fkt4s6/help_im_trying_to_install_tavern_and_kobold_wont/lo3kulh/"
                },
                {
                    "id": "loaivsf",
                    "author": "torako",
                    "body": "rtx 3060, and the windows binary. i am running it in noavx2 mode because it crashes otherwise. when i load a model, my RAM usage goes up but my VRAM usage stays the same.",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1726964958.0,
                    "parent_id": "t1_lo3kulh",
                    "link_id": "t3_1fkt4s6",
                    "permalink": "/r/KoboldAI/comments/1fkt4s6/help_im_trying_to_install_tavern_and_kobold_wont/loaivsf/"
                },
                {
                    "id": "loc1w9t",
                    "author": "henk717",
                    "body": "Sounds like you didn't select a GPU backend. Try theÂ  _oldcpu.exe and use cuda in the regular mode on that one.",
                    "score": 2,
                    "upvotes": 2,
                    "downvotes": 0,
                    "created_utc": 1726993704.0,
                    "parent_id": "t1_loaivsf",
                    "link_id": "t3_1fkt4s6",
                    "permalink": "/r/KoboldAI/comments/1fkt4s6/help_im_trying_to_install_tavern_and_kobold_wont/loc1w9t/"
                },
                {
                    "id": "lod5up0",
                    "author": "torako",
                    "body": "Is there anything i need to do to enable cuda besides --gpulayers (number)?",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1727014884.0,
                    "parent_id": "t1_loc1w9t",
                    "link_id": "t3_1fkt4s6",
                    "permalink": "/r/KoboldAI/comments/1fkt4s6/help_im_trying_to_install_tavern_and_kobold_wont/lod5up0/"
                },
                {
                    "id": "lodymah",
                    "author": "henk717",
                    "body": "Not on the latest version, but originally --usecublas",
                    "score": 2,
                    "upvotes": 2,
                    "downvotes": 0,
                    "created_utc": 1727024150.0,
                    "parent_id": "t1_lod5up0",
                    "link_id": "t3_1fkt4s6",
                    "permalink": "/r/KoboldAI/comments/1fkt4s6/help_im_trying_to_install_tavern_and_kobold_wont/lodymah/"
                },
                {
                    "id": "logr5sg",
                    "author": "torako",
                    "body": "It's working, thanks!",
                    "score": 2,
                    "upvotes": 2,
                    "downvotes": 0,
                    "created_utc": 1727057692.0,
                    "parent_id": "t1_lodymah",
                    "link_id": "t3_1fkt4s6",
                    "permalink": "/r/KoboldAI/comments/1fkt4s6/help_im_trying_to_install_tavern_and_kobold_wont/logr5sg/"
                },
                {
                    "id": "loi2s8i",
                    "author": "henk717",
                    "body": "If that made it work please update to the latest KoboldCpp.",
                    "score": 2,
                    "upvotes": 2,
                    "downvotes": 0,
                    "created_utc": 1727085359.0,
                    "parent_id": "t1_logr5sg",
                    "link_id": "t3_1fkt4s6",
                    "permalink": "/r/KoboldAI/comments/1fkt4s6/help_im_trying_to_install_tavern_and_kobold_wont/loi2s8i/"
                },
                {
                    "id": "loj0e44",
                    "author": "torako",
                    "body": "No not the command, your previous instructions. I just couldn't get to my computer when I asked that question and wanted to make sure i had the command right when I was able to do it.",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1727100694.0,
                    "parent_id": "t1_loi2s8i",
                    "link_id": "t3_1fkt4s6",
                    "permalink": "/r/KoboldAI/comments/1fkt4s6/help_im_trying_to_install_tavern_and_kobold_wont/loj0e44/"
                }
            ]
        },
        {
            "id": "1fkrnzx",
            "title": "Matching GPU vs mixed",
            "author": "CMDR_CHIEF_OF_BOOTY",
            "score": 3,
            "upvotes": 3,
            "downvotes": 0,
            "num_comments": 5,
            "created_utc": 1726769761.0,
            "url": "https://www.reddit.com/r/KoboldAI/comments/1fkrnzx/matching_gpu_vs_mixed/",
            "permalink": "/r/KoboldAI/comments/1fkrnzx/matching_gpu_vs_mixed/",
            "selftext": "I have a 3080ti and I'm looking to get a second GPU. Am I better off getting another matching used 3080ti or am I fine getting something like a 16gb 4060ti or maybe even a 7900xtx?\n\n Mainly asking cause the 3080ti is really fast until I try using a larger model or context size that has to load stuff from ram then it slows to a crawl.\n\nOther specs: \nCPU: And 5800x3d\n64gb Corsair 3200mhz ram\n\nApologizes if this gets asked alot.",
            "comments": [
                {
                    "id": "lnygkw9",
                    "author": "wh33t",
                    "body": "I would avoid mixing AMD and Nvidia together (would avoid AMD altogether personally). If you want all the speed and compatibility you can get, you can't beat CUDA from Nvidia.",
                    "score": 2,
                    "upvotes": 2,
                    "downvotes": 0,
                    "created_utc": 1726782640.0,
                    "parent_id": "t3_1fkrnzx",
                    "link_id": "t3_1fkrnzx",
                    "permalink": "/r/KoboldAI/comments/1fkrnzx/matching_gpu_vs_mixed/lnygkw9/"
                },
                {
                    "id": "lnyr3et",
                    "author": "Linkpharm2",
                    "body": "Go for a single 3090 or double 3090. Mixing will slow down one side, and 3090 is the best value.",
                    "score": 2,
                    "upvotes": 2,
                    "downvotes": 0,
                    "created_utc": 1726786346.0,
                    "parent_id": "t3_1fkrnzx",
                    "link_id": "t3_1fkrnzx",
                    "permalink": "/r/KoboldAI/comments/1fkrnzx/matching_gpu_vs_mixed/lnyr3et/"
                },
                {
                    "id": "lo29il6",
                    "author": "Ill_Yam_9994",
                    "body": "Get something as new or newer so it supports all the same low level features, and of the same brand.\n\n16GB 4060ti or used 3090 would be good.\n\n7900xtx will be annoying.",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1726845229.0,
                    "parent_id": "t3_1fkrnzx",
                    "link_id": "t3_1fkrnzx",
                    "permalink": "/r/KoboldAI/comments/1fkrnzx/matching_gpu_vs_mixed/lo29il6/"
                },
                {
                    "id": "lo52dnf",
                    "author": "henk717",
                    "body": "The slowest GPU tends to be the speed/compatibility they take but otherwise mixing is fine. As others mentioned you want an Nvidia card for the best results. Occam the Vulkan dev does mix with various brands so I know Vulkan can do it. But compared to Cuda you'd trade quite a bit of speed and compatibility.Â ",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1726879913.0,
                    "parent_id": "t3_1fkrnzx",
                    "link_id": "t3_1fkrnzx",
                    "permalink": "/r/KoboldAI/comments/1fkrnzx/matching_gpu_vs_mixed/lo52dnf/"
                },
                {
                    "id": "lo57rfc",
                    "author": "CMDR_CHIEF_OF_BOOTY",
                    "body": "That's what I was worried about with the 4060ti, people seemed to say the memory bandwidth was a massive bottleneck. I ended up getting another matching 3080ti, figured it's the best guarantee for the least headaches, and should be really fast still.",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1726882158.0,
                    "parent_id": "t1_lo52dnf",
                    "link_id": "t3_1fkrnzx",
                    "permalink": "/r/KoboldAI/comments/1fkrnzx/matching_gpu_vs_mixed/lo57rfc/"
                }
            ]
        },
        {
            "id": "1fkkkgl",
            "title": "Did a little benchmark to determine some general guidelines on what settings to prioritize for better speed in my 8GB setup. Quick final conclusions and derived guideline at the bottom.",
            "author": "input_a_new_name",
            "score": 16,
            "upvotes": 16,
            "downvotes": 0,
            "num_comments": 12,
            "created_utc": 1726751712.0,
            "url": "https://www.reddit.com/r/KoboldAI/comments/1fkkkgl/did_a_little_benchmark_to_determine_some_general/",
            "permalink": "/r/KoboldAI/comments/1fkkkgl/did_a_little_benchmark_to_determine_some_general/",
            "selftext": "The wiki page on github provides very useful overview over all the different parameters, but sort of leaves it to the user to figure out what's best to use in general or not and when. I did a little test to see in general what settings are better to prioritize for speed in my 8GB setup. Just sharing my observations.\n\nUsing a Q5\\_K\\_M of LLama 3.0 based model on RTX 4060ti 8GB.\n\n**Baseline setting**: 8k context, 35/35 layers on GPU, MMQ ON, FlashAttention ON, KV Cache quantization OFF, Low VRAM OFF\n\n[baseline results](https://preview.redd.it/gu5uq51ugqpd1.png?width=261&format=png&auto=webp&s=3f733f83896da2c39fcff86a362d5ff8054c9aac)\n\n\n\n**Test 1 - on/off parameters and KV cache quantization.**\n\n**MMQ on vs off**  \nObservations: processing speed suffers drastically without MMQ (\\~25% difference), generation speed unaffected. VRAM difference less than 100mb.  \nConclusion: preferable to keep ON\n\n[MMQ OFF](https://preview.redd.it/w3pfbrhddqpd1.png?width=254&format=png&auto=webp&s=d9480b4f8ae9b125507bea7fca1335ed1e84e48b)\n\n\n\n**Flash Attention on vs off**  \nObservations: OFF increases VRAM consumption by 400\\~500mb, reduces processing speed by a whopping 50%! Generation speed also slightly reduced.  \nConclusion: preferable to keep ON when the model supports it!\n\n[FlashAttention OFF](https://preview.redd.it/qtbntynafqpd1.png?width=260&format=png&auto=webp&s=bbe6dd2370e92467579d13e2e931b81e1f12042c)\n\n\n\n**Low VRAM on vs off**  \nObservations: at same 8k context - reduced VRAM consumption by \\~1gb. Processing speed reduced by \\~30%, generation speed reduced by 430%!!!  \nTried increasing context to 16k, 24k and 32k - VRAM consumption did not change (i'm only including 8k and 24k screenshots to reduce bloat). Processing and generation decrease exponentially with higher context. Increasing batch size from 512 to 2048 improved speed marginally, but ate up most of the freed up 1gb VRAM  \n  \nConclusion 1: the parameter lowers VRAM consumption by a flat 1gb (in my case) with an 8B model, and drastically decreases (annihilates) processing and generation speed. Allows to set higher context values without increasing VRAM requirement, but the speed suffers even more, exponentially. Increasing batch size to 2048 improved processing speed at 24k context by \\~25%, but at 8k the difference was negligible.   \nConclusion 2: not worth it as a means to increase context if speed is important. If whole model can be loaded on GPU alone, definitely best kept off.\n\n[Low VRAM ON 8k context](https://preview.redd.it/gq4vyrluiqpd1.png?width=262&format=png&auto=webp&s=cf2ce3d6f12a02d020b863158d7d251197f25f15)\n\n[Low VRAM ON 24k context](https://preview.redd.it/3ao43pwfmqpd1.png?width=252&format=png&auto=webp&s=8268597d10b696011c67c0db2b7479149b0674c4)\n\n[Low VRAM ON 24k context 2048 batch size](https://preview.redd.it/xd9i7lc1mqpd1.png?width=257&format=png&auto=webp&s=4ec8b47de873e0beba9f76831e5e3030ed2175eb)\n\n\n\n**Cache quantization off vs 8bit vs 4bit**  \nObservations: compared to off, 8bit cache reduced VRAM consumption by \\~500mb. 4bit cache reduced it further by another 100\\~200 mb. Processing and generation speed unaffected, or difference is negligible.\n\nConclusions: 8bit quantization of KV cache lowers VRAM consumption by a significant amount. 4bit lowers it further, but by a less impressive amount. However, due to how reportedly it lobotomizes lower models like Llama 3.0 and Mistral Nemo, probably best kept OFF unless the model is reported to work fine with it. \n\n[4bit cache](https://preview.redd.it/tkl1c6j6qqpd1.png?width=255&format=png&auto=webp&s=add21737466e699ac1e24d4a8cc3cec15a5bd224)\n\n  \n**Test 2 - importance of offloaded layers vs batch size**  \nFor this test I offloaded 5 layers to CPU and increased context to 16k. The point of the test is to determine whether it's better to lower batch size to cram an extra layer or two onto GPU vs increasing batch size to a high amount.\n\nObservations: loading 1 extra layer over increasing batch from 512 to 1024 had a bigger positive impact on performance. Loading yet more layers kept increasing the total performance even as batch size kept getting lowered. At 35/35 i tested lowest batch settings. 128 still performed well (behind 256, but not by far), but 64 slowed processing down significantly, while 32 annihilated it.\n\nConclusion: lowering batch size from 512 to 256 freed up \\~200mb VRAM. Going down to 128 didn't free up more than 50 extra mb. 128 is the lowest point at which the decrease in processing speed is positively offset by loading another layer or two onto GPU. 64, 32 and 1 tank performance for NO VRAM gain. 1024 batch size increases processing speed just a little, but at the cost of extra \\~200mb VRAM, making it not worth it if instead more layers can be loaded first.\n\n[30\\/35 layers, 512 batch](https://preview.redd.it/4ds5ikjcsqpd1.png?width=262&format=png&auto=webp&s=4a3dd542b0ce03dfe0ef040c849755a2c8797a68)\n\n[30\\/35 layers 1024 batch](https://preview.redd.it/0o3ciuvgsqpd1.png?width=263&format=png&auto=webp&s=95b25451f30bf64b137593648bcc7dbc4f418469)\n\n[32\\/35 layers, 256 batch](https://preview.redd.it/84h54n7nsqpd1.png?width=273&format=png&auto=webp&s=24dc53676d3effca7384722eedbf4507a67a67a9)\n\n[35\\/35 layers, 256 batch](https://preview.redd.it/l773gqoysqpd1.png?width=267&format=png&auto=webp&s=e42e101d44dd559daa978960a398ba006497cc03)\n\n[35\\/35 layers, 64 batch](https://preview.redd.it/8x6ds4u7tqpd1.png?width=253&format=png&auto=webp&s=31ca2616194685049c11de0d00e58925b3b2b18a)\n\n[35\\/35 layers, 32 batch](https://preview.redd.it/5qpy27ahtqpd1.png?width=239&format=png&auto=webp&s=f47db1793dc038efe4fce3bcd5c865cd97f29c61)\n\n\n\n**Test 3 - Low VRAM on vs off on a 20B** **Q4\\_K\\_M model at 4k context with split load**\n\nObservations: By default, i can load 27/65 layers onto GPU. At same 27 layers, Low VRAM ON reduced VRAM consumption by 2.2gb instead of 1gb like on an 8b model! I was able to fit 13 more layers onto GPU like this, totaling 40/65. The processing speed got a little faster, but the generation speed remained much lower, and thus overall speed remained worse than with the setting OFF at 27 layers! \n\nConclusion: Low VRAM ON was not worth it in situation where \\~40% of the model was loaded on GPU before and \\~60% after.\n\n[27\\/65 layers, Low VRAM OFF](https://preview.redd.it/xlcyzonj3rpd1.png?width=244&format=png&auto=webp&s=19ffc5bf5630e1bd812934f9869dc1777f6090f8)\n\n[27\\/65 layers, Low VRAM ON](https://preview.redd.it/lrwpzlj64rpd1.png?width=244&format=png&auto=webp&s=b0b0af165ab9cba68ce3aa495763fd3bb1292d47)\n\n[34\\/65 layers, Low VRAM ON](https://preview.redd.it/595qva6r4rpd1.png?width=240&format=png&auto=webp&s=d75aac4382dd144c5ca4ccb6abf3817f0d4ea640)\n\n[40\\/65 layers Low VRAM ON](https://preview.redd.it/8jm65j085rpd1.png?width=247&format=png&auto=webp&s=9a5c4e791d8dd1444f9aff6b75172d0c7c65515c)\n\n\n\n**Test 4 - Low VRAM on vs off on a 12B** **Q4\\_K\\_M model at 16k context**\n\nObservation: Finally discovered the case when Low VRAM ON provided a performance GAIN... of a \"whopping\" 4% total!  \n\nConclusion: Low VRAM ON is only useful in a very specific scenario when without it at least around 1/4th\\~1/3rd of the model is offloaded to CPU but with it all layers can fit on the GPU. And the worst part is, going to 31/43 with 256 batch size already gives a better performance boost than this setting at 43/43 layers with 512 batch...\n\n[30\\/43 layers, Low VRAM OFF, batch size 512](https://preview.redd.it/8zyjwl49frpd1.png?width=250&format=png&auto=webp&s=0571829b74532809fe501d6d17b68469cb4b040e)\n\n[43\\/43 layers, Low VRAM ON, batch size 512](https://preview.redd.it/t1atth8jbrpd1.png?width=246&format=png&auto=webp&s=66332336958bea0cec1700b5415a0718965c7956)\n\n\n\n**Final conclusions**\n\nIn a scenario where VRAM is scarce (8gb), priority should be given to fitting as many layers onto GPU as possible first, over increasing batch size. Batch sizes lower than 128 are definitely not worth it, 128 probably not worth it either. 256-512 seems to be the sweet spot.   \n  \nMMQ is better kept ON at least on RTX 4060 TI, improving the processing speed considerably (\\~30%) while costing less than 100mb VRAM.\n\nFlash Attention definitely best kept ON for any model that isn't known to have issues with it, major increase in processing speed and crazy VRAM savings (400\\~500mb)\n\nKV cache quantization: 8bit gave substantial VRAM savings (\\~500mb), 4bit provided \\~150mb further savings. However, people claim that this negatively impacts the output of small models like Llama 8b and Mistral 12b (severely in some cases), so probably avoid this setting unless absolutely certain.\n\nLow VRAM: After messing with this option A LOT, i came to the conclusion that it SUCKS and should be avoided. Only one very specific situation managed to squeeze an actual tiny performance boost out of it, but in all other cases where at least around 1/3 of the model fits on GPU already, the performance was considerably better without it. Perhaps it's a different story when even less than 1/3 of the model fits on the gpu, but i didn't test that far.\n\n\n\n**Derived guideline**  \nGeneral steps to find optimal settings for best performance are:   \n1. Turn on MMQ\n\n2. Turn on Flash Attention if the model isn't known to have issues with it\n\n3. If you're on Windows and have an Nvidia GPU - in control panel, make sure that CUDA fallback policy is set to Prefer No System Fallback (this will cause the model to crash instead of dipping into pagefile, this makes it easier to benchmark)\n\n4. Set batch size to 256 and find the maximum number of layers you fit on gpu at your chosen context length without the benchmark crashing\n\n5. At the exact number of layers you ended up with, test if you can increase batch size to 512\n\n6. In case you need more speed, stick with 256 batch size and lower context length, use the freed-up VRAM to cram more layers in, even a couple layers can make a noticeable difference.  \n6.1 In case you need more context, reduce amount of GPU layers and accept the speed penalty.\n\n7. Quantizing KV Cache can provide a significant VRAM reduction, but this option is known to be highly unstable, especially on smaller models, so probably don't use this unless you know what you're doing or you're reading this in 2027 and \"they\" have already optimized their models to work well with 8bit cache.\n\n8. Don't even think about turning Low VRAM ON!!! You have been warned about how useless or outright nasty it is!!!",
            "comments": [
                {
                    "id": "lnwnvl6",
                    "author": "FreedomHole69",
                    "body": "Just did some testing with my setup. Low vram I go from processing at 66t/s, gen at 4.5 ,and total at 2.3 to 259 , 6.47, and 5.2. this is using a 12b quant that just barely fits all the layers. Just to say, at least for my setup it made a huge difference.",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1726758461.0,
                    "parent_id": "t3_1fkkkgl",
                    "link_id": "t3_1fkkkgl",
                    "permalink": "/r/KoboldAI/comments/1fkkkgl/did_a_little_benchmark_to_determine_some_general/lnwnvl6/"
                },
                {
                    "id": "lnyzxru",
                    "author": "henk717",
                    "body": "Pretty much matches our own findings, low vram is an old option and isn't recommended for anyone except the few that asked us to keep it. It ensures that the context isn't running on the GPU. This helps with extreme context cases but is generally a much slower option. It mimics the behavior that used to be the case when you didn't offload every layer on old versions of KoboldCpp.\n\n\nMMQ we indeed recommend to keep on which is why it does this automatically in the UI.\n\n\nFlashAttention has been hit and miss in the community which is why we don't default to it. Especially for Nvidia cards with full offloads its been great and a must have but from AMD users last time I checked with them it causes a lot of slowdowns. This is also an avoid if you use Vulkan for example although I don't know if Vulkan supports it. There are some edge cases but with models like Gemma 2 that are known not to support it its still fine if you enable FlashAttention and i'd even go as far as recommending it. Yes, the model didn't support it but it will just warn you of this and turn it back off. This way when FlashAttention for a model is added you will begin to use it.",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1726789588.0,
                    "parent_id": "t3_1fkkkgl",
                    "link_id": "t3_1fkkkgl",
                    "permalink": "/r/KoboldAI/comments/1fkkkgl/did_a_little_benchmark_to_determine_some_general/lnyzxru/"
                },
                {
                    "id": "lnzeqgg",
                    "author": "Space_Pirate_R",
                    "body": "What you've said matches my experience, but I don't think it's true in general that lowvram \"lowers VRAM consumption by a flat 1gb.\"  It should lower VRAM usage by the size of the KV cache (size of which is proportional to context). I'm not doubting that it was a flat 1GB in your case, just saying I don't think it's the case in general.\n\nI think if you were to test with more VRAM or a smaller model (very low parameters or low quantization) you would find that different context sizes occupy different amounts of VRAM, and the \"lowvram\" option frees up whatever that amount is, by putting it in RAM instead of VRAM.\n\nPersonally I find the lowvram option useful, because with it my VRAM fits a Nemo 12B model at Q4\\_K\\_S, whereas without it I can only fit IQ3\\_\\_M.  It's a little bit slower, but the difference in smarts seems well worth it.",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1726795171.0,
                    "parent_id": "t3_1fkkkgl",
                    "link_id": "t3_1fkkkgl",
                    "permalink": "/r/KoboldAI/comments/1fkkkgl/did_a_little_benchmark_to_determine_some_general/lnzeqgg/"
                },
                {
                    "id": "lnwosv5",
                    "author": "input_a_new_name",
                    "body": "i see, what're your specs?",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1726758773.0,
                    "parent_id": "t1_lnwnvl6",
                    "link_id": "t3_1fkkkgl",
                    "permalink": "/r/KoboldAI/comments/1fkkkgl/did_a_little_benchmark_to_determine_some_general/lnwosv5/"
                },
                {
                    "id": "lo0f32c",
                    "author": "input_a_new_name",
                    "body": "It lowered it by a flat 1 with 8b model and by 2.2 with a 20b model. I wasn't making a case that it's the definitive amount, i simply recorded what it did while i was testing",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1726812055.0,
                    "parent_id": "t1_lnzeqgg",
                    "link_id": "t3_1fkkkgl",
                    "permalink": "/r/KoboldAI/comments/1fkkkgl/did_a_little_benchmark_to_determine_some_general/lo0f32c/"
                },
                {
                    "id": "lnwq5hx",
                    "author": "FreedomHole69",
                    "body": "1070 8gb, i5 4690k, 16gb ddr3. Usually with a Nemo 12b at q4_km",
                    "score": 2,
                    "upvotes": 2,
                    "downvotes": 0,
                    "created_utc": 1726759224.0,
                    "parent_id": "t1_lnwosv5",
                    "link_id": "t3_1fkkkgl",
                    "permalink": "/r/KoboldAI/comments/1fkkkgl/did_a_little_benchmark_to_determine_some_general/lnwq5hx/"
                },
                {
                    "id": "lnwqtg4",
                    "author": "FreedomHole69",
                    "body": "I think a q5 on 8b can fit enough cache to make low vram not worth it. If you went up to Q6, low vram might make a bigger difference.",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1726759448.0,
                    "parent_id": "t1_lnwosv5",
                    "link_id": "t3_1fkkkgl",
                    "permalink": "/r/KoboldAI/comments/1fkkkgl/did_a_little_benchmark_to_determine_some_general/lnwqtg4/"
                },
                {
                    "id": "lnx96xf",
                    "author": "input_a_new_name",
                    "body": "hmm, i see, then it's probably not contrary to my findings, i mentioned that the story might be different if you load even less than 1/3rd of the layers onto gpu.",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1726765323.0,
                    "parent_id": "t1_lnwqtg4",
                    "link_id": "t3_1fkkkgl",
                    "permalink": "/r/KoboldAI/comments/1fkkkgl/did_a_little_benchmark_to_determine_some_general/lnx96xf/"
                },
                {
                    "id": "lnxaz85",
                    "author": "FreedomHole69",
                    "body": "This is loading 100% of the layers onto the GPU.",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1726765883.0,
                    "parent_id": "t1_lnx96xf",
                    "link_id": "t3_1fkkkgl",
                    "permalink": "/r/KoboldAI/comments/1fkkkgl/did_a_little_benchmark_to_determine_some_general/lnxaz85/"
                },
                {
                    "id": "lo0es86",
                    "author": "input_a_new_name",
                    "body": "How are you loading all layers of q5 12b onto 8gb vram?",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1726811870.0,
                    "parent_id": "t1_lnxaz85",
                    "link_id": "t3_1fkkkgl",
                    "permalink": "/r/KoboldAI/comments/1fkkkgl/did_a_little_benchmark_to_determine_some_general/lo0es86/"
                },
                {
                    "id": "lo0ew5d",
                    "author": "input_a_new_name",
                    "body": "Ah wait, i'm stupid, using the low vram option, duh. I meant the anount of layers without the option enabled",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1726811938.0,
                    "parent_id": "t1_lnxaz85",
                    "link_id": "t3_1fkkkgl",
                    "permalink": "/r/KoboldAI/comments/1fkkkgl/did_a_little_benchmark_to_determine_some_general/lo0ew5d/"
                },
                {
                    "id": "lo0zd3l",
                    "author": "FreedomHole69",
                    "body": "Q4, sorry.",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1726825880.0,
                    "parent_id": "t1_lo0es86",
                    "link_id": "t3_1fkkkgl",
                    "permalink": "/r/KoboldAI/comments/1fkkkgl/did_a_little_benchmark_to_determine_some_general/lo0zd3l/"
                }
            ]
        },
        {
            "id": "1fkdfe0",
            "title": "Koboldcpp and samplers",
            "author": "dengopaiv",
            "score": 1,
            "upvotes": 1,
            "downvotes": 0,
            "num_comments": 2,
            "created_utc": 1726723221.0,
            "url": "https://www.reddit.com/r/KoboldAI/comments/1fkdfe0/koboldcpp_and_samplers/",
            "permalink": "/r/KoboldAI/comments/1fkdfe0/koboldcpp_and_samplers/",
            "selftext": "Hi,\nI decided to test out the xtc sampler on koboldcpp. I somehow made it to the point where an 8b parameter model, lumimaid, so far, produces coherent output, but basically always the same text. Would anyone be so kind as to share some sampler settings that would start producing variability again and maybe some reading on which I could educate myself on what samplers are, how they function and why they do so.\nps. I disabled most of the samplers, other than dry and xtc.",
            "comments": [
                {
                    "id": "lnuz327",
                    "author": "BangkokPadang",
                    "body": "Generally, LLM models output a ranked list of the most likely tokens, called logits.\n\nSamplers take this list of logits and whittle them down to a smaller and smaller list depending on how they work individually, and then often either randomly select one from the final short-list (or sometimes choose one based on their final weighted ranks, so more like â€˜semi-randomâ€™).\n\nTemperature isnâ€™t a â€˜samplerâ€™ but itâ€™s important to understand that it basically adjusts the â€˜randomnessâ€™ of the output logits, effectively reducing the highest rank and increasing the lowest ranks so that the range of chosen logit rankings are closer together, and thus more of the lower ranked logits end up likely to be chosen.\n\nTop P, for example, lets you set a â€˜goalâ€™ rank (letâ€™s say 0.8) and then it will add up the most likely tokens until their weights equal to your goal. So A might be 0.4, be B might be 0.2, C might be 0.2, and D might be .15. If your Top P is set to 0.8, then it will choose from A, B, and C because they add up to exactly 0.8, but then will ignore D and everything below that.\n\nTop K, lets you set a number, letâ€™s say 40, which will take the highest ranked 40 logits.\n\nFor the remaining samplers, if you use a frontend like SillyTavern to chat with koboldcppâ€™s API, itâ€™s sampler settings menu lets you hover over a little question mark icon next to each sampler, and it gives a little summary of the math of how that sampler works.\n\nYou can also google the name of the more complicated samplers (or even basic ones like typical P, Min P, Top A, etc.) like Mirostat, Smooth Sampling (aka quadratic smoothing) and DRY (Donâ€™t Repeat Yourself) sampling, etc. and read through their whitepapers to see explanations with the math and little charts/examples of how they select logits.\n\nYou can generally assume that the more samplers you start using, the smaller the final list of logits being chosen from is, which naturally starts making them repeat and/or produce similar responses. If you were trying to write a sentence and could choose between 3 words, or choose between 20 words, youâ€™d start repeating yourself much faster if you only had 3 words to pick from than if you had 20. Same thing with the model and sampling logits.",
                    "score": 2,
                    "upvotes": 2,
                    "downvotes": 0,
                    "created_utc": 1726727925.0,
                    "parent_id": "t3_1fkdfe0",
                    "link_id": "t3_1fkdfe0",
                    "permalink": "/r/KoboldAI/comments/1fkdfe0/koboldcpp_and_samplers/lnuz327/"
                },
                {
                    "id": "lnvdlhg",
                    "author": "dengopaiv",
                    "body": "Thank you so much for your kind comment. It makes things significantly better to understand.\nI've been a bit slow in adapting to st, because of it's interface being rather more cluttered and a bit of work to navigate with a screen reader, but I'm slowly getting a hang of it. Kccp is just considerably easier.\nThanks again.",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1726738134.0,
                    "parent_id": "t1_lnuz327",
                    "link_id": "t3_1fkdfe0",
                    "permalink": "/r/KoboldAI/comments/1fkdfe0/koboldcpp_and_samplers/lnvdlhg/"
                }
            ]
        },
        {
            "id": "1fk5vam",
            "title": "Combining a 3090 and 3060 for Kobold RP/chatting",
            "author": "neonstingray17",
            "score": 5,
            "upvotes": 5,
            "downvotes": 0,
            "num_comments": 2,
            "created_utc": 1726699509.0,
            "url": "https://www.reddit.com/r/KoboldAI/comments/1fk5vam/combining_a_3090_and_3060_for_kobold_rpchatting/",
            "permalink": "/r/KoboldAI/comments/1fk5vam/combining_a_3090_and_3060_for_kobold_rpchatting/",
            "selftext": "I'm building a PC to play with local LLMs for RP with the intent of using Koboldcpp and SillyTavern.  My acquired parts are a 3090 Kingpin Hydro Copper on an ASRock z690 Aqua with 64gb DDR5 and a 12900K.  From what I've read the newer versions of Kobold have gotten better at supporting multiple GPUs.  Since I have two PCI 5.0 x16 slots, I was thinking of adding a 12gb 3060 just for the extra vram.  I'm fully aware that the memory bandwidth on a 3060 is about 40% that of a 3090, but I was under the impression that even with the lower bandwidth, the additional vram would still give a noticeable advantage in loading models for inference vs a single 3090 with the rest off loaded to the CPU.  Is this the case?  Thanks!",
            "comments": [
                {
                    "id": "lntqkul",
                    "author": "henk717",
                    "body": "Yes, a 3090 + 3060 setup beats a 3090 + CPU setup. But, it will not be as fast as running solely on the 3090 if the model can fit in full. Won't require anything special other than having them both functional, our software will take care of the combo.",
                    "score": 6,
                    "upvotes": 6,
                    "downvotes": 0,
                    "created_utc": 1726706962.0,
                    "parent_id": "t3_1fk5vam",
                    "link_id": "t3_1fk5vam",
                    "permalink": "/r/KoboldAI/comments/1fk5vam/combining_a_3090_and_3060_for_kobold_rpchatting/lntqkul/"
                },
                {
                    "id": "lntrm9i",
                    "author": "neonstingray17",
                    "body": "Great, thank you!",
                    "score": 2,
                    "upvotes": 2,
                    "downvotes": 0,
                    "created_utc": 1726707358.0,
                    "parent_id": "t1_lntqkul",
                    "link_id": "t3_1fk5vam",
                    "permalink": "/r/KoboldAI/comments/1fk5vam/combining_a_3090_and_3060_for_kobold_rpchatting/lntrm9i/"
                }
            ]
        },
        {
            "id": "1fjl5w7",
            "title": "[Usermod] Chat with random character",
            "author": "freehuntx",
            "score": 6,
            "upvotes": 6,
            "downvotes": 0,
            "num_comments": 9,
            "created_utc": 1726636263.0,
            "url": "https://www.reddit.com/r/KoboldAI/comments/1fjl5w7/usermod_chat_with_random_character/",
            "permalink": "/r/KoboldAI/comments/1fjl5w7/usermod_chat_with_random_character/",
            "selftext": "I wrote a custom userscript which loads a random character from [chub.ai](http://chub.ai)  \n\nGist: [https://gist.github.com/freehuntx/331b1ce469b8be6d342c41054140602c](https://gist.github.com/freehuntx/331b1ce469b8be6d342c41054140602c)  \n\nJust paste the code in: Settings > Advanced > Apply User Mod  \n\nThen a button should appear when you open a new chat.  \n\nWould like to get feedback to improve the script :)",
            "comments": [
                {
                    "id": "lnozh5e",
                    "author": "International-Try467",
                    "body": "The only problem I have with this is how stupid some bot makers are that they still use the outdated pygmalion JSON or WI++ context formatting",
                    "score": 2,
                    "upvotes": 2,
                    "downvotes": 0,
                    "created_utc": 1726638694.0,
                    "parent_id": "t3_1fjl5w7",
                    "link_id": "t3_1fjl5w7",
                    "permalink": "/r/KoboldAI/comments/1fjl5w7/usermod_chat_with_random_character/lnozh5e/"
                },
                {
                    "id": "lnpb7rm",
                    "author": "Virtual-End-9003",
                    "body": "I love the concept but some botmakers make some sick unforgivable bots, maybe add some filter where the user can block certain tagged bots",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1726646685.0,
                    "parent_id": "t3_1fjl5w7",
                    "link_id": "t3_1fjl5w7",
                    "permalink": "/r/KoboldAI/comments/1fjl5w7/usermod_chat_with_random_character/lnpb7rm/"
                },
                {
                    "id": "lnozjpo",
                    "author": "freehuntx",
                    "body": "Maybe i can fix it by skipping such characters",
                    "score": 2,
                    "upvotes": 2,
                    "downvotes": 0,
                    "created_utc": 1726638738.0,
                    "parent_id": "t1_lnozh5e",
                    "link_id": "t3_1fjl5w7",
                    "permalink": "/r/KoboldAI/comments/1fjl5w7/usermod_chat_with_random_character/lnozjpo/"
                },
                {
                    "id": "lnpg2a3",
                    "author": "DishObjective2264",
                    "body": "Which formatting is actual nowadays?",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1726650212.0,
                    "parent_id": "t1_lnozh5e",
                    "link_id": "t3_1fjl5w7",
                    "permalink": "/r/KoboldAI/comments/1fjl5w7/usermod_chat_with_random_character/lnpg2a3/"
                },
                {
                    "id": "lnpbghd",
                    "author": "freehuntx",
                    "body": "Good idea!",
                    "score": 2,
                    "upvotes": 2,
                    "downvotes": 0,
                    "created_utc": 1726646860.0,
                    "parent_id": "t1_lnpb7rm",
                    "link_id": "t3_1fjl5w7",
                    "permalink": "/r/KoboldAI/comments/1fjl5w7/usermod_chat_with_random_character/lnpbghd/"
                },
                {
                    "id": "lnr4fpt",
                    "author": "FaceDeer",
                    "body": "One quick and easy way of filtering that might help here would be to let the user set a threshold for number of favourites. So you'd pick only from characters with at least X number of favourites.",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1726675790.0,
                    "parent_id": "t1_lnozjpo",
                    "link_id": "t3_1fjl5w7",
                    "permalink": "/r/KoboldAI/comments/1fjl5w7/usermod_chat_with_random_character/lnr4fpt/"
                },
                {
                    "id": "lnph8f8",
                    "author": "International-Try467",
                    "body": "The instruct and response tags with plaintext",
                    "score": 3,
                    "upvotes": 3,
                    "downvotes": 0,
                    "created_utc": 1726651063.0,
                    "parent_id": "t1_lnpg2a3",
                    "link_id": "t3_1fjl5w7",
                    "permalink": "/r/KoboldAI/comments/1fjl5w7/usermod_chat_with_random_character/lnph8f8/"
                },
                {
                    "id": "lnppa0s",
                    "author": "StoopPizzaGoop",
                    "body": "Is there an example we could look at someplace",
                    "score": 2,
                    "upvotes": 2,
                    "downvotes": 0,
                    "created_utc": 1726656359.0,
                    "parent_id": "t1_lnph8f8",
                    "link_id": "t3_1fjl5w7",
                    "permalink": "/r/KoboldAI/comments/1fjl5w7/usermod_chat_with_random_character/lnppa0s/"
                },
                {
                    "id": "lnq75hd",
                    "author": "DishObjective2264",
                    "body": "Oh... thought the deal was much different.\nThanks anyway ðŸ™‚",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1726664596.0,
                    "parent_id": "t1_lnph8f8",
                    "link_id": "t3_1fjl5w7",
                    "permalink": "/r/KoboldAI/comments/1fjl5w7/usermod_chat_with_random_character/lnq75hd/"
                }
            ]
        },
        {
            "id": "1fjfy9m",
            "title": "differences between koboldai and koboldcpp?",
            "author": "torako",
            "score": 4,
            "upvotes": 4,
            "downvotes": 0,
            "num_comments": 18,
            "created_utc": 1726619893.0,
            "url": "https://www.reddit.com/r/KoboldAI/comments/1fjfy9m/differences_between_koboldai_and_koboldcpp/",
            "permalink": "/r/KoboldAI/comments/1fjfy9m/differences_between_koboldai_and_koboldcpp/",
            "selftext": "this is probably a dumb question but i have koboldai installed on my computer and was wondering what the difference is between that and koboldcpp. should i switch to koboldcpp?\n\ni tried to google it before posting but google wasn't terribly helpful.",
            "comments": [
                {
                    "id": "lntqgg4",
                    "author": "None",
                    "body": "[removed]",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1726706916.0,
                    "parent_id": "t3_1fjfy9m",
                    "link_id": "t3_1fjfy9m",
                    "permalink": "/r/KoboldAI/comments/1fjfy9m/differences_between_koboldai_and_koboldcpp/lntqgg4/"
                },
                {
                    "id": "lo396on",
                    "author": "Parogarr",
                    "body": "KoboldCPP doesn't seem to load exllama v2 models though.",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1726856717.0,
                    "parent_id": "t1_lntqgg4",
                    "link_id": "t3_1fjfy9m",
                    "permalink": "/r/KoboldAI/comments/1fjfy9m/differences_between_koboldai_and_koboldcpp/lo396on/"
                },
                {
                    "id": "lo397zy",
                    "author": "Parogarr",
                    "body": "Only GGUF",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1726856729.0,
                    "parent_id": "t1_lo396on",
                    "link_id": "t3_1fjfy9m",
                    "permalink": "/r/KoboldAI/comments/1fjfy9m/differences_between_koboldai_and_koboldcpp/lo397zy/"
                },
                {
                    "id": "lo3u4xc",
                    "author": "None",
                    "body": "[removed]",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1726863561.0,
                    "parent_id": "t1_lo396on",
                    "link_id": "t3_1fjfy9m",
                    "permalink": "/r/KoboldAI/comments/1fjfy9m/differences_between_koboldai_and_koboldcpp/lo3u4xc/"
                },
                {
                    "id": "lo3uk11",
                    "author": "Parogarr",
                    "body": "Is kobold AI still being updated?",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1726863700.0,
                    "parent_id": "t1_lo3u4xc",
                    "link_id": "t3_1fjfy9m",
                    "permalink": "/r/KoboldAI/comments/1fjfy9m/differences_between_koboldai_and_koboldcpp/lo3uk11/"
                },
                {
                    "id": "lo3uo8e",
                    "author": "None",
                    "body": "[removed]",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1726863740.0,
                    "parent_id": "t1_lo3uk11",
                    "link_id": "t3_1fjfy9m",
                    "permalink": "/r/KoboldAI/comments/1fjfy9m/differences_between_koboldai_and_koboldcpp/lo3uo8e/"
                },
                {
                    "id": "lo3ut47",
                    "author": "Parogarr",
                    "body": "But I can't use the models I like because it doesn't support it",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1726863786.0,
                    "parent_id": "t1_lo3uo8e",
                    "link_id": "t3_1fjfy9m",
                    "permalink": "/r/KoboldAI/comments/1fjfy9m/differences_between_koboldai_and_koboldcpp/lo3ut47/"
                },
                {
                    "id": "lo3uwgk",
                    "author": "None",
                    "body": "[removed]",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1726863817.0,
                    "parent_id": "t1_lo3ut47",
                    "link_id": "t3_1fjfy9m",
                    "permalink": "/r/KoboldAI/comments/1fjfy9m/differences_between_koboldai_and_koboldcpp/lo3uwgk/"
                },
                {
                    "id": "lo3v2kt",
                    "author": "Parogarr",
                    "body": "Mistral 8x7b and its derivatives. They are mostly exl2",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1726863874.0,
                    "parent_id": "t1_lo3uwgk",
                    "link_id": "t3_1fjfy9m",
                    "permalink": "/r/KoboldAI/comments/1fjfy9m/differences_between_koboldai_and_koboldcpp/lo3v2kt/"
                },
                {
                    "id": "lo3v7lr",
                    "author": "None",
                    "body": "[removed]",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1726863920.0,
                    "parent_id": "t1_lo3v2kt",
                    "link_id": "t3_1fjfy9m",
                    "permalink": "/r/KoboldAI/comments/1fjfy9m/differences_between_koboldai_and_koboldcpp/lo3v7lr/"
                },
                {
                    "id": "lo3vew9",
                    "author": "Parogarr",
                    "body": "Yeah. But just out of curiosity why don't they just add the ability to load exl2 like kobald AI and Ooga has",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1726863988.0,
                    "parent_id": "t1_lo3v7lr",
                    "link_id": "t3_1fjfy9m",
                    "permalink": "/r/KoboldAI/comments/1fjfy9m/differences_between_koboldai_and_koboldcpp/lo3vew9/"
                },
                {
                    "id": "lo3vxfl",
                    "author": "None",
                    "body": "[removed]",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1726864162.0,
                    "parent_id": "t1_lo3vew9",
                    "link_id": "t3_1fjfy9m",
                    "permalink": "/r/KoboldAI/comments/1fjfy9m/differences_between_koboldai_and_koboldcpp/lo3vxfl/"
                },
                {
                    "id": "lo3w8ti",
                    "author": "Parogarr",
                    "body": "All right I'll give it a chance. Does it have all the features of Kobald AI? The lore book, the memory, the author's notes, etc.",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1726864269.0,
                    "parent_id": "t1_lo3vxfl",
                    "link_id": "t3_1fjfy9m",
                    "permalink": "/r/KoboldAI/comments/1fjfy9m/differences_between_koboldai_and_koboldcpp/lo3w8ti/"
                },
                {
                    "id": "lo40zh9",
                    "author": "Parogarr",
                    "body": "I gave it a try. I couldn't even get it to load a GGUF. It's just sitting at\n\nggml\\_cuda\\_init: found 1 CUDA devices:\n\n  Device 0: NVIDIA GeForce RTX 4090, compute capability 8.9, VMM: yes\n\nNo error messages or anything. Just sits here in the command prompt.",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1726865854.0,
                    "parent_id": "t1_lo3vxfl",
                    "link_id": "t3_1fjfy9m",
                    "permalink": "/r/KoboldAI/comments/1fjfy9m/differences_between_koboldai_and_koboldcpp/lo40zh9/"
                }
            ]
        },
        {
            "id": "1fj7gvi",
            "title": "Why there are no context templates in Koboldcpp?",
            "author": "Animus_777",
            "score": 1,
            "upvotes": 1,
            "downvotes": 0,
            "num_comments": 9,
            "created_utc": 1726599270.0,
            "url": "https://www.reddit.com/r/KoboldAI/comments/1fj7gvi/why_there_are_no_context_templates_in_koboldcpp/",
            "permalink": "/r/KoboldAI/comments/1fj7gvi/why_there_are_no_context_templates_in_koboldcpp/",
            "selftext": "In some RP models' cards on Huggingface there are recommended context templates that you can load in Silly Tavern. As I understand they are needed to properly read/parse character cards (text that goes into Memory field). But Kobold doesnt support them? If they are not important, why they are being made, and if they ARE needed why Kobold doesn't support them?",
            "comments": [
                {
                    "id": "lnmharo",
                    "author": "BangkokPadang",
                    "body": "They're just how the prompt sent to the model should be formatted for best compatibility with the model.\n\nIt's how the data the model trained or finetuned on was formatted, so generally models respond better to a prompt that is also formatted the same way as that training data was.\n\nIf you're using SillyTavern through Kobold's API, then that formatting of the prompt is handled by SillyTavern, not kobold. It isn't something you load into kobold as part of loading the model.\n\nOr are you asking about Kobold's 'Kobold Lite' interface? You can adjust this in the settings menu under the format tab. You can either select the instruct tag preset that matches the one suggested in the model card (Alpacca, Chat ML, Llama 3 Chat, etc.), or you can manually edit the system, user, prompt, etc. in the fields below the preset dropdown menu if the model suggests a unique format.\n\nKeep in mind that you do need to make sure you picked instruct mode when you load your character card, though. Just be aware that you won't be prompted to confirm this if you don't have the 'Card Import Prompt' box checked under the settings > advanced tab, so make sure it is, otherwise it will just default to chat mode rather than instruct.\n\nEDIT: I just reread your post and realized you're talking specifically about the context template (ie the order each segment of the system prompt and character data is included in the prompt) not the instruct preset, so just ignore everything I said. I don't think RP models are being trained with data formatted entirely with a specific context template, because it's unreasonable to expect a user to format their prompts that specifically. Which model did you notice this on? I'd like to check into it's datasets if they're public to actually see if it's formatted this way, or if it's just a suggestion in the way that samplers kindof are.\n\nIt's become pretty common for ST users to just share their samplers, context, and instruct .jsons because those three sets of settings basically make the model work the same for each user, so loading all 3 puts ST into a \"known working state.\" It's probably just the model maker's way of saying, \"I know that this works well, so please start with these so I don't have to troubleshoot an incorrectly set up instance of SillyTavern.",
                    "score": 5,
                    "upvotes": 5,
                    "downvotes": 0,
                    "created_utc": 1726603247.0,
                    "parent_id": "t3_1fj7gvi",
                    "link_id": "t3_1fj7gvi",
                    "permalink": "/r/KoboldAI/comments/1fj7gvi/why_there_are_no_context_templates_in_koboldcpp/lnmharo/"
                },
                {
                    "id": "lnn66pl",
                    "author": "henk717",
                    "body": "Its because we believe in an entirely different style than Tavern introduced after us.  \nTavern introduced the concept of arbitrarily splitting things about the characters into different fields, so they need a template on how to put that back together.\n\nIn KoboldAI thats entirely up to you, we don't have an arbitrary split. So instead of having a template you have the memory field in the context menu and everything is raw text to format as you see fit. That way your not limited to character cards but can make that anything you like, and prompting styles like I personally use would not work without it. It also means you can use it for purposes other than chat characters.",
                    "score": 3,
                    "upvotes": 3,
                    "downvotes": 0,
                    "created_utc": 1726611290.0,
                    "parent_id": "t3_1fj7gvi",
                    "link_id": "t3_1fj7gvi",
                    "permalink": "/r/KoboldAI/comments/1fj7gvi/why_there_are_no_context_templates_in_koboldcpp/lnn66pl/"
                },
                {
                    "id": "lnobsdo",
                    "author": "a_chatbot",
                    "body": "> It's become pretty common for ST users to just share their samplers, context, and instruct .jsons  \n\nI wish I knew where to find those, Most hugging face models don't work for me coherently except for some of the few that have detailed Silly Tavern setup instructions.",
                    "score": 2,
                    "upvotes": 2,
                    "downvotes": 0,
                    "created_utc": 1726627021.0,
                    "parent_id": "t1_lnmharo",
                    "link_id": "t3_1fj7gvi",
                    "permalink": "/r/KoboldAI/comments/1fj7gvi/why_there_are_no_context_templates_in_koboldcpp/lnobsdo/"
                },
                {
                    "id": "lnmul85",
                    "author": "Animus_777",
                    "body": ">Which model did you notice this on? I'd like to check into it's datasets if they're public to actually see if it's formatted this way, or if it's just a suggestion in the way that samplers kindof are.\n\nFor example [Nemomix Unleashed](https://huggingface.co/MarinaraSpaghetti/NemoMix-Unleashed-12B). Author recommends one of the following context templates (json with StoryString in the name):\n\n[https://huggingface.co/MarinaraSpaghetti/SillyTavern-Settings/tree/main/Basic/Mistral](https://huggingface.co/MarinaraSpaghetti/SillyTavern-Settings/tree/main/Basic/Mistral)\n\nor\n\n[https://huggingface.co/MarinaraSpaghetti/SillyTavern-Settings/tree/main/Customized/Mistral](https://huggingface.co/MarinaraSpaghetti/SillyTavern-Settings/tree/main/Customized/Mistral)",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1726607365.0,
                    "parent_id": "t1_lnmharo",
                    "link_id": "t3_1fj7gvi",
                    "permalink": "/r/KoboldAI/comments/1fj7gvi/why_there_are_no_context_templates_in_koboldcpp/lnmul85/"
                },
                {
                    "id": "lnmm4ji",
                    "author": "Animus_777",
                    "body": "Yes, I meant in Kobold Lite interface. I'm aware about Instruct presets. I specifically meant Context Templates. These are two different things:\n\n[https://imgur.com/fGzc7Zb](https://imgur.com/fGzc7Zb)\n\n[https://imgur.com/dlqoz4m](https://imgur.com/dlqoz4m)\n\nYou can't change/load Context Templates in Kobold UI.",
                    "score": 0,
                    "upvotes": 0,
                    "downvotes": 0,
                    "created_utc": 1726604736.0,
                    "parent_id": "t1_lnmharo",
                    "link_id": "t3_1fj7gvi",
                    "permalink": "/r/KoboldAI/comments/1fj7gvi/why_there_are_no_context_templates_in_koboldcpp/lnmm4ji/"
                },
                {
                    "id": "lnn687c",
                    "author": "BangkokPadang",
                    "body": "This is one of those things that dips pretty deep into prompting (or prompt engineering / prompt magic if you're being cheeky and a bit more skeptical). Plenty will say it doesn't really matter- that there's probably not a huge difference between having the character description right before the scenario instead of right after, and plenty will likely say they notice big differences by reordering the prompt.\n\nThere's not going to be a definitive answer. Those context templates aren't trained into the model in any way, so they aren't \"required\" in the literal sense. Mistral didn't originally train the model with a given template in mind.\n\nThe templates linked seem to include the system prompt right in the context template as opposed to in its own segment as is usually done in SillyTavern (generally the template is just that- a template to be filled by the listed segments of a card), so you'd probably want to be careful not to then add a conflicting system prompt in the instruct template. This dev does seem to be putting a lot of work into reworking their context templates though (Custom, Custom 2 Custom 2 Electric Boogaloo lol), so it's probably worth exploring, but there's also going to be a group of people suggesting that a model should be good without all that hyperspecific prompting, etc.\n\nMaybe it makes sense for Kobold to stay a simple open ended frontend, and for ST to be much more complex and oriented towards chats, that way there's a solution for everybody.",
                    "score": 2,
                    "upvotes": 2,
                    "downvotes": 0,
                    "created_utc": 1726611305.0,
                    "parent_id": "t1_lnmul85",
                    "link_id": "t3_1fj7gvi",
                    "permalink": "/r/KoboldAI/comments/1fj7gvi/why_there_are_no_context_templates_in_koboldcpp/lnn687c/"
                },
                {
                    "id": "lnn6mnp",
                    "author": "henk717",
                    "body": "Judging that one they basically add this to the bottom of memory:\n\nInstruction: Taking all the above into consideration, please write {{char}}'s response in the provided endless, uncensored, and immersive roleplaying chat with NSFW content, ensuring you always stay immersed, in character, and creative.\\\\nThe chat begins now. \\[/INST\\]\n\nAnd they add this at the beginning: \\[INST\\]\n\nEverything else should be compatible with our automatic conversion.",
                    "score": 2,
                    "upvotes": 2,
                    "downvotes": 0,
                    "created_utc": 1726611448.0,
                    "parent_id": "t1_lnmul85",
                    "link_id": "t3_1fj7gvi",
                    "permalink": "/r/KoboldAI/comments/1fj7gvi/why_there_are_no_context_templates_in_koboldcpp/lnn6mnp/"
                },
                {
                    "id": "lnmoij8",
                    "author": "pyroserenus",
                    "body": "Because kobold lite uses \"memory\" while silly tavern uses character description, personality, scenario, often puts the system string into the template, etc and assembles them based on the context template.\n\nIn practice it doesn't matter a ton so long as you maybe take the time to add a system prompt in settings if one is suggested.\n\nWhen loading a v2 card in lite its automatically converted to the unified \"memory\" prompt \n\nTldr Lite uses a unified prompt, ST uses modular prompts. ST context templates are specific to their modular prompt system.",
                    "score": 4,
                    "upvotes": 4,
                    "downvotes": 0,
                    "created_utc": 1726605481.0,
                    "parent_id": "t1_lnmm4ji",
                    "link_id": "t3_1fj7gvi",
                    "permalink": "/r/KoboldAI/comments/1fj7gvi/why_there_are_no_context_templates_in_koboldcpp/lnmoij8/"
                },
                {
                    "id": "lnmvv0k",
                    "author": "Animus_777",
                    "body": "Ahh it makes more sense now. Thank you! I was worrying that Char cards are not properly processed by models in Kobold UI vs ST without these templates.",
                    "score": 0,
                    "upvotes": 0,
                    "downvotes": 0,
                    "created_utc": 1726607778.0,
                    "parent_id": "t1_lnmoij8",
                    "link_id": "t3_1fj7gvi",
                    "permalink": "/r/KoboldAI/comments/1fj7gvi/why_there_are_no_context_templates_in_koboldcpp/lnmvv0k/"
                }
            ]
        },
        {
            "id": "1fj2kgk",
            "title": "Best settings for Text and image generation in general?",
            "author": "sissyexcited",
            "score": 2,
            "upvotes": 2,
            "downvotes": 0,
            "num_comments": 2,
            "created_utc": 1726587856.0,
            "url": "https://www.reddit.com/r/KoboldAI/comments/1fj2kgk/best_settings_for_text_and_image_generation_in/",
            "permalink": "/r/KoboldAI/comments/1fj2kgk/best_settings_for_text_and_image_generation_in/",
            "selftext": "Does anyone have any suggestions on setting up text generation and image generation in general? I have low consistency replies and image generators are primarily generating static. ",
            "comments": [
                {
                    "id": "lnl4n18",
                    "author": "None",
                    "body": "[removed]",
                    "score": 2,
                    "upvotes": 2,
                    "downvotes": 0,
                    "created_utc": 1726587949.0,
                    "parent_id": "t3_1fj2kgk",
                    "link_id": "t3_1fj2kgk",
                    "permalink": "/r/KoboldAI/comments/1fj2kgk/best_settings_for_text_and_image_generation_in/lnl4n18/"
                },
                {
                    "id": "lnl6bxq",
                    "author": "sissyexcited",
                    "body": "I have been getting better results by messing with tokens. I will try temperature next. Using specific prompts as in writing out exactly what is going on? Do you know of any prompting guides? I use the default generation tool set. But I know it has an option to change text prompting before generation. I did not think that could be causing consistent static.",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1726588489.0,
                    "parent_id": "t1_lnl4n18",
                    "link_id": "t3_1fj2kgk",
                    "permalink": "/r/KoboldAI/comments/1fj2kgk/best_settings_for_text_and_image_generation_in/lnl6bxq/"
                }
            ]
        },
        {
            "id": "1fitvs2",
            "title": "nocuda Vulkan creates garbled images, compared to images created with ROCm",
            "author": "BopDoBop",
            "score": 2,
            "upvotes": 2,
            "downvotes": 0,
            "num_comments": 2,
            "created_utc": 1726562244.0,
            "url": "https://www.reddit.com/r/KoboldAI/comments/1fitvs2/nocuda_vulkan_creates_garbled_images_compared_to/",
            "permalink": "/r/KoboldAI/comments/1fitvs2/nocuda_vulkan_creates_garbled_images_compared_to/",
            "selftext": "Hi\n\nI am using koboldccp for language and image generation with with SillyTavern.  \nI use standalone exe version.  \nI have AMD 7900XT so I use koboldcpp\\_rocm fork created by YellowRoseCx:  \n[https://github.com/YellowRoseCx/koboldcpp-rocm/releases](https://github.com/YellowRoseCx/koboldcpp-rocm/releases)\n\n1. Latest fully working version was koboldcpp\\_v1.72.yr0-rocm\\_6.1.2 By working \"fully\" I mean: it uses HipBLAS (ROCm) preset, and both text gen and image gen are done with GPU\n2. Latest v1.74.yr0-ROCm version doesn't work for me as it fails with this error: Traceback (most recent call last): File \"koboldcpp.py\", line 4881, in <module> File \"koboldcpp.py\", line 4526, in main File \"koboldcpp.py\", line 894, in load\\_model OSError: exception: access violation reading 0x0000000000000000 \\[363000\\] Failed to execute script 'koboldcpp' due to unhandled exception!\n3. Latest koboldcpp\\_nocuda 1.74 works but not fully. It utilizes GPU for both text and image gen but images are thrown \"garbled\" take a look into attached comparation pic.\n\nI use 11B gguf with it and SD 1.5 safetensors model from Civitai  \nLatest AMD drivers, Win 11 pro, all updated.\n\n**Questions:**\n\n1. Is it possible to get Vulkan to produce images like what ROCm does?\n2. How can I find what causes the error in my 2nd question?\n\nMy goal is to use latest version which uses GPU for both text and image gen.\n\nTy\n\nhttps://preview.redd.it/8bsdjoy10cpd1.jpg?width=1024&format=pjpg&auto=webp&s=bdd1a06e627281ad70bd0c41eacf4064b43fc798\n\n",
            "comments": [
                {
                    "id": "lov06fs",
                    "author": "BopDoBop",
                    "body": "LeBump  \nNobody?",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1727274924.0,
                    "parent_id": "t3_1fitvs2",
                    "link_id": "t3_1fitvs2",
                    "permalink": "/r/KoboldAI/comments/1fitvs2/nocuda_vulkan_creates_garbled_images_compared_to/lov06fs/"
                },
                {
                    "id": "lq4rpcc",
                    "author": "PuffyBloomerBandit",
                    "body": "dont use nocuda anyways, ROCM is literally 10x faster currently. as for the image looking like that, looks like what happens with too few steps and/or the vanilla SD1.5 model.",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1727961457.0,
                    "parent_id": "t3_1fitvs2",
                    "link_id": "t3_1fitvs2",
                    "permalink": "/r/KoboldAI/comments/1fitvs2/nocuda_vulkan_creates_garbled_images_compared_to/lq4rpcc/"
                }
            ]
        },
        {
            "id": "1fierry",
            "title": "Using KoboldAI to develop an Imaginary World",
            "author": "BaronGrivet",
            "score": 12,
            "upvotes": 12,
            "downvotes": 0,
            "num_comments": 13,
            "created_utc": 1726517928.0,
            "url": "https://www.reddit.com/r/KoboldAI/comments/1fierry/using_koboldai_to_develop_an_imaginary_world/",
            "permalink": "/r/KoboldAI/comments/1fierry/using_koboldai_to_develop_an_imaginary_world/",
            "selftext": "Me and my 13yo have created an imaginary world over the past couple of years. It's spawned writing, maps, drawings, Lego MOCs and many random discussions.\n\nI want to continue developing the world in a coherent way. So we've got lore we can build on and any stories, additions etc. we make fit in with the world we've built.\n\nLast night I downloaded KoboldCPP and trialled it with the mistral-6b-openorca.Q4\\_K\\_M model. It could make simple stories, but I realised I need a plan and some advice on how we should proceed.\n\nI was thinking of this approach:\n\n1. Source a comprehensive base language model that's fit for purpose.\n\n2. Load our current content into Kobold (currently around 9,000 words of lore and background).\n\n3. Use Kobold to create short stories about our world.\n\n4. Once we're happy with a story add it to the lore in Kobold.\n\n\n\nWhich leads to a bunch of questions:\n\n1. What language model/s should we use? \n\n2. Kobold has slots for \"Model\", \"Lora\", \"Lora Base\", \"\"LLaVA mmproj\", \"Preloaded Story\" and \"ChatCompletions Adapter\" - which should we be using?\n\n3. Should our lore be a single text file,a JSON file, or do we need to convert it to a GUFF? \n\n4. Does the lore go in the \"Preloaded Story\" slot? How do we combine our lore with the base model?\n\n5. Is it possible to write short stories that are 5,000-10,000 words long while the model still retains and references/ considers 10,000+ words of lore and previous stories?\n\n\n\nMy laptop is a Lenovo Legion 5 running Ubuntu 24.04 with 32GB RAM + Ryzen 7 + RTX4070 (8GB VRAM). Generation doesn't need to be fast - the aim is quality.\n\nI know that any GPT can easily spit out a bland \"story\" a few hundred words long. But my aim is for us to create structured short stories that hold up to the standards of a 13yo and their mates who read a lot of YA fiction. Starting with 1,000-2,000 words would be fine, but the goal is 5,000-10,000 word stories that gradually build up the world.\n\n\n\nBonus question:\n\nHow do we setup the image generation in Kobold so it can generate scenes from the stories that have a cohesive art style and characters between images and stories? Is that even possible in Kobold?\n\n\n\nThank you for your time.",
            "comments": [
                {
                    "id": "lnh4mhu",
                    "author": "FaceDeer",
                    "body": "I'm setting up to do something similar (there's an upcoming tabletop roleplaying game I'm planning to do comprehensive automated note-taking on) but I've only done a little experimenting so far. So this comment is partly advice, and also partly in the vein of \"the fastest way to get a correct answer on the Internet is to give an incorrect answer\" :)\n\nThe question of what language model to use is a doozy. Different models have different hardware requirements and different capabilities, and it's all a bit of black magic trying to benchmark them. One of the important capabilities you're going to need is a model with a large context length. The context length determines how much stuff the model can hold in its \"active memory\", which is where you're going to be putting the notes you've made as well as the ongoing discussion you're having with the AI.\n\nI am fortunate to have bought myself a fairly substantial graphics card so I went for a pretty big model, based on my initial investigation I was led to believe that [Command-R](https://huggingface.co/bartowski/c4ai-command-r-08-2024-GGUF) is specifically intended for RAG (retrieval-augmented generation). But I think with 8GB of VRAM that might be too big for your system to work nicely. Even though speed isn't important I think you'll agree that having the AI grind out one word every ten seconds would be infuriating, especially when after a couple of minutes it becomes clear that the sentence it's trying to create is \"I didn't quite understand what you just asked me, could you clarify blah blah blah etc.\" :)\n\nSince I don't have a broad range of experience with other models I won't give a specific recommendation here. Hopefully someone else will chime in with a good one that handles a lengthy context.\n\n> Kobold has slots for \"Model\", \"Lora\", \"Lora Base\", \"\"LLaVA mmproj\", \"Preloaded Story\" and \"ChatCompletions Adapter\" - which should we be using?\n\nThe model is the only vital part here. That's the main \"brains\" of your AI. Loras are a sort of \"modifier\" that can be loaded overtop of the model to make it behave differently, it's an advanced feature that's probably not going to be relevant.\n\n\"Preloaded story\" might be of some use to you, once you've set up your AI with the information it needs in its context you could save that session and then set it here so that whenever you start this particular AI it'll have that already in its context.\n\nChatCompletions Adapter might be useful to set, they contain hints on the particular \"dialect\" the model uses when it's talking. You're using a Mistral model right now so the Mistral adapter might help KoboldCPP work more smoothly. I actually didn't set that for a long time when I first started messing with KoboldCPP so I'm not sure how vital it is.\n\n> Should our lore be a single text file,a JSON file, or do we need to convert it to a GUFF? \n\nThere's a couple of ways you can get your lore into the chat's context that I know of.\n\nThe simplest would be to start the conversation with \"Hey, KoboldGPT, I've created an imaginary world over the past couple of years. I want to continue developing the world in a coherent way. So we've got lore we can build on and any stories, additions etc. we make fit in with the world we've built. Here it is...\" And then paste the giant text dump in. The big downside of this approach is that as you continue talking with the AI the context will continue to get longer and longer, and eventually the oldest stuff might start getting dropped to make room for newer stuff.\n\nA better approach would be to edit the context of the session directly. Click the gear icon in the lower right of KoboldCPP's interface, then the \"Context\" button. This gives you access to some special fields where you can put information that *won't* end up being removed from the context over time.\n\nThe \"Memory\" field is a block of text that's always present at the top of the context. Put all the most important stuff in here that the AI should always know about the setting. This is always at the top of your context.\n\nThe \"Author's Note\" is for stuff you'd like the AI to keep in mind *right now*, but maybe isn't always important. If you're writing a story, for example, this would be a good place to put a note like \"the heroes are currently in the lost caverns of Tscarnoth, trying to find their way out.\" That way the AI won't lose track of what's going on right now, but later on it's easy to update with new information once circumstances change.\n\nThe \"World Info\" tab is where things start getting fancy. This allows you to put a bunch of short \"facts\" into memory with keywords associated with them, so that they only get inserted into the context when they are actually mentioned. For example you could have an entry describing what the Lost Caverns of Tscarnoth are like under the keywords \"caverns, tscarnoth\" and then whenever a character mentions one of those words that information will be inserted into the context so the AI will know about it.\n\nSince you already have a bunch of text pre-written, what you might find easiest is to save a session as a .json file with a few bits of example information in them and then open the .json file in a text editor to bulk-add more stuff. I find this easier than working with KoboldCPP's web interface much of the time. Then you can load that session back in and it'll all be there.\n\nYou don't have to preload the story, you can manually do that after starting KoboldCPP up.\n\n> I know that any GPT can easily spit out a bland \"story\" a few hundred words long. But my aim is for us to create structured short stories that hold up to the standards of a 13yo and their mates who read a lot of YA fiction. Starting with 1,000-2,000 words would be fine, but the goal is 5,000-10,000 word stories that gradually build up the world.\n\nI've done a small amount of tinkering with this and I don't know of a good way to do this *automatically*, but a little work \"by hand\" can accomplish a lot. First generate the overall outline of the story, and then fill in the outline a few pages at a time. I've found that a lot of AIs really love to jump to the end of the story if you're not constantly reigning it in with \"we're starting here and writing up to here, no further.\"",
                    "score": 5,
                    "upvotes": 5,
                    "downvotes": 0,
                    "created_utc": 1726522849.0,
                    "parent_id": "t3_1fierry",
                    "link_id": "t3_1fierry",
                    "permalink": "/r/KoboldAI/comments/1fierry/using_koboldai_to_develop_an_imaginary_world/lnh4mhu/"
                },
                {
                    "id": "lngrg50",
                    "author": "mayo551",
                    "body": "I would recommend using sillytavern + rag.\n\nhttps://docs.sillytavern.app/usage/core-concepts/data-bank/\n\nYou can also use lore books in sillytavern, which for 9000 words should be fine.",
                    "score": 3,
                    "upvotes": 3,
                    "downvotes": 0,
                    "created_utc": 1726518525.0,
                    "parent_id": "t3_1fierry",
                    "link_id": "t3_1fierry",
                    "permalink": "/r/KoboldAI/comments/1fierry/using_koboldai_to_develop_an_imaginary_world/lngrg50/"
                },
                {
                    "id": "lnh4d1a",
                    "author": "None",
                    "body": "[deleted]",
                    "score": 3,
                    "upvotes": 3,
                    "downvotes": 0,
                    "created_utc": 1726522760.0,
                    "parent_id": "t3_1fierry",
                    "link_id": "t3_1fierry",
                    "permalink": "/r/KoboldAI/comments/1fierry/using_koboldai_to_develop_an_imaginary_world/lnh4d1a/"
                },
                {
                    "id": "lnk6gd4",
                    "author": "Ill_Yam_9994",
                    "body": "https://huggingface.co/bartowski/c4ai-command-r-08-2024-GGUF\n\nTry this at Q6k, if you run out of memory try a lower quant. It's a medium sized model that tends to work well for creative writing.\n\nLike the others have said, none of those Kobold options matter except the model. The preloaded story literally just automatically loads a saved conversation on startup.\n\nLook into using world info in the Kobold webui, or SillyTavern as the other guy suggested. \n\nhttps://github-wiki-see.page/m/KoboldAI/KoboldAI-Client/wiki/Memory,-Author%27s-Note-and-World-Info",
                    "score": 3,
                    "upvotes": 3,
                    "downvotes": 0,
                    "created_utc": 1726575921.0,
                    "parent_id": "t3_1fierry",
                    "link_id": "t3_1fierry",
                    "permalink": "/r/KoboldAI/comments/1fierry/using_koboldai_to_develop_an_imaginary_world/lnk6gd4/"
                },
                {
                    "id": "lnjfzga",
                    "author": "aseichter2007",
                    "body": "I'm pretty sure that the ChatCompletions adapter only changes the koboldcpp openai chat completion endpoint's prompt format. ChatCompletions adapter doesn't matter at all if you're connected to the standard completions endpoint of kobold. Typically, silly tavern handles applying the prompt format.",
                    "score": 2,
                    "upvotes": 2,
                    "downvotes": 0,
                    "created_utc": 1726560061.0,
                    "parent_id": "t1_lnh4mhu",
                    "link_id": "t3_1fierry",
                    "permalink": "/r/KoboldAI/comments/1fierry/using_koboldai_to_develop_an_imaginary_world/lnjfzga/"
                },
                {
                    "id": "lnmhryh",
                    "author": "BaronGrivet",
                    "body": "Thank you, I really appreciate you taking the time to type that all out.\n\nFor using \"Command-R\" as my model I went to the \"Files and versions\" tab in Huggingface - [https://huggingface.co/bartowski/c4ai-command-r-08-2024-GGUF/tree/main](https://huggingface.co/bartowski/c4ai-command-r-08-2024-GGUF/tree/main) - and got 25 different options. Do you have any suggestions on which I should try out first with my 8GB of VRAM?\n\nI needed to change the Kobold interface from Classic to Aesthetic for the Context tab to appear. So knowing it exists is a great step forward.\n\nAre the limitations of \"World Info\" based on the model? I guessing 2048 or 4096 tokens. If that's the case I could ask an AI chatbot to summarise our existing lore in <1,000 words of bullet points. I use [Claude Projects](https://support.anthropic.com/en/articles/9517075-what-are-projects) - which might work well for this use case. Projects have knowledge bases where you can upload files for context. So I upload all of our lore/ stories to Claude, get it to summarise to bullet points and drop that into World Info.\n\nI've also come across the AI drive to get to the end of the story (and give it a happy ending). I've used the overall outline, fill in process with earlier ChatGPTs and it worked well.\n\nGreat tips, thank you again for your time.",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1726603395.0,
                    "parent_id": "t1_lnh4mhu",
                    "link_id": "t3_1fierry",
                    "permalink": "/r/KoboldAI/comments/1fierry/using_koboldai_to_develop_an_imaginary_world/lnmhryh/"
                },
                {
                    "id": "lnmk1vs",
                    "author": "BaronGrivet",
                    "body": "I installed the Silly Tavern Launcher and it looks like it could work well. But a lot is going on and I need to invest some time to work out how the heck I should set it up.\n\nGoogling for SillyTavern guides brought up a bunch of waifu/ kink stuff that definitely doesn't fit my kid-friendly use case!\n\nThis seems like the best place for me to start - [https://docs.sillytavern.app/usage/local-llm-guide/how-to-use-a-self-hosted-model/](https://docs.sillytavern.app/usage/local-llm-guide/how-to-use-a-self-hosted-model/) - but any other links/ suggestions would be appreciated.",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1726604099.0,
                    "parent_id": "t1_lngrg50",
                    "link_id": "t3_1fierry",
                    "permalink": "/r/KoboldAI/comments/1fierry/using_koboldai_to_develop_an_imaginary_world/lnmk1vs/"
                },
                {
                    "id": "lnnfpm8",
                    "author": "None",
                    "body": "[deleted]",
                    "score": 2,
                    "upvotes": 2,
                    "downvotes": 0,
                    "created_utc": 1726614776.0,
                    "parent_id": "t1_lnh4d1a",
                    "link_id": "t3_1fierry",
                    "permalink": "/r/KoboldAI/comments/1fierry/using_koboldai_to_develop_an_imaginary_world/lnnfpm8/"
                },
                {
                    "id": "lnmqhlb",
                    "author": "FaceDeer",
                    "body": "Ah, thanks. That explains why nothing seemed to be broken before I spotted that and actually set it.",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1726606089.0,
                    "parent_id": "t1_lnjfzga",
                    "link_id": "t3_1fierry",
                    "permalink": "/r/KoboldAI/comments/1fierry/using_koboldai_to_develop_an_imaginary_world/lnmqhlb/"
                },
                {
                    "id": "lnmnvva",
                    "author": "FaceDeer",
                    "body": "> For using \"Command-R\" as my model I went to the \"Files and versions\" tab in Huggingface - https://huggingface.co/bartowski/c4ai-command-r-08-2024-GGUF/tree/main - and got 25 different options. Do you have any suggestions on which I should try out first with my 8GB of VRAM?\n\nHonestly, \"something other than Command-R\" would be my suggestion. :) I've got 24GB of VRAM and Command-R chugs pretty hard when I run it. If you go to the [main page for that model](https://huggingface.co/bartowski/c4ai-command-r-08-2024-GGUF) you'll see a list of recommendations for which of the various different files might be suitable. You'll see this on a lot of GGUF models at Huggingface. Basically, GGUF is a method of \"compressing\" AI models so that they don't take up as much space or memory. But as with most methods of compression, the more you compress the data the less \"good\" the result is. Highly compressed models tend to get dumber. So you need to strike a balance between the general qualities of \"big model, therefore smart\" and \"highly compressed, therefore actually usable\".\n\nThe version I'm using right now is Q4_K_M, but I'm not sure that'd be runable on your system. If you don't mind trying a 19 GB download that may not work, though, might as well give it a shot.\n\nAs I said in my giant wall of text, I'm unfortunately not widely versed in a lot of different models so I don't have much advice to give for what alternatives are good. There are a lot of models out there that are based on the Llama-3.1-8B model, it's a lot slimmer. I've used the [Celeste 8B model](https://huggingface.co/nothingiisreal/L3.1-8B-Celeste-V1.5?not-for-all-audiences=true) before, it's good at storytelling kinds of things. [GGUF versions of it are here](https://huggingface.co/bartowski/L3.1-8B-Celeste-V1.5-GGUF?not-for-all-audiences=true). If you want to try something a little fancier to see if your computer can handle it there's also [12B Celeste models](https://huggingface.co/nothingiisreal/Celeste-12B-V1.6), I haven't tried these but there's a link to some GGUF versions of them on its model page.\n\n> Are the limitations of \"World Info\" based on the model?\n\nYeah. Different models have different limits on how big their context can be. If the model doesn't tell you explicitly then look at the spew of output that Koboldcpp puts into the terminal window when you launch a model with it, the parameter you're looking for is \"n_ctx_train\". That tells you how many tokens of context (ctx) the model was trained to understand. You'll need to tell Kobold to actually *use* that many tokens when you launch it, too.\n\nEdit: Sorry, I got a little confused and missed the switch from \"context\" to \"World Info\". As mentioned elsewhere, the entire World Info isn't put into context at once, so you don't have to worry quite so much about context size when you're using world info.\n\n\"Token count\" and \"word count\" are not identical, but they're a reasonably comparable ballpark figure. If you want to get a feel for what tokens are like you might want to play around with [OpenAI's tokenizer](https://platform.openai.com/tokenizer). Different models use different tokenizers so again it's not a 1:1 comparison, but it might be illuminating to see how these LLMs actually perceive the input they're getting. The reason ChatGPT has so much trouble counting the \"r\"s in \"Strawberry\" is more understandable when you know that ChatGPT sees the word \"Strawberry\" as \"[3504, 1134, 19772]\" :)\n\n> Projects have knowledge bases where you can upload files for context. \n\nI haven't used Claude Projects myself, but what I'm guessing is happening here is that it's doing retrieval-augmented generation (RAG) with those files. So they're not being placed directly into the context in their entirety, but rather bits of them get placed in there based on a built-in search engine trying to figure out what's most relevant to the current subject at hand.\n\nIt's similar to the World Info tab. Not all of the contents of the World Info will be placed in the context at once, it'll be inserted based on whether the tags associated with them have come up.\n\nHeh. There I go spewing walls of text again. :)\n\nI hope this works out well for you. This tech is all still very new, so tools like Koboldcpp are still rather \"raw\" an not very user-friendly in a lot of ways. Eventually the tech will settle down a bit and the tools will catch up, but for now I'm not going to complain about how every month brings a new revolution in capacity.",
                    "score": 3,
                    "upvotes": 3,
                    "downvotes": 0,
                    "created_utc": 1726605285.0,
                    "parent_id": "t1_lnmhryh",
                    "link_id": "t3_1fierry",
                    "permalink": "/r/KoboldAI/comments/1fierry/using_koboldai_to_develop_an_imaginary_world/lnmnvva/"
                },
                {
                    "id": "lnqj95q",
                    "author": "Original-Success-784",
                    "body": "I am using SillyTavern for my Science-Fiction Adventure.  \nThe best source to setup and use SillyTavern is: [https://docs.sillytavern.app/](https://docs.sillytavern.app/) everything you need!  \n  \nI am using ST like this:  \n1. Set up World-Info (in wich World are U living, Planet, Galaxy...)  \n2. Create your own character (who U are)  \n3. Create Narrator (if wanted, telling things what is going on)  \n4. Create other characters (your buddys!)  \n5. Create a Group so Narrator and characters can write/answer together....\n\nYou can also create a character who creates: Story, World-Info or other characters... for you.\n\nI got good results with Gemma2 27b but this will be to much for 8GB VRAM: So you can try:  \nGemma2-9b: [https://huggingface.co/bartowski/gemma-2-9b-it-GGUF/blob/main/gemma-2-9b-it-Q4\\_K\\_M.gguf](https://huggingface.co/bartowski/gemma-2-9b-it-GGUF/blob/main/gemma-2-9b-it-Q4_K_M.gguf)\n\n\\*\\*\\* Your Bonus question: \\*\\*\\*  \nTo setup Image generation. Download a Model, for example: [https://civitai.com/models/4384/dreamshaper?modelVersionId=128713](https://civitai.com/models/4384/dreamshaper?modelVersionId=128713) (1,99GB)  \n1. Start KoboldCpp  \n2. Make all Settings for your LLM  \n2. Tab: Image Gen  \n3. Browse to you Model you downloaded before.  \n4. Start ... you are ready to go.  \nIn KoboldCpp you can create Pictures etc. or you can go to the settings and open a seperate window to create pictures (not related to your story, only your promps).\n\nIf you are using SillyTavern and you want image generation, you can also use the Koboldcpp settings OR other apps like StableDiffusion (seperate instance).",
                    "score": 3,
                    "upvotes": 3,
                    "downvotes": 0,
                    "created_utc": 1726669004.0,
                    "parent_id": "t1_lnmk1vs",
                    "link_id": "t3_1fierry",
                    "permalink": "/r/KoboldAI/comments/1fierry/using_koboldai_to_develop_an_imaginary_world/lnqj95q/"
                },
                {
                    "id": "lnmquod",
                    "author": "mayo551",
                    "body": "Yeah sillytavern is used forâ€¦ interesting thingsâ€¦ but that doesnâ€™t change that itâ€™s a tool you can use.\n\nThe lore books and rag sound exactly like what youâ€™re looking for.",
                    "score": 2,
                    "upvotes": 2,
                    "downvotes": 0,
                    "created_utc": 1726606202.0,
                    "parent_id": "t1_lnmk1vs",
                    "link_id": "t3_1fierry",
                    "permalink": "/r/KoboldAI/comments/1fierry/using_koboldai_to_develop_an_imaginary_world/lnmquod/"
                },
                {
                    "id": "lnmr5av",
                    "author": "FaceDeer",
                    "body": "It's important to bear in mind that much of human innovation is first driven forward by pornography, and only later are the results later adapted to other uses. :)\n\nSillytavern doesn't *have* to be used for kinky stuff, it's just a use case that happens to result in a lot of free developer effort being put into things. Adding some notes in the context telling the AI to keep things cool may be helpful if you use a model that's got a steamy side to it (like Celeste, which I linked in my earlier comment).",
                    "score": 2,
                    "upvotes": 2,
                    "downvotes": 0,
                    "created_utc": 1726606294.0,
                    "parent_id": "t1_lnmk1vs",
                    "link_id": "t3_1fierry",
                    "permalink": "/r/KoboldAI/comments/1fierry/using_koboldai_to_develop_an_imaginary_world/lnmr5av/"
                },
                {
                    "id": "lno8ooi",
                    "author": "BaronGrivet",
                    "body": "Thank you. Which version do you use? I'm still getting my head around what my laptop can handle.",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1726625756.0,
                    "parent_id": "t1_lnnfpm8",
                    "link_id": "t3_1fierry",
                    "permalink": "/r/KoboldAI/comments/1fierry/using_koboldai_to_develop_an_imaginary_world/lno8ooi/"
                },
                {
                    "id": "lnrpi7p",
                    "author": "Kalmaro",
                    "body": "Giving this a shot, curious to see how this works.\n\n\nI personally just use a ton of world in with keywords structured so that Kobold will just pull what's needed by default.",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1726682418.0,
                    "parent_id": "t1_lnnfpm8",
                    "link_id": "t3_1fierry",
                    "permalink": "/r/KoboldAI/comments/1fierry/using_koboldai_to_develop_an_imaginary_world/lnrpi7p/"
                }
            ]
        },
        {
            "id": "1fi9ssg",
            "title": "Runpod template context size",
            "author": "dengopaiv",
            "score": 1,
            "upvotes": 1,
            "downvotes": 0,
            "num_comments": 8,
            "created_utc": 1726505889.0,
            "url": "https://www.reddit.com/r/KoboldAI/comments/1fi9ssg/runpod_template_context_size/",
            "permalink": "/r/KoboldAI/comments/1fi9ssg/runpod_template_context_size/",
            "selftext": "Hi,\nRunning Koboldcpp on Runpod. The settings menu shows context size up to 4096, but I can set it bigger in the environment. Can I test if it functions or not?",
            "comments": [
                {
                    "id": "lnguf66",
                    "author": "mayo551",
                    "body": "When using runpod install vanilla koboldcpp, set context size from command line and then you can adjust sillytavern. Make sure you install cuda as well.",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1726519484.0,
                    "parent_id": "t3_1fi9ssg",
                    "link_id": "t3_1fi9ssg",
                    "permalink": "/r/KoboldAI/comments/1fi9ssg/runpod_template_context_size/lnguf66/"
                },
                {
                    "id": "lngws4d",
                    "author": "dengopaiv",
                    "body": "Thanks. I'm actually just using the runpod koboldcpp template. I might have forgotten to press save overrides, interestingly, the model loaded, just the context couldn't go higher than 4096.",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1726520241.0,
                    "parent_id": "t1_lnguf66",
                    "link_id": "t3_1fi9ssg",
                    "permalink": "/r/KoboldAI/comments/1fi9ssg/runpod_template_context_size/lngws4d/"
                },
                {
                    "id": "lngxelw",
                    "author": "mayo551",
                    "body": "Ah I donâ€™t use the templates. I just install a bash script to download and configure everything for me lol.",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1726520440.0,
                    "parent_id": "t1_lngws4d",
                    "link_id": "t3_1fi9ssg",
                    "permalink": "/r/KoboldAI/comments/1fi9ssg/runpod_template_context_size/lngxelw/"
                },
                {
                    "id": "lnhbazk",
                    "author": "BangkokPadang",
                    "body": "I love Koboldcpp for local stuff, ie models I canâ€™t fit in my own VRAM and need to split into system ram as well as on my M1 Mini, but I would highly recommend using an oobabooga template so you can take advantage of EXL2 models which are, even still, so much faster than  GGUF models on like hardware.\n\nI just figure since youâ€™re probably renting a big enough GPU to just run the model all in VRAM, and youâ€™re paying by the minute, you might as well get the fastest response possible.\n\nI can share one if you like that is fully configured where you donâ€™t have to even touch a command line, and the dashboard gives you a simple button for the webui, and a second button to copy/paste the API URL.\n\nJust paste the huggingface url into the model download field, click download, and then load the model (with all configuration settings like contrxt size, kv cache quantization, etc. exposed right in the webui), load the model, and then do everything else either in SillyTavern or oobaâ€™s webui.\n\nThis is absolutely nothing against kobold, itâ€™s just not quite as optimized for use on RunPod as Ooba ends up being, and then of course thereâ€™s the increased speed.",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1726525179.0,
                    "parent_id": "t1_lngws4d",
                    "link_id": "t3_1fi9ssg",
                    "permalink": "/r/KoboldAI/comments/1fi9ssg/runpod_template_context_size/lnhbazk/"
                },
                {
                    "id": "lnh7r73",
                    "author": "dengopaiv",
                    "body": "I will most likely want to learn that at some point.",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1726523918.0,
                    "parent_id": "t1_lngxelw",
                    "link_id": "t3_1fi9ssg",
                    "permalink": "/r/KoboldAI/comments/1fi9ssg/runpod_template_context_size/lnh7r73/"
                },
                {
                    "id": "lnjg8jl",
                    "author": "dengopaiv",
                    "body": "I have nothing against command line, but I would be thankful for the scripts or templates, so I could test something new. Thanks so much.",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1726560243.0,
                    "parent_id": "t1_lnhbazk",
                    "link_id": "t3_1fi9ssg",
                    "permalink": "/r/KoboldAI/comments/1fi9ssg/runpod_template_context_size/lnjg8jl/"
                },
                {
                    "id": "lnjkrn3",
                    "author": "BangkokPadang",
                    "body": "[https://www.runpod.io/console/explore/ktqdbmxoja](https://www.runpod.io/console/explore/ktqdbmxoja)\n\nJust pick this template when you're selecting your system/GPU and it will load up. The dashboard will have two links that look like rectangular buttons. One is :7860 and clicking that will open the text-generation-webui in a browser, and the other is :5000 and you right click that one, copy the link and then paste that into the Text Completion > Default \\[OpenAI/Completions compatible: Ooba, LM Studio, Etc.\\] Server URL field in SillyTavern (under the icon that looks like a plug)\n\nThen you open the text-generation-webui link in your browser, click the Model tab, and in the download field on the right side, you copy paste the devname/model-name link from huggingface into that field and click download. For GGUF models, you paste the URL into the field, and then click the grey list files button and then pick whatever size GGUF you want to download and paste it into the model field.\n\nFor the few EXL2 models with different quants listed in branches, you just add :branchname to the end of the devname/modelname link so as a made up example, it might be like TheDrummer/Donnager-70B-EXL2:4.5pbw (making sure to just use the appropriate names as they are in huggingface.\n\nThen you click the little refresh icon once the message area tells you the model has downloaded, pick the model you just downloaded from the list, and it will autoselect the right loader for it and autopopulate the native context size. You can also easily select things like flashattention and kv cache 4bit quantization or any other options you want just by clicking the appropriate checkbox buttons, and then click load.",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1726563565.0,
                    "parent_id": "t1_lnjg8jl",
                    "link_id": "t3_1fi9ssg",
                    "permalink": "/r/KoboldAI/comments/1fi9ssg/runpod_template_context_size/lnjkrn3/"
                },
                {
                    "id": "lnjwogi",
                    "author": "dengopaiv",
                    "body": "Thank you so much. I will give it a try.",
                    "score": 1,
                    "upvotes": 1,
                    "downvotes": 0,
                    "created_utc": 1726571266.0,
                    "parent_id": "t1_lnjkrn3",
                    "link_id": "t3_1fi9ssg",
                    "permalink": "/r/KoboldAI/comments/1fi9ssg/runpod_template_context_size/lnjwogi/"
                }
            ]
        }
    ]
}