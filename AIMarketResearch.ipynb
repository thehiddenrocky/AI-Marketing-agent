{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "tUKTdXIiM9BO",
   "metadata": {
    "id": "tUKTdXIiM9BO"
   },
   "source": [
    "# AI/NLP Development Environment Setup\n",
    "\n",
    "This document outlines the required Python packages for setting up an AI/NLP development environment that integrates various tools for document processing, Reddit API access, and Language Model interactions.\n",
    "\n",
    "## Required Packages\n",
    "\n",
    "- **chromadb**: Vector database for storing and managing embeddings\n",
    "- **praw**: Python Reddit API Wrapper for accessing Reddit data\n",
    "- **openai**: OpenAI's official client library\n",
    "- **python-dotenv**: Environment variable management\n",
    "- **langchain**: Framework for developing LLM-powered applications\n",
    "- **langchain-openai**: LangChain integration with OpenAI models\n",
    "- **langchain-text-splitters**: Text chunking and splitting utilities\n",
    "- **langgraph**: Graph-based operations for LangChain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5673d0d7d51ef2e0",
   "metadata": {
    "id": "5673d0d7d51ef2e0"
   },
   "outputs": [],
   "source": [
    "!pip install -q chromadb praw openai load-dotenv langchain langchain-openai langchain-text-splitters langgraph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fns4P9ANT_t",
   "metadata": {
    "id": "3fns4P9ANT_t"
   },
   "source": [
    "# Environment Configuration Setup\n",
    "\n",
    "This script handles the initialization of environment variables for OpenAI and Reddit API authentication. It uses the `python-dotenv` package to load environment variables from a `.env` file.\n",
    "\n",
    "## Required Environment Variables\n",
    "\n",
    "The following environment variables need to be set in your `.env` file:\n",
    "\n",
    "- `OPENAI_API_KEY`: Your OpenAI API key\n",
    "- `REDDIT_CLIENT_ID`: Your Reddit application client ID\n",
    "- `REDDIT_CLIENT_SECRET`: Your Reddit application client secret\n",
    "- `REDDIT_USER_AGENT`: Your Reddit API user agent string\n",
    "\n",
    "## Configuration Variables\n",
    "\n",
    "- `GENERATE_KNOWLEDGE`: Boolean flag (default: `False`) controlling knowledge generation functionality\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T14:41:36.332725Z",
     "start_time": "2024-11-17T14:41:34.523399Z"
    },
    "id": "initial_id"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import time\n",
    "from typing import List, Dict, TypedDict\n",
    "\n",
    "import chromadb\n",
    "import praw\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b521c1abdfb7d75c",
   "metadata": {
    "id": "b521c1abdfb7d75c"
   },
   "source": [
    "# Marketing Research Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4c9abf9e30f38",
   "metadata": {
    "id": "af4c9abf9e30f38"
   },
   "source": [
    "## Knowledge base generation pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "DhXn0QOjOlRN",
   "metadata": {
    "id": "DhXn0QOjOlRN"
   },
   "source": [
    "Loads environment variables and sets up configuration for OpenAI and Reddit API access. The `load_dotenv()` function imports variables from a `.env` file, while `os.getenv()` safely retrieves each value. `GENERATE_KNOWLEDGE` flag controls additional functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d74b9ef0648dd411",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T14:42:01.631900Z",
     "start_time": "2024-11-17T14:42:01.614375Z"
    },
    "id": "d74b9ef0648dd411"
   },
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "CLIENT_ID = os.getenv('REDDIT_CLIENT_ID')\n",
    "CLIENT_SECRET = os.getenv('REDDIT_CLIENT_SECRET')\n",
    "USER_AGENT = os.getenv('REDDIT_USER_AGENT')\n",
    "\n",
    "GENERATE_KNOWLEDGE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "792d211c92430f29",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T14:42:04.195390Z",
     "start_time": "2024-11-17T14:42:04.105941Z"
    },
    "id": "792d211c92430f29"
   },
   "outputs": [],
   "source": [
    "client = OpenAI()\n",
    "reddit = praw.Reddit(client_id=CLIENT_ID,\n",
    "                     client_secret=CLIENT_SECRET,\n",
    "                     user_agent=USER_AGENT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "UtmMXrbCPV83",
   "metadata": {
    "id": "UtmMXrbCPV83"
   },
   "source": [
    "# Reddit Search Phrase Generator\n",
    "\n",
    "## Description\n",
    "Generates relevant Reddit search phrases using OpenAI's GPT-3.5-turbo. The function takes a query string and returns a list of 5 Reddit-optimized search terms.\n",
    "\n",
    "## Features\n",
    "- Uses custom prompt engineering for Reddit-specific context\n",
    "- Handles common Reddit terminology and naming patterns\n",
    "- Includes error handling with fallback to original query\n",
    "- Filters and cleans AI response to remove formatting characters\n",
    "- Maintains 0.5 temperature for balanced creativity/relevance\n",
    "\n",
    "## Parameters\n",
    "- query: String (e.g., \"project management software\")\n",
    "\n",
    "## Returns\n",
    "List of 5 search phrases optimized for Reddit search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9cdebf856a9754c0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T14:42:06.806254Z",
     "start_time": "2024-11-17T14:42:06.800999Z"
    },
    "id": "9cdebf856a9754c0"
   },
   "outputs": [],
   "source": [
    "def get_relevant_topics(query: str) -> list:\n",
    "    \"\"\"\n",
    "    Use OpenAI to generate relevant search phrases for a query\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "        You are a Reddit search expert who understands how Redditors discuss and search for topics.\n",
    "        Given the query \"{query}\", generate 5 search phrases that will find the most relevant subreddits.\n",
    "\n",
    "        Consider:\n",
    "        - How Redditors naturally phrase their questions/discussions\n",
    "        - Common abbreviations and terminology used on Reddit\n",
    "        - Related tools, technologies, or concepts frequently discussed\n",
    "        - Industry-specific subreddit naming patterns\n",
    "        - Problem-focused search terms (as many discussions are about solving problems)\n",
    "\n",
    "        For example, if query is \"project management software\":\n",
    "        - projectmanagement (direct community)\n",
    "        - asana vs trello (tool comparison commonly discussed)\n",
    "        - agile tools (methodology + tools)\n",
    "        - jira alternatives (tool alternative discussions)\n",
    "        - remote team management (broader problem space)\n",
    "        Return exactly 5 search phrases, one per line.\n",
    "        Focus on phrases that would lead to active, relevant subreddit communities.\n",
    "        Do not include any bullets, numbers, or prefixes.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[{\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt\n",
    "            }],\n",
    "            temperature=0.5,\n",
    "            max_tokens=256\n",
    "        )\n",
    "\n",
    "        # Extract and clean phrases\n",
    "        search_phrases = [\n",
    "            phrase.strip()\n",
    "            for phrase in response.choices[0].message.content.split('\\n')\n",
    "            if phrase.strip() and not phrase.startswith(('-', '*', '•', '1', '2', '3', '4', '5'))\n",
    "        ]\n",
    "\n",
    "        print(f\"Generated search phrases: {search_phrases}\")\n",
    "        return search_phrases\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating topics: {e}\")\n",
    "        return [query]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sfQjXhpGPqEN",
   "metadata": {
    "id": "sfQjXhpGPqEN"
   },
   "source": [
    "# Subreddit Search Function\n",
    "\n",
    "## Description\n",
    "Performs multi-term subreddit search using PRAW (Reddit API wrapper). Takes a list of search terms and returns unique subreddits matching any term.\n",
    "\n",
    "## Parameters\n",
    "- search_terms: List of search phrases\n",
    "- limit: Max subreddits per term (default 50)\n",
    "\n",
    "## Returns\n",
    "Deduplicated list of subreddit names\n",
    "- Uses set for automatic deduplication\n",
    "- Handles API errors gracefully\n",
    "- Includes progress logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be594236c8f5d385",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T14:42:07.200385Z",
     "start_time": "2024-11-17T14:42:07.197178Z"
    },
    "id": "be594236c8f5d385"
   },
   "outputs": [],
   "source": [
    "def search_subreddits(search_terms, limit=50):\n",
    "    \"\"\"\n",
    "    Search for subreddits matching multiple search terms.\n",
    "\n",
    "    Args:\n",
    "        search_terms (list): List of search queries to use\n",
    "        limit (int): Maximum number of subreddits to return per search term\n",
    "\n",
    "    Returns:\n",
    "        list: List of unique subreddit names found across all search terms\n",
    "    \"\"\"\n",
    "    print(f\"Searching for subreddits matching {len(search_terms)} search terms...\")\n",
    "    subreddits = set()  # Using a set to avoid duplicates\n",
    "\n",
    "    try:\n",
    "        for term in search_terms:\n",
    "            print(f\"Searching with term: '{term}'...\")\n",
    "            term_subreddits = []\n",
    "            for subreddit in reddit.subreddits.search(term, limit=limit):\n",
    "                subreddits.add(subreddit.display_name)\n",
    "                term_subreddits.append(subreddit.display_name)\n",
    "            print(f\"Found with '{term}': {term_subreddits}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during subreddit search: {e}\")\n",
    "\n",
    "    return list(subreddits)  # Convert set back to list for consistency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "XfXJO2sDQjFt",
   "metadata": {
    "id": "XfXJO2sDQjFt"
   },
   "source": [
    "# Subreddit Information Retriever\n",
    "\n",
    "## Description\n",
    "Fetches key information about a specific subreddit using PRAW, returning a dictionary of essential subreddit metadata.\n",
    "\n",
    "## Parameters\n",
    "- subreddit_name: String (name of the subreddit)\n",
    "\n",
    "## Returns\n",
    "Dictionary containing:\n",
    "- name: Display name of subreddit\n",
    "- title: Subreddit title\n",
    "- subscribers: Number of subscribers\n",
    "- public_description: Public description text\n",
    "\n",
    "Returns None if retrieval fails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1aba7db8434c5333",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T14:42:07.541432Z",
     "start_time": "2024-11-17T14:42:07.538239Z"
    },
    "id": "1aba7db8434c5333"
   },
   "outputs": [],
   "source": [
    "def get_subreddit_info(subreddit_name):\n",
    "    \"\"\"\n",
    "    Retrieve information about a subreddit.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        subreddit = reddit.subreddit(subreddit_name)\n",
    "        return {\n",
    "            'name': subreddit.display_name,\n",
    "            'title': subreddit.title,\n",
    "            'subscribers': subreddit.subscribers,\n",
    "            'public_description': subreddit.public_description\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error retrieving info for r/{subreddit_name}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sSg_GWrHQsLh",
   "metadata": {
    "id": "sSg_GWrHQsLh"
   },
   "source": [
    "# Subreddit Post & Comment Scraper\n",
    "\n",
    "## Description\n",
    "Extracts comprehensive data from a subreddit's posts and their comments, including metadata and content. Includes rate limiting and error handling.\n",
    "\n",
    "## Parameters\n",
    "- subreddit_name: String (target subreddit)\n",
    "- max_posts: Integer (maximum posts to fetch)\n",
    "- max_comments: Integer, optional (comment fetch limit per post)\n",
    "\n",
    "## Returns\n",
    "List of dictionaries containing:\n",
    "### Post Data\n",
    "- id, title, author, score\n",
    "- upvotes, downvotes\n",
    "- num_comments, created_utc\n",
    "- url, permalink, selftext\n",
    "### Comment Data (per post)\n",
    "- id, author, body, score\n",
    "- upvotes, downvotes\n",
    "- created_utc, parent_id\n",
    "- link_id, permalink\n",
    "\n",
    "## Features\n",
    "- Rate limited (0.5s delay between posts)\n",
    "- Handles comment tree expansion\n",
    "- Includes error handling and logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e33237cf98b570b4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T14:42:07.837177Z",
     "start_time": "2024-11-17T14:42:07.830900Z"
    },
    "id": "e33237cf98b570b4"
   },
   "outputs": [],
   "source": [
    "def get_subreddit_posts(subreddit_name: str, max_posts: int, max_comments: int = None):\n",
    "    \"\"\"\n",
    "    Fetch all posts and comments from a subreddit, and extract problem-related sentences.\n",
    "    \"\"\"\n",
    "    subreddit = reddit.subreddit(subreddit_name)\n",
    "    posts_data = []\n",
    "\n",
    "    print(f\"Fetching posts from r/{subreddit_name}...\")\n",
    "\n",
    "    try:\n",
    "        for post in subreddit.new(limit=max_posts):\n",
    "            post_info = {\n",
    "                'id': post.id,\n",
    "                'title': post.title,\n",
    "                'author': str(post.author),\n",
    "                'score': post.score,\n",
    "                'upvotes': post.ups,\n",
    "                'downvotes': post.downs,\n",
    "                'num_comments': post.num_comments,\n",
    "                'created_utc': post.created_utc,\n",
    "                'url': post.url,\n",
    "                'permalink': post.permalink,\n",
    "                'selftext': post.selftext,\n",
    "                'comments': []\n",
    "            }\n",
    "\n",
    "            post.comments.replace_more(limit=max_comments)\n",
    "            print(f\"Fetching comments for post ID {post.id}...\")\n",
    "            for comment in post.comments.list():\n",
    "                comment_info = {\n",
    "                    'id': comment.id,\n",
    "                    'author': str(comment.author),\n",
    "                    'body': comment.body,\n",
    "                    'score': comment.score,\n",
    "                    'upvotes': comment.ups,\n",
    "                    'downvotes': comment.downs,\n",
    "                    'created_utc': comment.created_utc,\n",
    "                    'parent_id': comment.parent_id,\n",
    "                    'link_id': comment.link_id,\n",
    "                    'permalink': comment.permalink\n",
    "                }\n",
    "\n",
    "                post_info['comments'].append(comment_info)\n",
    "\n",
    "            posts_data.append(post_info)\n",
    "            time.sleep(0.5)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching posts from r/{subreddit_name}: {e}\")\n",
    "\n",
    "    return posts_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "_lsd_3ujQ-s2",
   "metadata": {
    "id": "_lsd_3ujQ-s2"
   },
   "source": [
    "# ChromaDB Document Processor\n",
    "\n",
    "## Description\n",
    "Processes Reddit posts and comments data into embeddings and stores them in a ChromaDB collection. Handles text chunking, embedding generation, and batch processing for large datasets.\n",
    "\n",
    "## Parameters\n",
    "- posts_data: List[Dict] (Reddit posts and comments)\n",
    "- collection_name: String (ChromaDB collection identifier)\n",
    "\n",
    "## Features\n",
    "- Uses OpenAI text-embedding-3-small model\n",
    "- Recursive text splitting (500 char chunks, 50 overlap)\n",
    "- Batch processing (500 documents per batch)\n",
    "- Persistent storage in ChromaDB\n",
    "- Handles both posts and comments\n",
    "\n",
    "## Document Metadata\n",
    "### Posts\n",
    "- id, type, title, author\n",
    "- url, created_utc\n",
    "\n",
    "### Comments\n",
    "- id, type, post_id, post_title\n",
    "- author, parent_id, url, created_utc\n",
    "\n",
    "## Returns\n",
    "Integer count of total documents stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7dabfbd537f3d756",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T14:42:08.193411Z",
     "start_time": "2024-11-17T14:42:08.186305Z"
    },
    "id": "7dabfbd537f3d756"
   },
   "outputs": [],
   "source": [
    "def process_and_store_in_chroma(posts_data: List[Dict], collection_name: str):\n",
    "    chroma_client = chromadb.PersistentClient(path=\"data/chroma_db\")\n",
    "    embeddings_client = OpenAIEmbeddings(\n",
    "        model=\"text-embedding-3-small\",\n",
    "        openai_api_key=OPENAI_API_KEY\n",
    "    )\n",
    "\n",
    "    collection = chroma_client.get_or_create_collection(name=collection_name)\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=500,\n",
    "        chunk_overlap=50\n",
    "    )\n",
    "\n",
    "    documents = []\n",
    "    metadatas = []\n",
    "    ids = []\n",
    "    doc_id = 0\n",
    "\n",
    "    print(f\"Processing posts and comments for {collection_name}...\")\n",
    "\n",
    "    for post in posts_data:\n",
    "        if post[\"selftext\"]:\n",
    "            chunks = text_splitter.split_text(post[\"selftext\"]) if len(post[\"selftext\"]) > 500 else [post[\"selftext\"]]\n",
    "\n",
    "            chunk_embeddings = embeddings_client.embed_documents(chunks)\n",
    "\n",
    "            for chunk, embedding in zip(chunks, chunk_embeddings):\n",
    "                documents.append(chunk)\n",
    "                metadatas.append({\n",
    "                    \"id\": post[\"id\"],\n",
    "                    \"type\": \"post\",\n",
    "                    \"title\": post[\"title\"],\n",
    "                    \"author\": post[\"author\"],\n",
    "                    \"url\": post[\"url\"],\n",
    "                    \"created_utc\": str(post[\"created_utc\"])\n",
    "                })\n",
    "                ids.append(f\"doc_{doc_id}\")\n",
    "                doc_id += 1\n",
    "\n",
    "        for comment in post.get(\"comments\", []):\n",
    "            if comment[\"body\"]:\n",
    "                chunks = text_splitter.split_text(comment[\"body\"]) if len(comment[\"body\"]) > 500 else [comment[\"body\"]]\n",
    "\n",
    "                chunk_embeddings = embeddings_client.embed_documents(chunks)\n",
    "\n",
    "                for chunk, embedding in zip(chunks, chunk_embeddings):\n",
    "                    documents.append(chunk)\n",
    "                    metadatas.append({\n",
    "                        \"id\": comment[\"id\"],\n",
    "                        \"type\": \"comment\",\n",
    "                        \"post_id\": post[\"id\"],\n",
    "                        \"post_title\": post[\"title\"],\n",
    "                        \"author\": comment[\"author\"],\n",
    "                        \"parent_id\": comment[\"parent_id\"],\n",
    "                        \"url\": comment[\"url\"],\n",
    "                        \"created_utc\": str(comment[\"created_utc\"])\n",
    "                    })\n",
    "                    ids.append(f\"doc_{doc_id}\")\n",
    "                    doc_id += 1\n",
    "\n",
    "        if len(documents) >= 500:\n",
    "            embeddings = embeddings_client.embed_documents(documents)\n",
    "            collection.add(\n",
    "                documents=documents,\n",
    "                embeddings=embeddings,\n",
    "                metadatas=metadatas,\n",
    "                ids=ids\n",
    "            )\n",
    "            print(f\"Added batch of {len(documents)} documents to Chroma\")\n",
    "            documents = []\n",
    "            metadatas = []\n",
    "            ids = []\n",
    "\n",
    "    # Add any remaining documents\n",
    "    if documents:\n",
    "        embeddings = embeddings_client.embed_documents(documents)\n",
    "        collection.add(\n",
    "            documents=documents,\n",
    "            embeddings=embeddings,\n",
    "            metadatas=metadatas,\n",
    "            ids=ids\n",
    "        )\n",
    "        print(f\"Added final batch of {len(documents)} documents to Chroma\")\n",
    "\n",
    "    return collection.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63kc1f36RG-s",
   "metadata": {
    "id": "63kc1f36RG-s"
   },
   "source": [
    "# Knowledge Base Generator\n",
    "\n",
    "## Description\n",
    "Orchestrates the full knowledge base generation pipeline, from Reddit search to ChromaDB storage. Combines subreddit search, data collection, and embedding storage functionalities.\n",
    "\n",
    "## Parameters\n",
    "- user_query: String (search topic)\n",
    "- max_subreddits: Integer (subreddit limit)\n",
    "- min_subscribers: Integer (minimum subscriber threshold)\n",
    "- max_posts: Integer (posts per subreddit)\n",
    "- max_comments: Integer, optional (comments per post)\n",
    "\n",
    "## Process Flow\n",
    "1. Generates search terms from query\n",
    "2. Searches relevant subreddits\n",
    "3. Filters by subscriber count\n",
    "4. Collects posts and comments\n",
    "5. Processes into ChromaDB collections\n",
    "\n",
    "## Features\n",
    "- Rate limiting between requests\n",
    "- Progress logging\n",
    "- Error handling per subreddit\n",
    "- Creates separate collection per subreddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "78d60fa8d41f57b3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T14:42:08.527535Z",
     "start_time": "2024-11-17T14:42:08.523598Z"
    },
    "id": "78d60fa8d41f57b3"
   },
   "outputs": [],
   "source": [
    "def generate_knowledge_base(user_query: str, max_subreddits: int, min_subscribers: int, max_posts: int, max_comments: int = None):\n",
    "    search_terms = get_relevant_topics(query=user_query)\n",
    "    found_subreddits = search_subreddits(search_terms=search_terms, limit=max_subreddits)\n",
    "\n",
    "    if not found_subreddits:\n",
    "        print(\"No subreddits with enough subscribers found with the given query.\")\n",
    "        return\n",
    "\n",
    "    print(\"\\nRetrieving subreddit information...\")\n",
    "    subreddit_infos = []\n",
    "    for subreddit_name in found_subreddits:\n",
    "        info = get_subreddit_info(subreddit_name)\n",
    "        if info:\n",
    "            subreddit_infos.append(info)\n",
    "        time.sleep(1)\n",
    "\n",
    "    for info in subreddit_infos:\n",
    "        if info['subscribers'] > min_subscribers:\n",
    "            subreddit_name = info['name']\n",
    "            print(f\"\\nProcessing r/{subreddit_name}...\")\n",
    "\n",
    "            posts = get_subreddit_posts(subreddit_name, max_posts, max_comments)\n",
    "\n",
    "            try:\n",
    "                collection_name = f\"reddit_{subreddit_name.lower()}\"\n",
    "                doc_count = process_and_store_in_chroma(posts, collection_name)\n",
    "                print(f\"Processed and stored {doc_count} documents in Chroma collection '{collection_name}'\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing and storing data in Chroma: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6RuBgG9RN9Q",
   "metadata": {
    "id": "c6RuBgG9RN9Q"
   },
   "source": [
    "# Main Execution Block\n",
    "\n",
    "## Description\n",
    "Entry point for knowledge base generation, triggered by `GENERATE_KNOWLEDGE` flag. Handles user input collection and initiates the generation process.\n",
    "\n",
    "## User Inputs\n",
    "- query: Search topic string\n",
    "- max_subreddits: Integer\n",
    "\n",
    "## Fixed Parameters\n",
    "- min_subscribers: 100,000\n",
    "- max_posts: 100 per subreddit\n",
    "\n",
    "## Usage\n",
    "Set `GENERATE_KNOWLEDGE=True` in environment to activate interactive mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "75663b31bfffed30",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T14:42:08.826137Z",
     "start_time": "2024-11-17T14:42:08.817062Z"
    },
    "id": "75663b31bfffed30"
   },
   "outputs": [],
   "source": [
    "if GENERATE_KNOWLEDGE:\n",
    "    query = input(\"Enter the search query for subreddits: \")\n",
    "    max_subreddits = int(input(\"Enter the number of subreddits to find: \"))\n",
    "    generate_knowledge_base(user_query=query, max_subreddits=max_subreddits, min_subscribers=100000, max_posts=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd45328086e0fca",
   "metadata": {
    "id": "bbd45328086e0fca"
   },
   "source": [
    "## Agent workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "VV56UNCNR4Cg",
   "metadata": {
    "id": "VV56UNCNR4Cg"
   },
   "source": [
    "# Core LangChain & LangGraph Imports\n",
    "\n",
    "Imports for OpenAI chat model integration (`ChatOpenAI`), prompt templating (`ChatPromptTemplate`), message types (`SystemMessage`, `AIMessage`, `HumanMessage`), and graph-based conversation flow components (`StateGraph`, `START`, `END`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "53990c6a46798f0b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T14:42:10.337836Z",
     "start_time": "2024-11-17T14:42:10.299750Z"
    },
    "id": "53990c6a46798f0b"
   },
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.messages import SystemMessage, AIMessage, HumanMessage\n",
    "from langgraph.graph import END, START, StateGraph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "evCRXW2GSGB4",
   "metadata": {
    "id": "evCRXW2GSGB4"
   },
   "source": [
    "# ChatOpenAI Model Initialization\n",
    "\n",
    "Initializes GPT-4 model with zero temperature (no randomness) for deterministic outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "460ae210411f8a56",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T14:42:10.754797Z",
     "start_time": "2024-11-17T14:42:10.731612Z"
    },
    "id": "460ae210411f8a56"
   },
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-4\", temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4sfO4H_YSOZz",
   "metadata": {
    "id": "4sfO4H_YSOZz"
   },
   "source": [
    "# query_chroma Function\n",
    "- Queries Chroma DB with OpenAI embeddings to find similar content\n",
    "- Input: query text, collection name, number of results (default=5)\n",
    "- Uses text-embedding-3-small model and local persistent storage\n",
    "- Returns matching documents from specified collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7502cee186316763",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T14:42:10.923975Z",
     "start_time": "2024-11-17T14:42:10.918863Z"
    },
    "id": "7502cee186316763"
   },
   "outputs": [],
   "source": [
    "def query_chroma(query_text: str, collection_name: str, n_results: int = 5):\n",
    "    \"\"\"\n",
    "    Query the Chroma database for similar content.\n",
    "    \"\"\"\n",
    "    chroma_client = chromadb.PersistentClient(path=\"data/chroma_db\")\n",
    "    embeddings_client = OpenAIEmbeddings(\n",
    "        model=\"text-embedding-3-small\",\n",
    "        openai_api_key=OPENAI_API_KEY\n",
    "    )\n",
    "\n",
    "    collection = chroma_client.get_collection(name=collection_name)\n",
    "    query_embedding = embeddings_client.embed_query(query_text)\n",
    "\n",
    "    results = collection.query(\n",
    "        query_embeddings=[query_embedding],\n",
    "        n_results=n_results\n",
    "    )\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pJPg-vZ_SUPE",
   "metadata": {
    "id": "pJPg-vZ_SUPE"
   },
   "source": [
    "# query_and_format_results Function\n",
    "- Extends query_chroma by formatting results into organized dictionary\n",
    "- Input: query text, collection name, result count (default=5)\n",
    "- Returns: Dict with query info (search query, collection, total results) and matches (content, metadata, distance)\n",
    "- Includes error handling with exception reporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a669951fc079ccd1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T14:42:11.083053Z",
     "start_time": "2024-11-17T14:42:11.078273Z"
    },
    "id": "a669951fc079ccd1"
   },
   "outputs": [],
   "source": [
    "def query_and_format_results(query_text: str, collection_name: str, n_results: int = 5) -> Dict[dict, list]:\n",
    "    \"\"\"\n",
    "    Query Chroma and return formatted results with all related information.\n",
    "\n",
    "    Args:\n",
    "        query_text (str): The search query\n",
    "        collection_name (str): Name of the Chroma collection to search\n",
    "        n_results (int): Number of results to return\n",
    "\n",
    "    Returns:\n",
    "        dict: Formatted results with query info and matched documents\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Get raw results from Chroma\n",
    "        results = query_chroma(query_text, collection_name, n_results)\n",
    "\n",
    "        # Format the results\n",
    "        formatted_results = {\n",
    "            \"query_info\": {\n",
    "                \"search_query\": query_text,\n",
    "                \"collection\": collection_name,\n",
    "                \"total_results\": len(results['ids'][0])\n",
    "            },\n",
    "            \"matches\": []\n",
    "        }\n",
    "\n",
    "        # Process each result\n",
    "        for i in range(len(results['ids'][0])):\n",
    "            match = {\n",
    "                \"content\": results['documents'][0][i],\n",
    "                \"metadata\": results['metadatas'][0][i],\n",
    "                \"distance\": results['distances'][0][i] if 'distances' in results else None\n",
    "            }\n",
    "            formatted_results[\"matches\"].append(match)\n",
    "\n",
    "        return formatted_results\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error querying database: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Y9jQshMaSZlN",
   "metadata": {
    "id": "Y9jQshMaSZlN"
   },
   "source": [
    "# augment_query Function\n",
    "- Enhances search query using rules from JSON file\n",
    "- Input: query text and optional JSON path (default='augment.json')\n",
    "- Applies prefix/suffix rules from JSON to create multiple query variations\n",
    "- Falls back to original query if JSON loading fails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7eeba914d30f2025",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T14:42:11.230777Z",
     "start_time": "2024-11-17T14:42:11.227068Z"
    },
    "id": "7eeba914d30f2025"
   },
   "outputs": [],
   "source": [
    "def augment_query(query_text: str, query_file_path: str = 'augment.json'):\n",
    "    \"\"\"Augment query using json\"\"\"\n",
    "    try:\n",
    "        with open(query_file_path, 'r') as f:\n",
    "            rules = json.load(f)\n",
    "            return [f\"{rule.get('prefix', '')} {query_text} {rule.get('suffix', '')}\".strip()\n",
    "                    for rule in rules.get('searches', [])]\n",
    "    except:\n",
    "        return [query_text]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "MUl_xcqqSfN2",
   "metadata": {
    "id": "MUl_xcqqSfN2"
   },
   "source": [
    "# retrieve Function\n",
    "- Retrieves matching documents from database using query augmentation\n",
    "- Takes state dict with query (collection name currently hardcoded)\n",
    "- Runs augmented variations of original query through query_and_format_results\n",
    "- Returns dict containing search results and original query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "af4586c1a11d49bc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T14:42:11.390349Z",
     "start_time": "2024-11-17T14:42:11.387895Z"
    },
    "id": "af4586c1a11d49bc"
   },
   "outputs": [],
   "source": [
    "def retrieve(state):\n",
    "    query = state[\"query\"]\n",
    "    # TODO: Determine best collection\n",
    "    # collection_name = state[\"collection\"]\n",
    "    collection_name = \"reddit_digitalmarketing\"\n",
    "\n",
    "    augmented_queries = augment_query(query)\n",
    "\n",
    "    for augmented_query in augmented_queries:\n",
    "        #print(f\"=== Search augmented query: {augmented_query} ===\")\n",
    "        results = query_and_format_results(augmented_query, collection_name)\n",
    "\n",
    "    return {\"results\": results, \"query\": query}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Rp9l-OyWSkSw",
   "metadata": {
    "id": "Rp9l-OyWSkSw"
   },
   "source": [
    "# generate Function\n",
    "- Formats search results into context for LLM response generation\n",
    "- Uses source metadata to add context information (type, title for posts)\n",
    "- Loads system prompt from generator_prompt.txt\n",
    "- Creates chat template with context and query, invokes LLM\n",
    "- Returns dict with results, query, and generated response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "986d983f1db6c90a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T14:42:11.537104Z",
     "start_time": "2024-11-17T14:42:11.534114Z"
    },
    "id": "986d983f1db6c90a"
   },
   "outputs": [],
   "source": [
    "def generate(state):\n",
    "    results = state[\"results\"]\n",
    "    query = state[\"query\"]\n",
    "\n",
    "    context_parts = []\n",
    "    for match in results['matches']:\n",
    "        metadata = match['metadata']\n",
    "        source_info = f\"Source: {metadata['type'].capitalize()}\"\n",
    "        if metadata['type'] == 'post':\n",
    "            source_info += f\" - Title: {metadata['title']}\"\n",
    "\n",
    "        context_parts.append(f\"{source_info}\\nContent: {match['content']}\\n\")\n",
    "\n",
    "    context = \"\\n\".join(context_parts)\n",
    "\n",
    "    with open('generator_prompt.txt', 'r') as f:\n",
    "        sys_prompt = f.read()\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        SystemMessage(content=sys_prompt),\n",
    "        AIMessage(content=f\"**Context:**\\n{context}\\n\\n\"),\n",
    "        HumanMessage(content=f\"**Question:** {query}\")\n",
    "    ])\n",
    "\n",
    "    generator = prompt | llm\n",
    "    response = generator.invoke({})\n",
    "\n",
    "    return {\"results\": results, \"query\": query, \"generation\": response.content}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "_inv-35rSpZm",
   "metadata": {
    "id": "_inv-35rSpZm"
   },
   "source": [
    "# AgentState TypedDict\n",
    "- Defines structure for agent's conversation state\n",
    "- Contains query (str), generation (str), and results (nested dict/list)\n",
    "- Used for type checking and state management in conversation flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "214c9bbe6fa1e691",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T14:42:11.695346Z",
     "start_time": "2024-11-17T14:42:11.692804Z"
    },
    "id": "214c9bbe6fa1e691"
   },
   "outputs": [],
   "source": [
    "class AgentState(TypedDict):\n",
    "    query : str\n",
    "    generation: str\n",
    "    results : Dict[dict, list]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6sQHedLSuwi",
   "metadata": {
    "id": "b6sQHedLSuwi"
   },
   "source": [
    "# LangGraph Workflow Setup\n",
    "- Creates StateGraph with AgentState type\n",
    "- Defines two processing nodes: retrieve and generate\n",
    "- Sets linear flow: START → retrieve → generate → END\n",
    "- Compiles graph for execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "52716f0f97c410a4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T14:42:11.831623Z",
     "start_time": "2024-11-17T14:42:11.827972Z"
    },
    "id": "52716f0f97c410a4"
   },
   "outputs": [],
   "source": [
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "workflow.add_node(retrieve, \"retrieve\")\n",
    "workflow.add_node(generate, \"generate\")\n",
    "\n",
    "workflow.add_edge(start_key=START, end_key=\"retrieve\")\n",
    "workflow.add_edge(start_key=\"retrieve\", end_key=\"generate\")\n",
    "workflow.add_edge(start_key=\"generate\", end_key=END)\n",
    "\n",
    "graph = workflow.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "KPa4qe1TS031",
   "metadata": {
    "id": "KPa4qe1TS031"
   },
   "source": [
    "# Workflow Visualization Command\n",
    "- Uses IPython to display workflow graph\n",
    "- Renders Mermaid diagram showing node connections\n",
    "- XRay mode reveals detailed graph structure\n",
    "- Helps visualize START → retrieve → generate → END flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6827a8528a3dcd4c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T14:42:12.694296Z",
     "start_time": "2024-11-17T14:42:11.984323Z"
    },
    "id": "6827a8528a3dcd4c"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "display(Image(graph.get_graph(xray=True).draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "YuhUg5KvS6LG",
   "metadata": {
    "id": "YuhUg5KvS6LG"
   },
   "source": [
    "# Process query function\n",
    "- Takes input dict with search query\n",
    "- Streams graph execution output step by step\n",
    "- Prints completion of each node (retrieve, generate)\n",
    "- Shows generated response when available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1bb73ff7a40fa7e3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T14:42:23.083748Z",
     "start_time": "2024-11-17T14:42:12.714588Z"
    },
    "id": "1bb73ff7a40fa7e3"
   },
   "outputs": [],
   "source": [
    "def process_query(inputs):\n",
    "    augmented_inputs = augment_query(inputs[\"query\"])\n",
    "    for augmented_query in augmented_inputs:\n",
    "        augmented_input = {\"query\": augmented_query} \n",
    "        print (\"Augmented query\", augmented_query)\n",
    "        for output in graph.stream(augmented_input):\n",
    "            for key, value in output.items():\n",
    "                print(f\"Finished running: {key}:\")\n",
    "                if 'generation' in value:\n",
    "                    print(value['generation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d0c9bf1564b2d1",
   "metadata": {
    "id": "92d0c9bf1564b2d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmented query Find popular tools for lead generation tools  and their main features\n",
      "Finished running: retrieve:\n",
      "Finished running: generate:\n",
      "**Key Findings:**\n",
      "\n",
      "1. The most frequently mentioned tools for lead generation and management include Monday CRM, LinkedIn’s SalesFlow, Warmly, ZoomInfo, Buffer, Hootsuite, Hubspot, ActiveCampaign, and Mailchimp.\n",
      "2. Monday CRM and LinkedIn’s SalesFlow are popular for their CRM capabilities, while Warmly and ZoomInfo are known for their lead generation features.\n",
      "3. Buffer and Hootsuite are recommended for organic growth and analytics, focusing on genuine engagement strategies.\n",
      "4. Hubspot is recognized for its comprehensive lead gen and management features but is noted to be pricey. ActiveCampaign is suggested as a lower-cost alternative with automation and lead tracking capabilities.\n",
      "5. Mailchimp is mentioned as a free tool up to a certain limit, presumably for email marketing and lead nurturing.\n",
      "\n",
      "**Market Implications:**\n",
      "\n",
      "1. There is a high demand for affordable, user-friendly lead generation and management tools, especially for small businesses and solo entrepreneurs.\n",
      "2. The market is saturated with a variety of tools, making it challenging for users to choose the most suitable one.\n",
      "3. There is a need for tools that offer comprehensive features, including CRM, lead generation, analytics, and automation.\n",
      "\n",
      "**Recommendations:**\n",
      "\n",
      "1. Companies should focus on creating user-friendly interfaces and providing comprehensive training or guides to help users navigate the technical side of these tools.\n",
      "2. Marketing efforts should highlight the tool's affordability, comprehensive features, and ease of use to attract small businesses and solo entrepreneurs.\n",
      "3. Companies should consider offering tiered pricing or free trials to attract users who are cautious about investing in a new tool.\n",
      "4. It would be beneficial to integrate features that support organic growth and genuine engagement strategies, as these are highly valued by users.\n",
      "5. Companies should consider offering robust customer support to assist users in maximizing the tool's potential.\n",
      "Augmented query List common complaints about lead generation tools  tools and software\n",
      "Finished running: retrieve:\n",
      "Finished running: generate:\n",
      "**Key Findings:**\n",
      "1. Overwhelming number of options: Users are finding it difficult to choose from the vast array of lead generation and management tools available in the market.\n",
      "2. Technical complexity: Some users, especially those who are not tech-savvy, find it challenging to navigate the technical aspects of these tools.\n",
      "3. Cost: High cost is a concern for users, especially those working with limited budgets.\n",
      "4. Sketchy offers: Users have expressed concerns about the legitimacy of some tools in the market.\n",
      "\n",
      "**Market Implications:**\n",
      "1. The market for lead generation and management tools is highly saturated, leading to decision paralysis for potential users.\n",
      "2. There is a demand for tools that are user-friendly and cater to non-technical users.\n",
      "3. There is a potential market for cost-effective tools, especially for small businesses or individual users.\n",
      "4. Trust and credibility are significant factors influencing users' choice of tools.\n",
      "\n",
      "**Recommendations:**\n",
      "1. Companies should focus on creating user-friendly interfaces and provide comprehensive user guides or customer support to help non-technical users navigate their tools.\n",
      "2. Offering competitive pricing or tiered pricing models could attract users with limited budgets.\n",
      "3. Companies should build trust and credibility by being transparent about their offerings, providing testimonials or case studies, and avoiding \"sketchy\" marketing tactics.\n",
      "4. Companies could consider offering a free trial or freemium version of their tool to help users make an informed decision.\n",
      "Augmented query Identify market leaders in lead generation tools  tools and their advantages\n",
      "Finished running: retrieve:\n",
      "Finished running: generate:\n",
      "**Key Findings:**\n",
      "\n",
      "1. The lead generation and management tools market is crowded with various options, causing confusion for potential users.\n",
      "2. Tools like Monday CRM, LinkedIn’s SalesFlow, Warmly, ZoomInfo, Hubspot, ActiveCampaign, and Mailchimp are mentioned as potential solutions.\n",
      "3. Cost is a significant factor for users when choosing a tool, with a preference for low-cost or free options.\n",
      "4. There is a need for tools that are easy to navigate, especially for users who are not tech-savvy.\n",
      "5. Some users believe that marketing is a better tool for increasing revenue than outbound sales.\n",
      "\n",
      "**Market Implications:**\n",
      "\n",
      "1. The crowded market indicates a high demand for lead generation and management tools, but also a potential for market saturation.\n",
      "2. The mention of several tools suggests that there is no clear market leader, indicating a competitive market.\n",
      "3. The emphasis on cost suggests that there is a market for affordable, yet effective tools.\n",
      "4. The need for user-friendly tools indicates a gap in the market for tools that are easy to navigate, especially for non-tech-savvy users.\n",
      "5. The belief in the effectiveness of marketing over outbound sales suggests a potential shift in market focus from sales to marketing tools.\n",
      "\n",
      "**Recommendations:**\n",
      "\n",
      "1. Companies should focus on creating user-friendly tools that are easy to navigate, even for non-tech-savvy users.\n",
      "2. Companies should consider offering affordable options or tiered pricing to attract users with different budgets.\n",
      "3. Companies should emphasize the effectiveness of their tools in marketing and lead generation, as this is seen as a more effective way to increase revenue than outbound sales.\n",
      "4. Companies should consider offering free trials or demos to allow potential users to test the tool before committing to a purchase.\n",
      "5. Companies should focus on differentiating their tools from others in the market to stand out in the crowded market.\n",
      "Augmented query Find current trends in lead generation tools  tools and technologies\n",
      "Finished running: retrieve:\n"
     ]
    }
   ],
   "source": [
    "inputs = {\"query\": \"lead generation tools \"}\n",
    "process_query(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef7efee-61bb-4d64-bfda-82960cf3ba2a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
